<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   1 Perception
  </title>
  <meta charset="utf-8"/>
  <meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="MobileRoboticsLectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="MobileRoboticsLectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/styles/atom-one-dark.min.css" rel="stylesheet"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/makefile.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     1
   
     
   

   Perception
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         1.1
        </small>
        <a href="#x3-30001.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.1.1
        </small>
        <a href="#x3-40001.1.1">
         Sensors for Mobile Robotics
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.1.2
        </small>
        <a href="#x3-50001.1.2">
         Sensor Classification
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.1.3
        </small>
        <a href="#x3-60001.1.3">
         Characterising Sensor Performance
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.1.4
        </small>
        <a href="#x3-120001.1.4">
         Wheel and Motor Sensors
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.2
        </small>
        <a href="#x3-210001.2">
         Active Ranging
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.2.1
        </small>
        <a href="#x3-230001.2.1">
         The Ultrasonic Sensor
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.2.2
        </small>
        <a href="#x3-300001.2.2">
         Motion and Speed Sensors
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.3
        </small>
        <a href="#x3-320001.3">
         Vision Based Sensors
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.1
        </small>
        <a href="#x3-340001.3.1">
         CMOS Technology
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.2
        </small>
        <a href="#x3-350001.3.2">
         Visual Ranging Sensors
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.3.3
        </small>
        <a href="#x3-360001.3.3">
         Depth from Focus
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         1.4
        </small>
        <a href="#x3-370001.4">
         Feature Extraction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.4.1
        </small>
        <a href="#x3-380001.4.1">
         Defining Feature
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         1.4.2
        </small>
        <a href="#x3-390001.4.2">
         Using Range Data
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="MobileRoboticsLectureBookch2.html" style="float:right; font-size:10px">
       NEXT CHAPTER →
      </a>
      <a href="MobileRoboticsLectureBookli1.html" style="float:left; font-size:10px">
       ← PREV CHAPTER
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      1
     </div>
     <a id="x3-20001">
     </a>
     Perception
    </h1>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <div class="section-head" id="section-1" sec="section-1">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.1
      </small>
      <a id="x3-30001.1">
      </a>
      Introduction
     </h2>
     <p class="noindent">
      One
of
the
most
important
tasks
of
an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       Autonomous
Mobile
Robotics
(AMR)
      </a>
      is
to
acquire
knowledge
about
its
environment.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          1
         </sup>
        </span>
       </alert>
       <span id="textcolor1">
        One
       could
       even
       argue
       it
       is
       the
       definition
       of
       life,
       if
       you
       ask
       a
       biologist
       as
       the
       ability
       to
       feel
       and
       act
       on
       its
       environment
       is
       the
       bare
       necessity.
       </span>
      </span>
      This
     is
     achieved
     by
     taking
     measurements
     using
     various
     sensors
     and
     then
     extracting
     meaningful
     information
     from
     those
     measurements.
     </p>
     <p class="noindent">
      In
     this
     chapter
     we
     present
     the
     most
     common
     sensors
     used
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      and
     then
     discuss
     strategies
     for
      <alert style="color: #821131;">
       extracting
information
      </alert>
      from
     the
     sensors.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.1.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.1.1
      </small>
      <a id="x3-40001.1.1">
      </a>
      Sensors for Mobile Robotics
     </h3>
     <p class="noindent">
      There is a wide variety of sensors used in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s (Fig. 4.1). Some are used to measure simple values like the
     internal temperature of a robot’s electronics or the rotational speed of the motors in its wheels or actuators.
     Other, more sophisticated sensors can be used to acquire information about the robot’s environment or even to
     directly measure a robot’s global position. Here, we focus primarily on sensors used to extract information about
     the robot’s environment. Because a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      moves around, it will frequently encounter
      <alert style="color: #821131;">
       unforeseen
      </alert>
      environmental
     characteristics, and therefore such sensing is particularly critical. We begin with a functional classification of sensors.
     Then, after presenting basic tools for describing a sensor’s performance, we proceed to describe selected sensors in
     detail.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.1.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.1.2
      </small>
      <a id="x3-50001.1.2">
      </a>
      Sensor Classification
     </h3>
     <p class="noindent">
      We classify sensors using two <alert style="color: #821131;">(2)</alert>
     important functional axes. Let’s define these terms for clarity;
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Proprioceptive
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        sensors which measure values
        <alert style="color: #821131;">
         internal
        </alert>
        to the robot.
       </p>
       <div class="quoteblock">
        <p class="noindent">
         e.g.,
        motor
        speed,
        wheel
        load,
        robot
        arm
        joint
        angles,
        battery
        voltage.
        </p>
       </div>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Exteroceptive
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        sensors which measure information from the
        <alert style="color: #821131;">
         robot’s environment
        </alert>
        ;
       </p>
       <div class="quoteblock">
        <p class="noindent">
         e.g.,
        distance
        measurements,
        light
        intensity,
        sound
        amplitude.
        </p>
       </div>
       <div class="knowledge">
        <p class="noindent">
         exteroceptive sensor measurements are interpreted by the robot to extract meaningful environmental features.
        </p>
       </div>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Passive
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        sensors measure ambient environmental energy entering the sensor.
       </p>
       <div class="quoteblock">
        <p class="noindent">
         e.g.,
        temperature
        probes,
        microphones
        and
         <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
          Charge
         Coupled
         Device
         (CCD)
         </a>
         or
        CMOS
        cameras.
        </p>
       </div>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Active
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        sensors emit energy into the environment, then measure the environmental reaction. Because active sensors can manage
       more controlled interactions with the environment, they often achieve superior performance. However, active sensing
       introduces several risks: the outbound energy may affect the very characteristics that the sensor is attempting to measure.
       Furthermore, an active sensor may suffer from interference between its signal and those beyond its control. For example,
       signals emitted by other nearby robots, or similar sensors on the same robot my influence the resulting
       measurements. Examples of active sensors include wheel quadrature encoders, ultrasonic sensors and laser
       rangefinders.
       </p>
      </dd>
     </dl>
     <p class="noindent">
      The sensor classes in Table (4.1) are arranged in ascending order of complexity and descending order of technological maturity.
     Tactile sensors and proprioceptive sensors are critical to virtually all mobile robots, and are well understood and
     easily implemented. Commercial quadrature encoders, for example, may be purchased as part of a gear-motor
     assembly used in a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      . At the other extreme, visual interpretation by means of one or more
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      /CMOS
     cameras provides a broad array of potential functionalities, from obstacle avoidance and localisation to human face
     recognition. However, commercially available sensor units that provide visual functionalities are only now beginning to
     emerge
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.1.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.1.3
      </small>
      <a id="x3-60001.1.3">
      </a>
      Characterising Sensor Performance
     </h3>
     <p class="noindent">
      The sensors we describe in this chapter vary greatly in their performance characteristics. Some sensors provide extreme accuracy
     in well-controlled laboratory settings, but are overcome with error when subjected to real-world environmental variations. Other
     sensors provide narrow, high precision data in a wide variety settings. To quantify such performance characteristics,
     first we formally define the sensor performance terminology that will be valuable throughout the rest of this
     chapter.
      <a id="subsubsection*.2">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-7000">
      </a>
      Basic Sensor Response Ratings
     </h5>
     <p class="noindent">
      A number of sensor characteristics can be rated
      <alert style="color: #821131;">
       quantitatively
      </alert>
      in a laboratory setting. Such performance ratings will necessarily
     be best-case scenarios when the sensor is placed on a real-world robot, but are nevertheless useful.
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Dynamic Range
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Used
       to
       measure
       the
       spread
       between
       the
       lower
       and
       upper
       limits
       of
       inputs
       values
       to
       the
       sensor
       while
       maintaining
       normal
       sensor
       operation.
       Formally,
       the
       dynamic
       range
       is
       the
       ratio
       of
       the
       maximum
       input
       value
       to
       the
       minimum
       measurable
       input
       value.
       Because
       this
       raw
       ratio
       can
       be
       unwieldy,
       it
       is
       usually
       measured
       in
       Decibels,
       which
       is
       computed
       as
       ten
       times
       the
       common
       logarithm
       of
       the
       dynamic
       range.
       However,
       there
       is
       potential
       confusion
       in
       the
       calculation
       of
       Decibels,
       which
       are
       meant
       to
       measure
       the
       ratio
       between
       powers,
       such
       as
       Watts
       or
       Horsepower.
       </p>
       <p class="noindent">
        Suppose your sensor measures motor current and can register values from a minimum of 1
       mA
       to 20
       A
       . The dynamic
       range of this current sensor is defined as:
       </p>
       <table class="equation">
        <tr>
         <td>
          <p class="noindent">
          </p>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mstyle class="label" id="x3-7001r1">
           </mstyle>
           <mn>
            1
           </mn>
           <mn>
            0
           </mn>
           <mo class="MathClass-bin" stretchy="false">
            ⋅
           </mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
           <mrow>
            <mo fence="true" form="prefix">
             [
            </mo>
            <mrow>
             <mfrac>
              <mrow>
               <mn>
                2
               </mn>
               <mn>
                0
               </mn>
              </mrow>
              <mrow>
               <mn>
                0
               </mn>
               <mo class="MathClass-punc" stretchy="false">
                .
               </mo>
               <mn>
                0
               </mn>
               <mn>
                0
               </mn>
               <mn>
                1
               </mn>
              </mrow>
             </mfrac>
            </mrow>
            <mo fence="true" form="postfix">
             ]
            </mo>
           </mrow>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mn>
            4
           </mn>
           <mn>
            3
           </mn>
           <mspace class="quad" width="1em">
           </mspace>
           <mstyle class="text">
            <mtext>
             dB
            </mtext>
           </mstyle>
          </math>
         </td>
         <td class="eq-no">
          (1.1)
         </td>
        </tr>
       </table>
       <p class="noindent">
        Now suppose you have a voltage sensor that measures the voltage of your robot’s battery, measuring any value from 1
       mV
       to 20
       V
       . Voltage is
        <span id="bold" style="font-weight:bold;">
         NOT
        </span>
        a unit of power, but the square of voltage is proportional to power. Therefore, we use 20
       instead of 10:
       </p>
       <table class="equation">
        <tr>
         <td>
          <p class="noindent">
          </p>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mstyle class="label" id="x3-7002r2">
           </mstyle>
           <mn>
            2
           </mn>
           <mn>
            0
           </mn>
           <mo class="MathClass-bin" stretchy="false">
            ⋅
           </mo>
           <mi class="loglike">
            log
           </mi>
           <mo>
            ⁡
           </mo>
           <mrow>
            <mo fence="true" form="prefix">
             [
            </mo>
            <mrow>
             <mfrac>
              <mrow>
               <mn>
                2
               </mn>
               <mn>
                0
               </mn>
              </mrow>
              <mrow>
               <mn>
                0
               </mn>
               <mo class="MathClass-punc" stretchy="false">
                .
               </mo>
               <mn>
                0
               </mn>
               <mn>
                0
               </mn>
               <mn>
                1
               </mn>
              </mrow>
             </mfrac>
            </mrow>
            <mo fence="true" form="postfix">
             ]
            </mo>
           </mrow>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mn>
            8
           </mn>
           <mn>
            6
           </mn>
           <mspace class="quad" width="1em">
           </mspace>
           <mstyle class="text">
            <mtext>
             dB
            </mtext>
           </mstyle>
          </math>
         </td>
         <td class="eq-no">
          (1.2)
         </td>
        </tr>
       </table>
       <p class="noindent">
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Range
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        An important rating in
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        because often robot sensors operate in environments where they are frequently exposed to
       input values beyond their working range. In such cases, it is critical to understand how the sensor will respond. For
       example, an optical rangefinder will have a minimum operating range and can thus provide spurious data when
       measurements are taken with object closer than that minimum.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Resolution
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        The minimum difference between two <alert style="color: #821131;">(2)</alert>
       values that can be detected by a sensor. Usually, the lower limit of the
       dynamic range of a sensor is equal to its resolution. However, in the case of digital sensors, this is not necessarily so. For
       example, suppose that you have a sensor that measures voltage, performs an analogue-to-digital conversion and outputs
       the converted value as an 8-bit number linearly corresponding to between 0 and 5 Volts. If this sensor is truly linear, then
       it has
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <msup>
          <mrow>
           <mn>
            2
           </mn>
          </mrow>
          <mrow>
           <mn>
            8
           </mn>
          </mrow>
         </msup>
         <mo class="MathClass-bin" stretchy="false">
          −
         </mo>
         <mn>
          1
         </mn>
        </math>
        total output values or a resolution of:
       </p>
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mfrac>
            <mrow>
             <mn>
              5
             </mn>
            </mrow>
            <mrow>
             <mn>
              2
             </mn>
             <mn>
              5
             </mn>
             <mn>
              5
             </mn>
            </mrow>
           </mfrac>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mn>
            2
           </mn>
           <mn>
            0
           </mn>
           <mspace class="quad" width="1em">
           </mspace>
           <mstyle class="text">
            <mtext>
             mV
            </mtext>
           </mstyle>
          </math>
         </td>
        </tr>
       </table>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Linearity
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        is an important measure governing the behaviour of the sensor’s output signal as the input signal varies. A linear response indicates that if
       two <alert style="color: #821131;">(2)</alert>
       inputs, say
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          x
         </mi>
        </math>
        and
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          y
         </mi>
        </math>
        result in
       the two outputs
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          f
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
        and
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          f
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
        , then for
       any values
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          a
         </mi>
        </math>
        and
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          b
         </mi>
        </math>
        ,
       the following relation can be derived:
       </p>
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mi>
            f
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                x
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                +
               </mo>
               <mi>
                y
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            f
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                x
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mo class="MathClass-bin" stretchy="false">
            +
           </mo>
           <mi>
            f
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                y
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mo class="MathClass-punc" stretchy="false">
            .
           </mo>
          </math>
         </td>
        </tr>
       </table>
       <p class="noindent">
        This means that a plot of the sensor’s input/output response is simply a straight line.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Bandwidth or Frequency
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        is used to measure the speed with which a sensor can provide a stream of readings. Formally, the number of
       measurements per second is defined as the sensor’s frequency in
       Hz
       . Because of the dynamics of moving through their
       environment, mobile robots often are limited in maximum speed by the bandwidth of their obstacle detection sensors.
       Thus increasing the bandwidth of ranging and vision-based sensors has been a high-priority goal in the robotics
       community.
       </p>
      </dd>
     </dl>
     <a id="subsubsection*.3">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x3-8000">
      </a>
      In Situ Sensor Performance
     </h5>
     <p class="noindent">
      The above sensor characteristics can be reasonably measured in a laboratory environment, with confident extrapolation to
     performance in real-world deployment. However, a number of important measures cannot be reliably acquired without deep
     understanding of the complex interaction between all environmental characteristics and the sensors in question. This
     is most relevant to the most sophisticated sensors, including active ranging sensors and visual interpretation
     sensors.
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Sensitivity
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        A
       measure
       of
       the
       degree
       to
       which
       an
       incremental
       change
       in
       the
       target
       input
       signal
       changes
       the
       output
       signal.
       Formally,
       sensitivity
       is
       the
       ratio
       of
       output
       change
       to
       input
       change.
       Unfortunately,
       however,
       the
       sensitivity
       of
       exteroceptive
       sensors
       is
       often
       confounded
       by
       undesirable
       sensitivity
       and
       performance
       coupling
       to
       other
       environmental
       parameters.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Cross-Sensitivity
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        is
       the
       technical
       term
       for
       sensitivity
       to
       environmental
       parameters
       that
       are
       orthogonal
       to
       the
       target
       parameters
       for
       the
       sensor.
       For
       example,
       a
       flux-gate
       compass
       can
       demonstrate
       high
       sensitivity
       to
       magnetic
       north
       and
       is
       therefore
       of
       use
       for
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        navigation.
       However,
       the
       compass
       will
       also
       demonstrate
       high
       sensitivity
       to
       ferrous
       building
       materials,
       so
       much
       so
       that
       its
       cross-sensitivity
       often
       makes
       the
       sensor
       useless
       in
       some
       indoor
       environments.
       High
       cross-sensitivity
       of
       a
       sensor
       is
       generally
       undesirable,
       especially
       so
       when
       it
       cannot
       be
       modelled.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Error
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        of
       a
       sensor
       is
       defined
       as
       the
       difference
       between
       the
       sensor’s
       output
       measurements
       and
       the
       true
       values
       being
       measured,
       within
       some
       specific
       operating
       context.
       </p>
       <p class="noindent">
        As an example, given a true value
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          v
         </mi>
        </math>
        and a measured value
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          m
         </mi>
        </math>
        ,
       we can define error as:
       </p>
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mstyle class="text">
            <mtext>
             Error
            </mtext>
           </mstyle>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            m
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mi>
            v
           </mi>
           <mo class="MathClass-punc" stretchy="false">
            .
           </mo>
          </math>
         </td>
        </tr>
       </table>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Accuracy
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        defined as the degree of conformity between the sensor’s measurement and the true value, and is often expressed as a
       proportion of the true value (e.g. 97.5% accuracy):
       </p>
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mstyle class="text">
            <mtext>
             Accuracy
            </mtext>
           </mstyle>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mn>
            1
           </mn>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mfrac>
            <mrow>
             <mrow class="mathinner">
              <mspace class="negthinspace" width="-0.17em">
              </mspace>
              <mrow>
               <mo fence="true" form="prefix">
                |
               </mo>
               <mrow>
                <mi>
                 m
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 −
                </mo>
                <mi>
                 v
                </mi>
               </mrow>
               <mo fence="true" form="postfix">
                |
               </mo>
              </mrow>
             </mrow>
            </mrow>
            <mrow>
             <mi>
              v
             </mi>
            </mrow>
           </mfrac>
           <mo class="MathClass-punc" stretchy="false">
            .
           </mo>
          </math>
         </td>
        </tr>
       </table>
       <p class="noindent">
        Of course, obtaining the ground truth (
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          v
         </mi>
        </math>
        ),
       can be difficult or impossible, and so establishing a confident characterisation of sensor accuracy can be problematic.
       Further, it is important to distinguish between two different sources of error:
       </p>
       <ul class="itemize1">
        <li class="itemize">
         <p class="noindent">
          Systematic errors are caused by factors or processes that can in theory be modelled. These errors are, therefore,
         deterministic.
          <alert style="color: #821131;">
           <span id="bold" style="font-weight:bold;">
            <sup class="textsuperscript">
             2
            </sup>
           </span>
          </alert>
          <span class="marginnote">
           <alert style="color: #821131;">
            <span id="bold" style="font-weight:bold;">
             <sup class="textsuperscript">
              2
             </sup>
            </span>
           </alert>
           <span id="textcolor3">
            Meaning, it’s value is not determined by a random process and therefore should, in theory, be
           predictable.
           </span>
          </span>
         </p>
         <div class="quoteblock">
          <p class="noindent">
           Poor
          calibration
          of
          a
          laser
          rangefinder,
          un-modelled
          slope
          of
          a
          hallway
          floor
          and
          a
          bent
          stereo
          camera
          head
          due
          to
          an
          earlier
          collision
          are
          all
          possible
          causes
          of
          systematic
          sensor
          errors
          </p>
         </div>
        </li>
        <li class="itemize">
         <p class="noindent">
          Random errors cannot be predicted using a sophisticated model nor can they be mitigated with more precise sensor
         machinery. These errors can only be described in probabilistic terms (i.e. stochastic). Hue instability in a colour
         camera, spurious range-finding errors and black level noise in a camera are all examples of random
         errors.
         </p>
        </li>
       </ul>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Precision
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        is often confused with accuracy, and now we have the tools to clearly distinguish these two terms. Intuitively, high
       precision relates to reproducibility of the sensor results. For example, one sensor taking multiple readings of the same
       environmental state has high precision if it produces the same output. In another example, multiple copies of
       this sensors taking readings of the same environmental state have high precision if their outputs agree.
       Precision does not, however, have any bearing on the accuracy of the sensor’s output with respect to the true
       value being measured. Suppose that the random error of a sensor is characterised by some mean value
       (
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          μ
         </mi>
        </math>
        ) and a standard
       deviation (
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          σ
         </mi>
        </math>
        ).
       The formal definition of precision is the ratio of the sensor’s output range to the standard deviation:
       </p>
       <table class="equation-star">
        <tr>
         <td>
          <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
           <mstyle class="text">
            <mtext>
             Precision
            </mtext>
           </mstyle>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mfrac>
            <mrow>
             <mstyle class="text">
              <mtext>
               Range
              </mtext>
             </mstyle>
            </mrow>
            <mrow>
             <mi>
              σ
             </mi>
            </mrow>
           </mfrac>
           <mo class="MathClass-punc" stretchy="false">
            .
           </mo>
          </math>
         </td>
        </tr>
       </table>
       <div class="warning">
        <p class="noindent">
         Only
         <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>
           σ
          </mi>
         </math>
         and
         <span id="bold" style="font-weight:bold;">
          NOT
         </span>
         <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
          <mi>
           μ
          </mi>
         </math>
         has impact on precision. In contrast mean error is directly proportional to overall sensor error and inversely proportional
        to sensor accuracy.
        </p>
       </div>
      </dd>
     </dl>
     <a id="subsubsection*.4">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x3-9000">
      </a>
      Characterising Error
     </h5>
     <p class="noindent">
      Mobile robots depend heavily on
      <alert style="color: #821131;">
       exteroceptive
      </alert>
      sensors. Many of these sensors concentrate on a central task for the
     robot:
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       <italic>
        acquiring
information
on
objects
in
the
robot’s
immediate
vicinity
so
that
it
may
interpret
the
state
of
its
surroundings.
       </italic>
      </p>
     </div>
     <p class="noindent">
      Of
     course,
     these
     “objects”
     surrounding
     the
     robot
     are
     all
     detected
     from
     the
     viewpoint
     of
     its
     local
     reference
     frame.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          3
         </sup>
        </span>
       </alert>
       <span id="textcolor5">
        In
       this
       case
       we
       are
       referring
       to
       the
       robot
       reference
       frame.
       </span>
      </span>
      Since
     the
     systems
     we
     study
     are
      <alert style="color: #821131;">
       mobile
      </alert>
      ,
     their
     ever-changing
     position
     and
     their
     motion
     has
     a
     significant
     impact
     on
     overall
     sensor
     behaviour.
     </p>
     <p class="noindent">
      Now
     that
     we
     have
     the
     necessary
     knowledge
     on
     the
     fundamental
     concepts
     and
     terminology,
     we
     can
     now
     describe
     how
     dramatically
     the
     sensor
     error
     of
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      <alert style="color: #821131;">
       disagrees
      </alert>
      with
     the
     ideal
     picture
     drawn
     in
     the
     previous
     section.
     </p>
     <h5 class="likesubsubsectionHead">
      <a id="x3-10000">
      </a>
      Blurring of Systematical and Random Errors
     </h5>
     <p class="noindent">
      Active ranging sensors tend to have failure modes which are triggered largely by specific relative positions of the sensor and
     environment targets.
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example,
      a
      sonar
      sensor
      will
      product
      specular
      reflections,
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          4
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           4
          </sup>
         </span>
        </alert>
        <span id="textcolor6">
         The
        incident
        light
        is
        reflected
        into
        a
        single
        outgoing
        direction.
        </span>
       </span>
       producing
      grossly
      inaccurate
      measurements
      of
      range,
      at
      specific
      angles
      to
      a
      smooth
      sheet-rock
      wall.
      </p>
     </div>
     <p class="noindent">
      During motion of the robot, such relative angles occur at stochastic intervals. This is especially true in a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      outfitted with a
     ring of multiple sonars. The chances of one sonar entering this error mode during robot motion is high. From the perspective of
     the moving robot, the sonar measurement error is a
      <alert style="color: #821131;">
       random error
      </alert>
      in this case. However, if the robot were to stop, becoming
     motionless, then a very different error modality is possible.
     </p>
     <p class="noindent">
      If the robot’s static position causes a particular sonar to fail in this manner, the sonar will fail consistently and will tend to
     return precisely the same (and incorrect!) reading time after time. Once the robot is motionless, the error appears to be
     systematic and high precision.
     </p>
     <div class="warning">
      <p class="noindent">
       The fundamental mechanism at work here is the cross-sensitivity of
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       sensors to robot pose and robot-environment dynamics.
      </p>
     </div>
     <p class="noindent">
      The
     models
     for
     such
     cross-sensitivity
     are
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      ,
     in
     an
     underlying
     sense,
     truly
     random.
     However,
     these
     physical
     interrelationships
     are
     rarely
     modelled
     and
     therefore,
     from
     the
     point
     of
     view
     of
     an
     incomplete
     model,
     the
     errors
     appear
     random
     during
     motion
     and
     systematic
     when
     the
     robot
     is
     at
     rest.
     Sonar
     is
     not
     the
     only
     sensor
     subject
     to
     this
     blurring
     of
     systematic
     and
     random
     error
     modality.
     Visual
     interpretation
     through
     the
     use
     of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera
     is
     also
     highly
     susceptible
     to
     robot
     motion
     and
     position
     because
     of
     camera
     dependency
     on
     lighting.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          5
         </sup>
        </span>
       </alert>
       <span id="textcolor7">
        such
       as
       glare
       and
       reflections.
       </span>
      </span>
     </p>
     <div class="knowledge">
      <p class="noindent">
       The
      important
      point
      is
      to
      realise
      that,
      while
      systematic
      error
      and
      random
      error
      are
      well-defined
      in
      a
      controlled
      setting,
      the
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       can
      exhibit
      error
      characteristics
      that
      bridge
      the
      gap
      between
      deterministic
      and
      stochastic
      error
      mechanisms.
      </p>
     </div>
     <h5 class="likesubsubsectionHead">
      <a id="x3-11000">
      </a>
      Multi-Modal
     Error
     Distributions
     </h5>
     <p class="noindent">
      It
     is
     common
     to
     characterise
     the
     behaviour
     of
     a
     sensor’s
     random
     error
     in
     terms
     of
     a
     probability
     distribution
     over
     various
     output
     values.
     In
     general,
     one
     knows
     very
     little
     about
     the
     causes
     of
     random
     error
     and
     therefore
     several
     simplifying
     assumptions
     are
     commonly
     used.
     For
     example,
     we
     can
     assume
     that
     the
     error
     is
     zero-mean
     (
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        μ
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mn>
        0
       </mn>
      </math>
      ),
     in
     that
     it
     symmetrically
     generates
     both
     positive
     and
     negative
     measurement
     error.
     We
     can
     go
     even
     further
     and
     assume
     that
     the
     probability
     density
     curve
     is
     Gaussian.
     Although
     we
     discuss
     the
     mathematics
     of
     this
     in
     detail
     later,
     it
     is
     important
     for
     now
     to
     recognise
     the
     fact
     that
     one
     frequently
     assumes
     symmetry
     as
     well
     as
     unimodal
     distribution.
     This
     means
     that
     measuring
     the
     correct
     value
     is
     most
     probable,
     and
     any
     measurement
     that
     is
     further
     away
     from
     the
     correct
     value
     is
     less
     likely
     than
     any
     measurement
     that
     is
     closer
     to
     the
     correct
     value.
     These
     are
     strong
     assumptions
     that
     enable
     powerful
     mathematical
     principles
     to
     be
     applied
     to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      problems,
     but
     it
     is
     important
     to
     realise
     how
     wrong
     these
     assumptions
     usually
     are.
     </p>
     <p class="noindent">
      Consider,
     for
     example,
     the
     sonar
     sensor
     once
     again.
     When
     ranging
     an
     object
     that
     reflects
     the
     sound
     signal
     well,
     the
     sonar
     will
     exhibit
     high
     accuracy,
     and
     will
     induce
     random
     error
     based
     on
     noise,
     for
     example,
     in
     the
     timing
     circuitry.
     This
     portion
     of
     its
     sensor
     behaviour
     will
     exhibit
     error
     characteristics
     that
     are
     fairly
      <alert style="color: #821131;">
       symmetric
      </alert>
      and
      <alert style="color: #821131;">
       unimodal
      </alert>
      .
     However,
     when
     the
     sonar
     sensor
     is
     moving
     through
     an
     environment
     and
     is
     sometimes
     faced
     with
     materials
     that
     cause
     coherent
     reflection
     rather
     than
     returning
     the
     sound
     signal
     to
     the
     sonar
     sensor,
     then
     the
     sonar
     will
     grossly
     overestimate
     distance
     to
     the
     object.
     In
     such
     cases,
     the
     error
     will
     be
     biased
     toward
     positive
     measurement
     error
     and
     will
     be
     far
     from
     the
     correct
     value.
     The
     error
     is
     not
     strictly
     systematic,
     and
     so
     we
     are
     left
     modelling
     it
     as
     a
     probability
     distribution
     of
     random
     error.
     So
     the
     sonar
     sensor
     has
     two
     <alert style="color: #821131;">(2)</alert>
     separate
     types
     of
     operational
     modes,
     one
     in
     which
     the
     signal
     does
     return
     and
     some
     random
     error
     is
     possible,
     and
     the
     second
     in
     which
     the
     signal
     returns
     after
     a
     multi-path
     reflection,
     and
     gross
     overestimation
     error
     occurs.
     The
     probability
     distribution
     could
     easily
     be
     at
     least
     bimodal
     in
     this
     case,
     and
     since
     overestimation
     is
     more
     common
     than
     underestimation
     it
     will
     also
     be
     asymmetric.
     </p>
     <p class="noindent">
      As
     a
     second
     example,
     consider
     ranging
     via
     stereo
     vision.
     Once
     again,
     we
     can
     identify
     two
     <alert style="color: #821131;">(2)</alert>
     modes
     of
     operation.
     If
     the
     stereo
     vision
     system
     correctly
     correlates
     two
     images,
     then
     the
     resulting
     random
     error
     will
     be
     caused
     by
     camera
     noise
     and
     will
     limit
     the
     measurement
     accuracy.
     But
     the
     stereo
     vision
     system
     can
     also
     correlate
     two
     images
     incorrectly,
     matching
     two
     fence
     posts
     for
     example
     that
     are
     not
     the
     same
     post
     in
     the
     real
     world.
     In
     such
     a
     case
     stereo
     vision
     will
     exhibit
     gross
     measurement
     error,
     and
     one
     can
     easily
     imagine
     such
     behaviour
     violating
     both
     the
     unimodal
     and
     the
     symmetric
     assumptions.
     The
     thesis
     of
     this
     section
     is
     that
     sensors
     in
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      may
     be
     subject
     to
     multiple
     modes
     of
     operation
     and,
     when
     the
     sensor
     error
     is
     characterised,
     uni
     modality
     and
     symmetry
     may
     be
     grossly
     violated.
     Nonetheless,
     as
     you
     will
     see,
     many
     successful
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      systems
     make
     use
     of
     these
     simplifying
     assumptions
     and
     the
     resulting
     mathematical
     techniques
     with
     great
     empirical
     success.
     The
     above
     sections
     have
     presented
     a
     terminology
     with
     which
     we
     can
     characterise
     the
     advantages
     and
     disadvantages
     of
     various
     mobile
     robot
     sensors.
     In
     the
     following
     sections,
     we
     do
     the
     same
     for
     a
     sampling
     of
     the
     most
     commonly
     used
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      sensors
     today.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.1.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.1.4
      </small>
      <a id="x3-120001.1.4">
      </a>
      Wheel
     and
     Motor
     Sensors
     </h3>
     <p class="noindent">
      Wheel/motor
     sensors
     are
     devices
     use
     to
     measure
     the
     internal
     state
     and
     dynamics
     of
     a
     mobile
     robot.
     These
     sensors
     have
     vast
     applications
     outside
     of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      and,
     as
     a
     result,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      has
     enjoyed
     the
     benefits
     of
     high-quality,
     low-cost
     wheel
     and
     motor
     sensors
     which
     offer
     excellent
     resolution.
     </p>
     <p class="noindent">
      In
     the
     next
     part,
     we
     sample
     just
     one
     such
     sensor,
     the
     optical
     incremental
     encoder.
      <a id="subsubsection*.5">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-13000">
      </a>
      Optical
     Encoders
     </h5>
     <p class="noindent">
      Optical
     incremental
     encoders
     have
     become
     the
     most
     popular
     device
     for
     measuring
     angular
     speed
     and
     position
     within
     a
     motor
     drive
     or
     at
     the
     shaft
     of
     a
     wheel
     or
     steering
     mechanism.
     In
     mobile
     robotics,
     encoders
     are
     used
     to
     control
     the
     position
     or
     speed
     of
     wheels
     and
     other
     mo-
     tor-driven
     joints.
     Because
     these
     sensors
     are
     proprioceptive,
     their
     estimate
     of
     position
     is
     best
     in
     the
     reference
     frame
     of
     the
     robot
     and,
     when
     applied
     to
     the
     problem
     of
     robot
     localisation,
     significant
     corrections
     are
     required
     as
     discussed
     in
     Chapter
     5.
     </p>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/rotary-encoder.jpg" width="100%"/>
      <a id="x3-13001r1">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.1:
       </span>
       <span class="content">
        An   example   of   a
       rotary encoder. <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Farnell2025">[1]</a></span>
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      An
     optical
     encoder
     is
     basically
     a
     mechanical
     light
     chopper
     that
     produces
     a
     certain
     number
     of
     sine
     or
     square
     wave
     pulses
     for
     each
     shaft
     revolution.
     It
     consists
     of
     an
     illumination
     source,
     a
     fixed
     grating
     that
     masks
     the
     light,
     a
     rotor
     disc
     with
     a
     fine
     optical
     grid
     that
     rotates
     with
     the
     shaft,
     and
     fixed
     optical
     detectors.
     As
     the
     rotor
     moves,
     the
     amount
     of
     light
     striking
     the
     optical
     detectors
     varies
     based
     on
     the
     alignment
     of
     the
     fixed
     and
     moving
     gratings.
     In
     robotics,
     the
     resulting
     sine
     wave
     is
     transformed
     into
     a
     discrete
     square
     wave
     using
     a
     threshold
     to
     choose
     between
     light
     and
     dark
     states.
     Resolution
     is
     measured
     in
     Cycles
     Per
     Revolution
     (CPR).
     The
     minimum
     angular
     resolution
     can
     be
     readily
     computed
     from
     an
     encoder’s
     CPR
     rating.
     A
     typical
     encoder
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      may
     have
     2,000
     CPR
     while
     the
     optical
     encoder
     industry
     can
     readily
     manufacture
     encoders
     with
     10,000
     CPR.
     In
     terms
     of
     required
     bandwidth,
     it
     is
     of
     course
     critical
     that
     the
     encoder
     be
     sufficiently
     fast
     to
     count
     at
     the
     shaft
     spin
     speeds
     that
     are
     expected.
     Industrial
     optical
     encoders
     present
     no
     bandwidth
     limitation
     to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications.
     Usually
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      the
     quadrature
     encoder
     is
     used.
     In
     this
     case,
     a
     second
     illumination
     and
     detector
     pair
     is
     placed
     90ř
     shifted
     with
     respect
     to
     the
     original
     in
     terms
     of
     the
     rotor
     disc.
     The
     resulting
     twin
     square
     waves,
     shown
     in
     Fig.
     4.2,
     provide
     significantly
     more
     information.
     The
     ordering
     of
     which
     square
     wave
     produces
     a
     rising
     edge
     first
     identifies
     the
     direction
     of
     rotation.
     Furthermore,
     the
     four
     detectability
     different
     states
     improve
     the
     resolution
     by
     a
     factor
     of
     four
     with
     no
     change
     to
     the
     rotor
     disc.
     Thus,
     a
     2,000
     CPR
     encoder
     in
     quadrature
     yields
     8,000
     counts.
     Further
     improvement
     is
     possible
     by
     retaining
     the
     sinusoidal
     wave
     measured
     by
     the
     optical
     detectors
     and
     performing
     sophisticated
     interpolation.
     Such
     methods,
     although
     rare
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ,
     can
     yield
     1000-fold
     improvements
     in
     resolution.
     As
     with
     most
     proprioceptive
     sensors,
     encoders
     are
     generally
     in
     the
     controlled
     environment
     of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     internal
     structure,
     and
     so
     systematic
     error
     and
     cross-sensitivity
     can
     be
     engineered
     away.
     The
     accuracy
     of
     optical
     encoders
     is
     often
     assumed
     to
     be
     100%
     and,
     although
     this
     may
     not
     entirely
     correct,
     any
     errors
     at
     the
     level
     of
     an
     optical
     encoder
     are
     dwarfed
     by
     errors
     downstream
     of
     the
     motor
     shaft.
      <a id="subsubsection*.6">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-14000">
      </a>
      Heading
     Sensors
     </h5>
     <p class="noindent">
      Heading
     sensors
     can
     be
     proprioceptive
     (gyroscope,
     inclinometer)
     or
     exteroceptive
     (com-
     pass).
     They
     are
     used
     to
     determine
     the
     robots
     orientation
     and
     inclination.
     They
     allow
     us,
     to-
     gether
     with
     appropriate
     velocity
     information,
     to
     integrate
     the
     movement
     to
     a
     position
     estimate.
     This
     procedure,
     which
     has
     its
     roots
     in
     vessel
     and
     ship
     navigation,
     is
     called
     dead
     reckoning.
     </p>
     <h5 class="likesubsubsectionHead">
      <a id="x3-15000">
      </a>
      Compasses
     </h5>
     <p class="noindent">
      The
     two
     most
     common
     modern
     sensors
     for
     measuring
     the
     direction
     of
     a
     magnetic
     field
     are
     the
     Hall
     Effect
     and
     Flux
     Gate
     compasses.
     Each
     has
     advantages
     and
     disadvantages,
     as
     described
     below.
     The
     Hall
     Effect
     describes
     the
     behaviour
     of
     electric
     potential
     in
     a
     semiconductor
     when
     in
     the
     presence
     of
     a
     magnetic
     field.
     When
     a
     constant
     current
     is
     applied
     across
     the
     length
     of
     a
     semi-
     conductor,
     there
     will
     be
     a
     voltage
     difference
     in
     the
     perpendicular
     direction,
     across
     the
     semi-
     conductor’s
     width,
     based
     on
     the
     relative
     orientation
     of
     the
     semiconductor
     to
     magnetic
     flux
     </p>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/digital-compass.jpg" width="100%"/>
      <a id="x3-15001r2">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.2:
       </span>
       <span class="content">
        An      example      of
       an electronic compass <span class="cite"><a href="MobileRoboticsLectureBookad1.html#flyrobo2025">[2]</a></span>.
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      lines.
     In
     addition,
     the
     sign
     of
     the
     voltage
     potential
     identifies
     the
     direction
     of
     the
     magnetic
     field.
     Thus,
     a
     single
     semiconductor
     provides
     a
     measurement
     of
     flux
     and
     direction
     along
     one
     dimension.
     Hall
     Effect
     digital
     compasses
     are
     popular
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ,
     and
     contain
     two
     such
     semiconductors
     at
     right
     angles,
     providing
     two
     axes
     of
     magnetic
     field
     (thresholded)
     direction,
     thereby
     yielding
     one
     of
     8
     possible
     compass
     directions.
     The
     instruments
     are
     inexpensive
     but
     also
     suffer
     from
     a
     range
     of
     disadvantages.
     Resolution
     of
     a
     digital
     hall
     effect
     compass
     is
     poor.
     Internal
     sources
     of
     error
     include
     the
     nonlinearity
     of
     the
     basic
     sensor
     and
     systematic
     bias
     errors
     at
     the
     semiconductor
     level.
     The
     resulting
     circuitry
     must
     perform
     significant
     filtering,
     and
     this
     lowers
     the
     bandwidth
     of
     hall
     effect
     compasses
     to
     values
     that
     are
     slow
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      terms.
     For
     example
     the
     hall
     effect
     compasses
     pictured
     in
     figure
     4.3
     needs
     2.5
     seconds
     to
     settle
     after
     a
     90ř
     spin.
     The
     Flux
     Gate
     compass
     operates
     on
     a
     different
     principle.
     Two
     small
     coils
     are
     wound
     on
     fer-
     rite
     cores
     and
     are
     fixed
     perpendicular
     to
     one-another.
     When
     alternating
     current
     is
     activated
     in
     both
     coils,
     the
     magnetic
     field
     causes
     shifts
     in
     the
     phase
     depending
     upon
     its
     relative
     alignment
     with
     each
     coil.
     By
     measuring
     both
     phase
     shifts,
     the
     direction
     of
     the
     magnetic
     field
     in
     two
     dimensions
     can
     be
     computed.
     The
     flux-gate
     compass
     can
     accurately
     measure
     the
     strength
     of
     a
     magnetic
     field
     and
     has
     improved
     resolution
     and
     accuracy;
     however
     it
     is
     both
     larger
     and
     more
     expensive
     than
     a
     Hall
     Effect
     compass.
     Regardless
     of
     the
     type
     of
     compass
     used,
     a
     major
     drawback
     concerning
     the
     use
     of
     the
     Earth’s
     magnetic
     field
     for
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications
     involves
     disturbance
     of
     that
     magnetic
     field
     by
     other
     magnetic
     objects
     and
     man-made
     structures,
     as
     well
     as
     the
     bandwidth
     limitations
     of
     electronic
     compasses
     and
     their
     susceptibility
     to
     vibration.
     Particularly
     in
     indoor
     environments
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications
     have
     often
     avoided
     the
     use
     of
     compasses,
     although
     a
     compass
     can
     conceivably
     provide
     useful
     local
     orientation
     information
     indoors,
     even
     in
     the
     precense
     of
     steel
     structures.
      <a id="subsubsection*.7">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-16000">
      </a>
      Gyroscope
     </h5>
     <p class="noindent">
      Gyroscopes
     are
     heading
     sensors
     which
     preserve
     their
     orientation
     in
     relation
     to
     a
     fixed
     refer-
     ence
     frame.
     Thus
     they
     provide
     an
     absolute
     measure
     for
     the
     heading
     of
     a
     mobile
     system.
     Gy-
     roscopes
     can
     be
     classified
     in
     two
     categories,
     mechanical
     gyroscopes
     and
     optical
     gyroscopes.
      <a id="subsubsection*.8">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-17000">
      </a>
      Mechanical
     Gyroscopes
     </h5>
     <p class="noindent">
      The
     concept
     of
     a
     mechanical
     gyroscope
     relies
     on
     the
     inertial
     properties
     of
     a
     fast
     spinning
     rotor.
     The
     property
     of
     interest
     is
     known
     as
     the
     gyroscopic
     precession.
     If
     you
     try
     to
     rotate
     a
     fast
     spinning
     wheel
     around
     its
     vertical
     axis,
     you
     will
     feel
     a
     harsh
     reaction
     in
     the
     horizontal
     axis.
     This
     is
     due
     to
     the
     angular
     momentum
     associated
     with
     a
     spinning
     wheel
     and
     will
     keep
     the
     axis
     of
     the
     gyroscope
     inertially
     stable.
     The
     reactive
     torque
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        τ
       </mi>
      </math>
      and
     thus
     the
     tracking
     stability
     with
     the
     inertial
     frame
     are
     proportional
     to
     the
     spinning
     speed
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        ω
       </mi>
      </math>
      ,
     the
     precession
     speed
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi mathvariant="normal">
        Ω
       </mi>
      </math>
      and
     the
     wheel’s
     inertia
     I.
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          τ
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          I
         </mi>
         <mi>
          ω
         </mi>
         <mi mathvariant="normal">
          Ω
         </mi>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      By
     arranging
     a
     spinning
     wheel
     as
     seen
     in
     Figure
     4.4,
     no
     torque
     can
     be
     transmitted
     from
     the
     outer
     pivot
     to
     the
     wheel
     axis.
     The
     spinning
     axis
     will
     therefore
     be
     space-stable
     (i.e.
     fixed
     in
     an
     inertial
     reference
     frame).
     Nevertheless,
     the
     remaining
     friction
     in
     the
     bearings
     of
     the
     gyro-
     axis
     introduce
     small
     torques,
     thus
     limiting
     the
     long
     term
     space
     stability
     and
     introducing
     small
     errors
     over
     time.
     A
     high
     quality
     mechanical
     gyroscope
     can
     cost
     up
     to
     $100,000
     and
     has
     an
     angular
     drift
     of
     about
     0.1ř
     in
     6
     hours.
     For
     navigation,
     the
     spinning
     axis
     has
     to
     be
     initially
     selected.
     If
     the
     spinning
     axis
     is
     aligned
     with
     the
     north-south
     meridian,
     the
     earth’s
     rotation
     has
     no
     effect
     on
     the
     gyro’s
     horizontal
     axis.
     If
     it
     points
     east-west,
     the
     horizontal
     axis
     reads
     the
     earth
     rotation.
     Rate
     gyros
     have
     the
     same
     basic
     arrangement
     as
     shown
     in
     Figure
     4.4
     but
     with
     a
     slight
     modification.
     The
     gimbals
     are
     restrained
     by
     a
     torsional
     spring
     with
     additional
     viscous
     damping.
     This
     enables
     the
     sensor
     to
     measure
     angular
     speeds
     instead
     of
     absolute
     orientation.
     </p>
     <h5 class="likesubsubsectionHead">
      <a id="x3-18000">
      </a>
      Optical
     Gyroscopes
     </h5>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/optical-gyroscope.jpg" width="150%"/>
       <a id="x3-18001r3">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.3:
       </span>
       <span class="content">
        Optical Gyroscopes have no moving parts, (unlike mechanical gyroscopes) making them extremely reliable <span class="cite"><a href="MobileRoboticsLectureBookad1.html#findlight2025">[3]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      Optical
     gyroscopes
     are
     a
     relatively
     new
     innovation.
     Commercial
     use
     began
     in
     the
     early
     1980’s
     when
     they
     were
     first
     installed
     in
     aircraft.
     Optical
     gyroscopes
     are
     angular
     speed
     sen-
     sors
     that
     use
     two
     monochromatic
     light
     beams,
     or
     lasers,
     emitted
     from
     the
     same
     source
     in-
     stead
     of
     moving,
     mechanical
     parts.
     They
     work
     on
     the
     principle
     that
     the
     speed
     of
     light
     remains
     unchanged
     and,
     therefore,
     geometric
     change
     can
     cause
     light
     to
     take
     a
     varying
     amount
     of
     time
     to
     reach
     its
     destination.
     One
     laser
     beam
     is
     sent
     traveling
     clockwise
     through
     a
     fiber
     while
     the
     other
     travels
     counterclockwise.
     Because
     the
     laser
     traveling
     in
     the
     direction
     of
     rotation
     has
     a
     slightly
     shorter
     path,
     it
     will
     have
     a
     higher
     frequency.
     The
     difference
     in
     fre-
     quency
     of
     the
     two
     beams
     is
     a
     proportional
     to
     the
     angular
     velocity
     of
     the
     cylinder.
     New
     solid-state
     optical
     gyroscopes
     based
     on
     the
     same
     principle
     are
     build
     using
     microfabrication
     technology,
     thereby
     providing
     heading
     information
     with
     resolution
     and
     bandwidth
     far
     beyond
     the
     needs
     of
     mobile
     robotic
     applications.
     Bandwidth,
     for
     instance,
     can
     easily
     exceed
     100KHz
     while
     resolution
     can
     be
     smaller
     than
     0.0001ř/hr.
      <a id="subsubsection*.9">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-19000">
      </a>
      Ground
     Based
     Beacons
     </h5>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/ground-based-beacons.jpg" width="150%"/>
       <a id="x3-19001r4">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.4:
       </span>
       <span class="content">
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      One
     elegant
     approach
     to
     solving
     the
     localization
     problem
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      is
     to
     use
     active
     or
     passive
     beacons.
     Using
     the
     interaction
     of
     on-board
     sensors
     and
     the
     environmental
     bea-
     cons,
     the
     robot
     can
     identify
     its
     position
     precisely.
     Although
     the
     general
     intuition
     is
     identical
     to
     that
     of
     early
     human
     navigation
     beacons,
     such
     as
     stars,
     mountains
     and
     lighthouses,
     modern
     technology
     has
     enabled
     sensors
     to
     localize
     an
     outdoor
     robot
     with
     accuracies
     of
     better
     than
     5
     cm
     within
     areas
     that
     are
     kilometres
     in
     size.
     </p>
     <p class="noindent">
      In
     the
     following
     subsection,
     we
     describe
     one
     such
     beacon
     system,
     the
     Global
     Positioning
     System
     (GPS),
     which
     is
     extremely
     effective
     for
     outdoor
     ground-based
     and
     flying
     robots.
     In-
     door
     beacon
     systems
     have
     been
     generally
     less
     successful
     for
     a
     number
     of
     reasons.
     The
     ex-
     pense
     of
     environmental
     modification
     in
     an
     indoor
     setting
     is
     not
     amortized
     over
     an
     extremely
     large
     useful
     area,
     as
     it
     is
     for
     example
     in
     the
     case
     of
     GPS.
     Furthermore,
     indoor
     environments
     offer
     significant
     challenges
     not
     seen
     outdoors,
     including
     multipath
     and
     environment
     dynam-
     ics.
     A
     laser-based
     indoor
     beacon
     system,
     for
     example,
     must
     disambiguate
     the
     one
     true
     laser
     signal
     from
     possibly
     tens
     of
     other
     powerful
     signals
     that
     have
     reflected
     off
     of
     walls,
     smooth
     floors
     and
     doors.
     Confounding
     this,
     humans
     and
     other
     obstacles
     may
     be
     constantly
     changing
     the
     environment,
     for
     example
     occluding
     the
     one
     true
     path
     from
     the
     beacon
     to
     the
     robot.
     In
     commercial
     applications
     such
     as
     manufacturing
     plants,
     the
     environment
     can
     be
     carefully
     controlled
     to
     ensure
     success.
     In
     less
     structured
     indoor
     settings,
     beacons
     have
     nonetheless
     been
     used,
     and
     the
     problems
     are
     mitigated
     by
     careful
     beacon
     placement
     and
     the
     useful
     of
     passive
     sensing
     modalities.
      <a id="subsubsection*.10">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-20000">
      </a>
      Global
     Positioning
     System
     </h5>
     <p class="noindent">
      The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       Global
      Positioning
      System
      (GPS)
      </a>
      was
     initially
     developed
     for
     military
     use
     but
     is
     now
     freely
     available
     for
     civilian
     navigation.
     There
     are
     at
     least
     24
     operational
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      satellites
     at
     all
     times.
     The
     satellites
     orbit
     every
     12
     hours
     at
     a
     height
     of
     20.190km.
     There
     are
     four
     <alert style="color: #821131;">(4)</alert>
     satellites
     which
     located
     in
     each
     of
     six
     planes
     inclined
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           55
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mo class="MathClass-bin" stretchy="false">
          ∘
         </mo>
        </mrow>
       </msup>
      </math>
      with
     respect
     to
     the
     plane
     of
     the
     earth’s
     equator
     (figure
     4.5).
     </p>
     <p class="noindent">
      Each
     satellite
     continuously
     transmits
     data
     which
     indicates
     its
     location
     and
     the
     current
     time.
     Therefore,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      receivers
     are
      <alert style="color: #821131;">
       completely
passive
      </alert>
      but
      <alert style="color: #821131;">
       exteroceptive
      </alert>
      sensors.
     The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      satellites
     synchronise
     their
     transmissions
     to
     allow
     their
     signals
     to
     be
     sent
     at
     the
     same
     time.
     When
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      receiver
     reads
     the
     transmission
     of
     two
     <alert style="color: #821131;">(2)</alert>
     or
     more
     satellites,
     the
     arrival
     time
     differences
     inform
     the
     receiver
     as
     to
     its
     relative
     distance
     to
     each
     satellite.
     </p>
     <p class="noindent">
      By
     combining
     information
     regarding
     the
     arrival
     time
     and
     instantaneous
     location
     of
     four
     <alert style="color: #821131;">(4)</alert>
     satellites,
     the
     receiver
     can
     infer
     its
     own
     position.
     </p>
     <div class="warning">
      <p class="noindent">
       In
      theory,
      such
      triangulation
      requires
      only
      three
      <alert style="color: #821131;">(3)</alert>
      data
      points.
      However,
      timing
      is
      extremely
      critical
      in
      the
      GPS
      application
      because
      the
      time
      intervals
      being
      measured
      are
      in
      ns
      .
      </p>
     </div>
     <p class="noindent">
      It
     is,
     of
     course,
     mandatory
     the
     satellites
     to
     be
     well
     synchronised.
     To
     this
     end,
     they
     are
     updated
     by
     ground
     stations
     regularly
     and
     each
     satellite
     carries
     on-board
     atomic
     clocks
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Perception/raster/cesium-frequency-clock.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          6
         </sup>
        </span>
       </alert>
       <span id="textcolor8">
        An
       example
       of
       a
       csium
       clock
       for
       use
       in
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
         GPS
        </a>
        .
       </span>
      </span>
      for
     timing.
     The
     GPS
     receiver
     clock
     is
     also
     important
     so
     that
     the
     travel
     time
     of
     each
     satellite’s
     transmission
     can
     be
     accurately
     measured.
     But
     GPS
     receivers
     have
     a
     simple
     quartz
     clock.
     So,
     although
     3
     satellites
     would
     ideally
     provide
     position
     in
     three
     axes,
     the
     GPS
     receiver
     requires
     4
     satellites,
     using
     the
     additional
     information
     to
     solve
     for
     4
     variables:
     three
     position
     axes
     plus
     a
     time
     correction.
     The
     fact
     that
     the
     GPS
     receiver
     must
     read
     the
     transmission
     of
     4
     satellites
     simultaneously
     is
     a
     significant
     limitation.
     GPS
     satellite
     transmissions
     are
     extremely
     low-power,
     and
     reading
     them
     successfully
     requires
     direct
     line-of-sight
     communication
     with
     the
     satellite.
     Thus,
     in
     confined
     spaces
     such
     as
     city
     blocks
     with
     tall
     buildings
     or
     dense
     forests,
     one
     is
     unlikely
     to
     receive
     4
     satellites
     reliably.
     Of
     course,
     most
     indoor
     spaces
     will
     also
     fail
     to
     provide
     sufficient
     visibility
     of
     the
     sky
     for
     a
     GPS
     receiver
     to
     function.
     For
     these
     reasons,
     GPS
     has
     been
     a
     pop-
     ular
     sensor
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ,
     but
     has
     been
     relegated
     to
     projects
     involving
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      traversal
     of
     wide-open
     spaces
     and
     autonomous
     flying
     machines.
     A
     number
     of
     factors
     affect
     the
     performance
     of
     a
     localization
     sensor
     that
     makes
     use
     of
     GPS.
     First,
     it
     is
     important
     to
     understand
     that,
     because
     of
     the
     specific
     orbital
     paths
     of
     the
     GPS
     sat-
     ellites,
     coverage
     is
     not
     geometrically
     identical
     in
     different
     portions
     of
     the
     Earth
     and
     therefore
     resolution
     is
     not
     uniform.
     Specifically,
     at
     the
     North
     and
     South
     poles,
     the
     satellites
     are
     very
     close
     to
     the
     horizon
     and,
     thus,
     while
     resolution
     in
     the
     latitude
     and
     longitude
     directions
     is
     good,
     resolution
     of
     altitude
     is
     relatively
     poor
     as
     compared
     to
     more
     equatorial
     locations.
     </p>
     <p class="noindent">
      The
     second
     point
     is
     that
     GPS
     satellites
     are
     merely
     an
     information
     source.
     They
     can
     be
     employed
     with
     various
     strategies
     in
     order
     to
     achieve
     dramatically
     different
     levels
     of
     localisation
     resolution.
     The
     basic
     strategy
     for
     GPS
     use,
     called
     pseudorange
     and
     described
     above,
     generally
     performs
     at
     a
     resolution
     of
     15m.
     An
     extension
     of
     this
     method
     is
     differential
     GPS,
     which
     makes
     use
     of
     a
     second
     receiver
     that
     is
     static
     and
     at
     a
     known
     exact
     position.
     A
     number
     of
     errors
     can
     be
     corrected
     using
     this
     reference,
     and
     so
     resolution
     improves
     to
     the
     order
     of
     1m
     or
     less.
     A
     disadvantage
     of
     this
     technique
     is
     that
     the
     stationary
     receiver
     must
     be
     installed,
     its
     location
     must
     be
     measured
     very
     carefully
     and
     of
     course
     the
     moving
     robot
     must
     be
     within
     ki-
     lometers
     of
     this
     static
     unit
     in
     order
     to
     benefit
     from
     the
     DGPS
     technique.
     A
     further
     improved
     strategy
     is
     to
     take
     into
     account
     the
     phase
     of
     the
     carrier
     signals
     of
     each
     received
     satellite
     transmission.
     There
     are
     two
     carriers,
     at
     19cm
     and
     24cm,
     therefore
     signif-
     icant
     improvements
     in
     precision
     are
     possible
     when
     the
     phase
     difference
     between
     multiple
     satellites
     is
     measured
     successfully.
     Such
     receivers
     can
     achieve
     1cm
     resolution
     for
     point
     po-
     sitions
     and,
     with
     the
     use
     of
     multiple
     receivers
     as
     in
     DGPS,
     sub-1cm
     resolution.
     A
     final
     consideration
     for
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications
     is
     bandwidth.
     GPS
     will
     generally
     offer
     no
     better
     than
     200
     -
     300ms
     latency,
     and
     so
     one
     can
     expect
     no
     better
     than
     5Hz
     GPS
     updates.
     On
     a
     fast-moving
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      or
     flying
     robot,
     this
     can
     mean
     that
     local
     motion
     integration
     will
     be
     required
     for
     proper
     control
     due
     to
     GPS
     latency
     limitations.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-2" sec="section-2">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.1.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.2
      </small>
      <a id="x3-210001.2">
      </a>
      Active
     Ranging
     </h2>
     <p class="noindent">
      Active
     range
     sensors
     continue
     to
     be
     the
     most
     popular
     sensors
     used
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      .
     Many
     ranging
     sensors
     have
     a
     low
     price
     point,
     and
     most
     importantly
     all
     ranging
     sensors
     provide
     easily
     interpreted
     outputs:
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       Direct
      measurements
      of
      distance
      from
      the
      robot
      to
      objects
      in
      its
      vicinity.
      </p>
     </div>
     <p class="noindent">
      For
     obstacle
     detection
     and
     avoidance,
     most
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      rely
     heavily
     on
     active
     ranging
     sensors.
     But
     the
     local
     free-space
     information
     provided
     by
     range
     sensors
     can
     also
     be
     accumulated
     into
     representations
     beyond
     the
     robot’s
     current
     local
     reference
     frame.
     Therefore,
     active
     range
     sensors
     are
     also
     commonly
     found
     as
     part
     of
     the
     localisation
     and
     environmental
     modelling
     processes
     of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s.
     </p>
     <p class="noindent">
      It
     is
     only
     with
     the
     slow
     advent
     of
     successful
     visual
     interpretation
     competency
     that
     we
     can
     expect
     the
     class
     of
     active
     ranging
     sensors
     to
     gradually
     lose
     their
     primacy
     as
     the
     sensor
     class
     of
     choice
     among
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      engineers.
     </p>
     <p class="noindent">
      Below,
     we
     present
     two
     <alert style="color: #821131;">(2)</alert>
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       Time-of-Flight
      (ToF)
      </a>
      active
     range
     sensors:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        the
       ultrasonic
       sensor,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        the
       laser
       rangefinder.
       </p>
      </li>
     </ul>
     <p class="noindent">
      Continuing
     onwards,
     we
     then
     present
     two
     <alert style="color: #821131;">(2)</alert>
     geometric
     active
     range
     sensors:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        the
       optical
       triangulation
       sensor,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        the
       structured
       light
       sensor.
       </p>
      </li>
     </ul>
     <a id="subsubsection*.11">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x3-22000">
      </a>
      Time-of-FLight
     Active
     Ranging
     </h5>
     <p class="noindent">
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      ranging
     makes
     use
     of
     the
      <alert style="color: #821131;">
       propagation
speed
of
sound
      </alert>
      or
     an
      <alert style="color: #821131;">
       electromagnetic
wave
      </alert>
      .
     In
     general,
     the
     travel
     distance
     of
     a
     sound
     of
     electromagnetic
     wave
     is
     given
     by:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          d
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          c
         </mi>
         <mi>
          t
         </mi>
         <mo class="MathClass-punc" stretchy="false">
          ,
         </mo>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        d
       </mi>
      </math>
      is
     the
     distance
     travelled
     usually
     round-trip
     (
     m
     ),
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        c
       </mi>
      </math>
      the
     speed
     of
     wave
     propagation
     (
     ms
      <sup class="textsuperscript">
       <span class="ts1-lmr9-">
        −
       </span>
       1
      </sup>
      ),
     and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        t
       </mi>
      </math>
      is
     the
     time
     it
     takes
     to
     travel
     (
     s
     ).
     </p>
     <p class="noindent">
      It
     is
     important
     to
     point
     out
     the
     propagation
     speed
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        v
       </mi>
      </math>
      of
     sound
     is
     approximately
     0.3
     mms
      <sup class="textsuperscript">
       <span class="ts1-lmr9-">
        −
       </span>
       1
      </sup>
      whereas
     the
     speed
     of
     an
     electromagnetic
     signal
     is
     0.3
     mns
      <sup class="textsuperscript">
       <span class="ts1-lmr9-">
        −
       </span>
       1
      </sup>
      ,
     which
     is
     one
     million
     times
     faster.
     The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      for
     a
     typical
     distance,
     say
     3
     m
     ,
     is
     10
     ms
     for
     an
     ultrasonic
     system
     but
     only
     10
     ns
     for
     a
     laser
     rangefinder.
     It
     is
     therefore
     obvious
     that
     measuring
     the
     time
     of
     flight
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        t
       </mi>
      </math>
      with
     electromagnetic
     signals
     is
     more
     technologically
     challenging.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          7
         </sup>
        </span>
       </alert>
       <span id="textcolor13">
        This
       explains
       why
       laser
       range
       sensors
       have
       only
       recently
       become
       affordable
       and
       robust
       for
       use
       on
       mobile
       robots.
       </span>
      </span>
     </p>
     <p class="noindent">
      The quality of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      range sensors depends mainly on the following:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        Uncertainties
       in
       determining
       the
       exact
       time
       of
       arrival
       of
       the
       reflected
       signal,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Inaccuracies
       in
       the
       time
       of
       flight
       measurement,
       particularly
       with
       laser
       range
       sensors,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        The
       dispersal
       cone
       of
       the
       transmitted
       beam
       mainly
       with
       ultrasonic
       range
       sensors
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Interaction
       with
       the
       target
       (e.g.,
       surface
       absorption,
       specular
       reflections)
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Variation
       of
       propagation
       speed,
       and
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        The
       speed
       of
       the
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        and
       target
       (in
       the
       case
       of
       a
       dynamic
       target).
       </p>
      </li>
     </ul>
     <p class="noindent">
      As discussed below, each type of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      sensor is sensitive to a particular subset of the above list of factors.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.2.1
      </small>
      <a id="x3-230001.2.1">
      </a>
      The  Ultrasonic  Sensor
     </h3>
     <p class="noindent">
      The
     main
     ethos
     of
     an
     ultrasonic
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          8
         </sup>
        </span>
       </alert>
       <span id="textcolor20">
        Ultrasound
       is
       sound
       with
       frequencies
       greater
       than
       20
       kHz
       .
       </span>
      </span>
      sensor
     is
     to
     transmit
     a
     packet
     of
     ultrasonic
     pressure
     waves
     and
     to
      <alert style="color: #821131;">
       measure
the
time
it
takes
for
this
wave
to
reflect
and
return
to
the
receiver
      </alert>
      .
     The
     distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        d
       </mi>
      </math>
      of
     the
     object
     causing
     the
     reflection
     can
     be
     calculated
     based
     on
     the
     propagation
     speed
     of
     sound
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          9
         </sup>
        </span>
       </alert>
       <span id="textcolor21">
        Of
       course
       in
       this
       regard
       careful
       consideration
       needs
       to
       be
       made
       if
       the
       medium
       is
       significantly
       different
       than
       that
       of
       air
       (i.e.,
       water).
       </span>
      </span>
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        c
       </mi>
      </math>
      and
     the
     time
     of
     flight
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        t
       </mi>
      </math>
      .
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          d
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            c
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            ×
           </mo>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      The
     speed
     of
     sound
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            v
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      in
     air
     is
     given
     by
     the
     following
     relation:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          v
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msqrt>
          <mrow>
           <mi>
            γ
           </mi>
           <mi>
            R
           </mi>
           <mi>
            T
           </mi>
          </mrow>
         </msqrt>
        </math>
       </td>
      </tr>
     </table>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/ultrasonic-sensor-signals.png" width="150%"/>
       <a id="x3-23001r5">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.5:
       </span>
       <span class="content">
        Signals of an ultrasonic sensor.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        γ
       </mi>
      </math>
      is
     the
     ratio
     of
     specific
     heat,
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        R
       </mi>
      </math>
      is
     the
     gas
     constant
     (
     Jmol
      <sup class="textsuperscript">
       <span class="ts1-lmr9-">
        −
       </span>
       1
      </sup>
      K
      <sup class="textsuperscript">
       <span class="ts1-lmr9-">
        −
       </span>
       1
      </sup>
      ),
     and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        T
       </mi>
      </math>
      is
     the
     temperature
     in
     Kelvin
     (
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mstyle class="text">
        <mtext>
         K
        </mtext>
       </mstyle>
      </math>
      ).
     In
     air,
     at
     standard
     pressure,
     and
     20
      <msup>
       <mrow>
       </mrow>
       <mi>
        ∘
       </mi>
      </msup>
      C
     the
     speed
     of
     sound
     is
     approximately:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          v
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mn>
          3
         </mn>
         <mn>
          4
         </mn>
         <mn>
          3
         </mn>
         <mspace class="quad" width="1em">
         </mspace>
         <mstyle class="text">
          <mtext>
           ms
           <sup class="textsuperscript">
            −1
           </sup class="textsuperscript">
          </mtext>
         </mstyle>
         <mo class="MathClass-punc" stretchy="false">
          .
         </mo>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      We
     can
     see
     the
     different
     signal
     output
     and
     input
     of
     an
     ultrasonic
     sensor
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-23001r5">
       1.5
      </a>
      .
     </p>
     <p class="noindent">
      First,
     a
     series
     of
     sound
     pulses
     are
     emitted,
     which
     creates
     the
     wave
     packet.
     An
     integrator
     also
     begins
     to
      <alert style="color: #821131;">
       linearly
climb
      </alert>
      in
     value,
     measuring
     the
     time
     from
     the
     transmission
     of
     these
     sound
     waves
     to
     detection
     of
     an
     echo.
     A
     threshold
     value
     is
     set
     for
     triggering
     an
     incoming
     sound
     wave
     as
     a
     valid
     echo.
     </p>
     <div class="knowledge">
      <p class="noindent">
       This
      threshold
      is
      often
      decreasing
      in
      time,
      because
      the
      amplitude
      of
      the
      expected
      echo
      decreases
      over
      time
      based
      on
      dispersal
      as
      it
      travels
      longer.
      </p>
     </div>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/ultrasonic-sensor.jpg" width="100%"/>
      <a id="x3-23002r6">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.6:
       </span>
       <span class="content">
        An              example
       of an ultrasonic sensor
       used in Raspberry Pi
       applications <span class="cite"><a href="MobileRoboticsLectureBookad1.html#PiHut2025">[4]</a></span>.
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      But
     during
     transmission
     of
     the
     initial
     sound
     pulses
     and
     just
     afterwards,
     the
     threshold
     is
     set
     very
     high
     to
     suppress
     triggering
     the
     echo
     detector
     with
     the
     outgoing
     sound
     pulses.
     A
     transducer
     will
     continue
     to
     ring
     for
     up
     to
     several
     ms
     after
     the
     initial
     transmission,
     and
     this
     governs
     the
     blanking
     time
     of
     the
     sensor.
     </p>
     <div class="warning">
      <p class="noindent">
       If,
      during
      the
      blanking
      time,
      the
      transmitted
      sound
      were
      to
      reflect
      off
      of
      an
      extremely
      close
      object
      and
      return
      to
      the
      ultrasonic
      sensor,
      it
      may
      fail
      to
      be
      detected.
      </p>
     </div>
     <p class="noindent">
      However,
     once
     the
     blanking
     interval
     has
     passed,
     the
     system
     will
     detect
     any
     above-threshold
     reflected
     sound,
     triggering
     a
     digital
     signal
     and
     producing
     the
     distance
     measurement
     using
     the
     integrator
     value.
     </p>
     <p class="noindent">
      The
     ultrasonic
     wave
     typically
     has
     a
     frequency
     between
     40
     and
     180
     kHz
     and
     is
     usually
     generated
     by
     a
     piezo
     or
     electrostatic
     transducer.
     Often
     the
     same
     unit
     is
     used
     to
     measure
     the
     reflected
     signal,
     although
     the
     required
     blanking
     interval
     can
     be
     reduced
     through
     the
     use
     of
     separate
     output
     and
     input
     devices.
     Frequency
     can
     be
     used
     to
     select
     a
     useful
     range
     when
     choosing
     the
     appropriate
     ultrasonic
     sensor
     for
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      .
     Lower
     frequencies
     correspond
     to
     a
     longer
     range,
     but
     with
     the
     disadvantage
     of
     longer
     post-transmission
     ringing
     and,
     therefore,
     the
     need
     for
     longer
     blanking
     intervals.
     </p>
     <p class="noindent">
      Most
     ultrasonic
     sensors
     used
     by
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     have
     an
     effective
     range
     of
     roughly
     12
     cm
     to
     5
     metres.
     The
     published
     accuracy
     of
     commercial
     ultrasonic
     sensors
     varies
     between
     98%
     and
     99.1%.
     In
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications,
     specific
     implementations
     generally
     achieve
     a
     resolution
     of
     approximately
     2
     cm
     .
     </p>
     <p class="noindent">
      In
     most
     cases
     one
     may
     want
     a
     narrow
     opening
     angle
     for
     the
     sound
     beam
     in
     order
     to
     also
     obtain
     precise
     directional
     information
     about
     objects
     that
     are
     encountered.
     This
     is
     a
     major
     limitation
     since
     sound
     propagates
     in
     a
     cone-like
     manner
     with
     opening
     angles
     around
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           20
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mo class="MathClass-bin" stretchy="false">
          ∘
         </mo>
        </mrow>
       </msup>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msup>
        <mrow>
         <mstyle class="text">
          <mtext>
           40
          </mtext>
         </mstyle>
        </mrow>
        <mrow>
         <mo class="MathClass-bin" stretchy="false">
          ∘
         </mo>
        </mrow>
       </msup>
      </math>
      .
     Consequently,
     when
     using
     ultrasonic
     ranging
     one
     does
     not
     acquire
     depth
     data
     points
     but,
     rather,
     entire
     regions
     of
     constant
     depth.
     This
     means
     that
     the
     sensor
     tells
     us
     only
     that
     there
     is
     an
     object
     at
     a
     certain
     distance
     in
     within
     the
     area
     of
     the
     measurement
     cone.
     The
     sensor
     readings
     must
     be
     plotted
     as
     segments
     of
     an
     arc
     (sphere
     for
     3D)
     and
     not
     as
     point
     measurements.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         10
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Perception/raster/sound-arc.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          10
         </sup>
        </span>
       </alert>
       <span id="textcolor22">
        The
       results
       of
       a
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <msup>
          <mrow>
           <mstyle class="text">
            <mtext>
             360
            </mtext>
           </mstyle>
          </mrow>
          <mrow>
           <mo class="MathClass-bin" stretchy="false">
            ∘
           </mo>
          </mrow>
         </msup>
        </math>
        scan
       of
       a
       room.
       </span>
      </span>
      However,
     recent
     research
     developments
     show
     significant
     improvement
     of
     the
     measurement
     quality
     in
     using
     sophisticated
     echo
     processing.
     Ultrasonic
     sensors
     suffer
     from
     several
     additional
     drawbacks,
     namely
     in
     the
     areas
     of
      <alert style="color: #821131;">
       error
      </alert>
      ,
      <alert style="color: #821131;">
       bandwidth
      </alert>
      and
      <alert style="color: #821131;">
       cross-sensitivity
      </alert>
      .
     The
     published
     accuracy
     values
     for
     ultrasonic
     sensors
     are
     nominal
     values
     based
     on
     successful,
     perpendicular
     reflections
     of
     the
     sound
     wave
     off
     an
     acoustically
     reflective
     material.
     </p>
     <p class="noindent">
      This
     does
     not
     capture
     the
     effective
     error
     modality
     seen
     on
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      moving
     through
     its
     environment.
     As
     the
     ultrasonic
     transducer’s
     angle
     to
     the
     object
     being
     ranged
     varies
     away
     from
     perpendicular,
     the
     chances
     become
     good
     that
     the
     sound
     waves
     will
     coherently
     reflect
     away
     from
     the
     sensor,
     just
     as
     light
     at
     a
     shallow
     angle
     reflects
     off
     of
     a
     mirror.
     Therefore,
     the
     true
     error
     behavior
     of
     ultrasonic
     sensors
     is
     compound,
     with
     a
     well-understood
     error
     distribution
     near
     the
     true
     value
     in
     the
     case
     of
     a
     successful
     retro-reflection,
     and
     a
     more
     poorly-understood
     set
     of
     range
     values
     that
     are
     grossly
     larger
     than
     the
     true
     value
     in
     the
     case
     of
     coherent
     reflection.
     </p>
     <p class="noindent">
      Of
     course
     the
     acoustic
     properties
     of
     the
     material
     being
     ranged
     have
     direct
     impact
     on
     the
     sensor’s
     performance.
     Again,
     the
     impact
     is
     discrete,
     with
     one
     material
     possibly
     failing
     to
     produce
     a
     reflection
     that
     is
     sufficiently
     strong
     to
     be
     sensed
     by
     the
     unit.
     For
     example,
     foam,
     fur
     and
     cloth
     can,
     in
     various
     circumstances,
     acoustically
     absorb
     the
     sound
     waves.
     A
     final
     limitation
     for
     ultrasonic
     ranging
     relates
     to
     bandwidth.
     Particularly
     in
     moderately
     open
     spaces,
     a
     single
     ultrasonic
     sensor
     has
     a
     relatively
     slow
     cycle
     time.
     </p>
     <p class="noindent">
      For
     example,
     measuring
     the
     distance
     to
     an
     object
     that
     is
     3
     m
     away
     will
     take
     such
     a
     sensor
     20ms,
     limiting
     its
     operating
     speed
     to
     50
     Hz.
     But
     if
     the
     robot
     has
     a
     ring
     of
     20
     ultrasonic
     sensors,
     each
     firing
     sequentially
     and
     measuring
     to
     minimize
     interference
     between
     the
     sensors,
     then
     the
     ring’s
     cycle
     time
     becomes
     0.4s
     and
     the
     overall
     update
     frequency
     of
     any
     one
     sensor
     is
     just
     2.5
     Hz.
     For
     a
     robot
     conducting
     moderate
     speed
     motion
     while
     avoiding
     obstacles
     using
     ultrasonic
     sensor,
     this
     update
     rate
     can
     have
     a
     measurable
     impact
     on
     the
     maximum
     speed
     possible
     while
     still
     sensing
     and
     avoiding
     obstacles
     safely.
     </p>
     <div class="warning">
      <p class="noindent">
       Ultrasonic measurements may be limited through barrier layers with large salinity, temperature or vortex differentials.
      </p>
     </div>
     <a id="subsubsection*.12">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x3-24000">
      </a>
      Laser  Rangefinder
     </h5>
     <p class="noindent">
      The
     laser
     rangefinder
     is
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      sensor
     which
     achieves
     significant
     improvements
     over
     the
     ultrasonic
     range
     sensor
     due
     to
     the
      <alert style="color: #821131;">
       use
of
laser
light
instead
of
sound
      </alert>
      .
     This
     type
     of
     sensor
     consists
     of
     a
     transmitter
     which
     illuminates
     a
     target
     with
     a
     collimated
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         11
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          11
         </sup>
        </span>
       </alert>
       <span id="textcolor23">
        meaning
       all
       the
       rays
       in
       questions
       are
       made
       accurately
       parallel.
       </span>
      </span>
      beam
     (e.g.
     laser),
     and
     a
     receiver
     capable
     of
     detecting
     the
     component
     of
     light
     which
     is
     essentially
     coaxial
     with
     the
     transmitted
     beam.
     Often
     referred
     to
     as
     optical
     radar
     or
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:lidar">
       Light
      Detection
      and
      Ranging
      (LIDAR)
      </a>
      ,
     these
     devices
     produce
     a
     range
     estimate
     based
     on
     the
     time
     needed
     for
     the
     light
     to
     reach
     the
     target
     and
     return.
     </p>
     <p class="noindent">
      A
     mechanical
     mechanism
     with
     a
     mirror
     sweeps
     the
     light
     beam
     to
     cover
     the
     required
     scene
     in
     a
     plane
     or
     even
     in
     3
     dimensions,
     using
     a
     rotating
     mirror.
     One
     way
     to
     measure
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:tof">
       ToF
      </a>
      for
     the
     light
     beam
     is
     to
     use
     a
     pulsed
     laser
     and
     then
     measured
     the
     elapsed
     time
     directly,
     just
     as
     in
     the
     ultrasonic
     solution
     described
     in
     just
     a
     little
     bit.
     Electronics
     capable
     of
     resolving
     ps
     are
     required
     in
     such
     devices
     and
     they
     are
     therefore
     very
     expensive.
     A
     second
     method
     is
     to
     measure
     the
     beat
     frequency
     between
     a
     frequency
     modulated
     continuous
     wave
     and
     its
     received
     reflection.
     Another,
     even
     easier
     method
     is
     to
     measure
     the
     phase
     shift
     of
     the
     reflected
     light.
      <a id="paragraph*.13">
      </a>
     </p>
     <p class="noindent">
      <span class="paragraphHead">
       <a id="x3-25000">
       </a>
       Continuous  Wave  Radar
      </span>
      It is a type of radar system where a known stable frequency continuous wave radio energy is transmitted and then received from
     any reflecting objects. Individual objects can be detected using the Doppler effect, which causes the received signal to have a
     different frequency from the transmitted signal, allowing it to be detected by filtering out the transmitted frequency.
     </p>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/laser-range-finder.jpg" width="100%"/>
      <a id="x3-25001r7">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.7:
       </span>
       <span class="content">
        A       laser       range
       finder used in robotics
       applications
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      Doppler-analysis of radar returns can allow the filtering out of slow or non-moving objects, thus offering immunity to interference from
     large stationary objects and slow-moving clutter. This makes it particularly useful for looking for objects against a background
     reflector, for instance, allowing a high-flying aircraft to look for aircraft flying at low altitudes against the background of the surface.
     Because the very strong reflection off the surface can be filtered out, the much smaller reflection from a target can still be seen.
      <a id="paragraph*.14">
      </a>
     </p>
     <p class="noindent">
      <span class="paragraphHead">
       <a id="x3-26000">
       </a>
       Phase  Shift  Measurement
      </span>
      Near infrared light, which could be from an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:led">
       Light-Emitting Diode (LED)
      </a>
      or a laser, is collimated and transmitted from the transmitter
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        T
       </mi>
      </math>
      in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-26001r8">
       1.8
      </a>
      and hits a
     point
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
      </math>
      in the environment.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/phase-shift-measurement.png" width="150%"/>
       <a id="x3-26001r8">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.8:
       </span>
       <span class="content">
        Schematic of laser rangefinding by phase-shift measurement.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      For
     surfaces
     having
     a
     roughness
     greater
     than
     the
     wavelength
     of
     the
     incident
     light,
     diffuse
     reflection
     will
     occur,
     meaning
     that
     the
     light
     is
     reflected
     almost
     isotropically
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          12
         </sup>
        </span>
       </alert>
       <span id="textcolor24">
        Something
       that
       is
       isotropic
       has
       the
       same
       size
       or
       physical
       properties
       when
       it
       is
       measured
       in
       different
       directions
       </span>
      </span>
      .
     The
     wavelength
     of
     the
     infrared
     light
     emitted
     is
     824
     nm
     and
     so
     most
     surfaces
     with
     the
     exception
     of
     only
     highly
     polished
     reflecting
     objects,
     will
     be
     diffuse
     reflectors.
     The
     component
     of
     the
     infrared
     light
     which
     falls
     within
     the
     receiving
     aperture
     of
     the
     sensor
     will
     return
     almost
     parallel
     to
     the
     transmitted
     beam,
     for
     distant
     objects.
     The
     sensor
     transmits
     100%
     amplitude
     modulated
     light
     at
     a
     known
     frequency
     and
     measures
     the
     phase
     shift
     between
     the
     transmitted
     and
     reflected
     signals.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/range-estimation.png" width="150%"/>
       <a id="x3-26002r9">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.9:
       </span>
       <span class="content">
        Range estimation by measuring the phase shift between transmitted and received signals.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-26002r9">
       1.9
      </a>
      shows how this technique can be used to measure range. The wavelength of the modulating signal obeys the equation
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        c
       </mi>
       <mo class="MathClass-rel" stretchy="false">
        =
       </mo>
       <mi>
        f
       </mi>
       <mi>
        λ
       </mi>
      </math>
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        c
       </mi>
      </math>
      is the speed of light and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        f
       </mi>
      </math>
      the modulating frequency.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example,
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         f
        </mi>
       </math>
       =
      5
      MHz
      ,
      the
      wavelength
      is
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         λ
        </mi>
       </math>
       =
      60
      m
      .
      </p>
     </div>
     <p class="noindent">
      The total distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          D
         </mi>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
         <mi>
          ′
         </mi>
        </mrow>
       </msubsup>
      </math>
      covered by the emitted light is:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <msubsup>
          <mrow>
           <mi>
            D
           </mi>
          </mrow>
          <mrow>
          </mrow>
          <mrow>
           <mi>
            ′
           </mi>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          L
         </mi>
         <mo class="MathClass-bin" stretchy="false">
          +
         </mo>
         <mn>
          2
         </mn>
         <mi>
          D
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          L
         </mi>
         <mfrac>
          <mrow>
           <mi>
            𝜃
           </mi>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            π
           </mi>
          </mrow>
         </mfrac>
         <mi>
          λ
         </mi>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        D
       </mi>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        L
       </mi>
      </math>
      are the distances defined in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-26001r8">
       1.8
      </a>
      . The required distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        D
       </mi>
      </math>
      ,
     between the beam splitter and the target, is therefore given by:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          D
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            λ
           </mi>
          </mrow>
          <mrow>
           <mn>
            4
           </mn>
           <mi>
            π
           </mi>
          </mrow>
         </mfrac>
         <mi>
          𝜃
         </mi>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        𝜃
       </mi>
      </math>
      is the electronically measured phase difference between the transmitted and reflected light beams, and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        λ
       </mi>
      </math>
      the known modulating
     wavelength. It can be seen that the transmission of a single frequency modulated wave can theoretically result in ambiguous range estimates since
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example
      if
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         λ
        </mi>
        <mo class="MathClass-rel" stretchy="false">
         =
        </mo>
        <mn>
         6
        </mn>
        <mn>
         0
        </mn>
        <mstyle class="text">
         <mtext>
          m
         </mtext>
        </mstyle>
       </math>
       ,
      a
      target
      at
      a
      range
      of
      5
      m
      would
      give
      an
      indistinguishable
      phase
      measurement
      from
      a
      target
      at
      65
      m
      ,
      since
      each
      phase
      angle
      would
      be
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msup>
         <mrow>
          <mstyle class="text">
           <mtext>
            360
           </mtext>
          </mstyle>
         </mrow>
         <mrow>
          <mo class="MathClass-bin" stretchy="false">
           ∘
          </mo>
         </mrow>
        </msup>
       </math>
       apart.
      </p>
     </div>
     <p class="noindent">
      We therefore define an
      <alert style="color: #821131;">
       ambiguity interval
      </alert>
      of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        λ
       </mi>
      </math>
      ,
     but in practice we note that the range of the sensor is much lower than
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        λ
       </mi>
      </math>
      due to the attenuation of the signal in air. It can be shown that the confidence in the range (phase estimate)
     is inversely proportional to the square of the received signal amplitude, directly affecting the sensor’s
     accuracy. Hence dark, distant objects will not produce as good range estimates as close, bright objects.
     </p>
     <p class="noindent">
      As with ultrasonic ranging sensors, an important error mode involves coherent reflection of the energy. With light, this will only
     occur when striking a highly polishes surface. Practically, a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      may encounter such surfaces in the form of a polished desktop,
     file cabinet or of course a mirror. Unlike ultrasonic sensors, laser rangefinders cannot detect the presence of optically transparent
     materials such as glass, and this can be a significant obstacle in environments, for example museums, where glass is commonly used.
      <a id="subsubsection*.15">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-27000">
      </a>
      Triangulation-based  Active  Ranging
     </h5>
     <p class="noindent">
      Triangulation-based ranging sensors use geometrical properties in their measuring strategy to establish distance readings to
     objects. The simplest class of triangulation-based rangers are active because they project a known light pattern (e.g., a point, a
     line or a texture) onto the environment. The reflection of the known pattern is captured by a receiver and, together with known
     geometric values, the system can use simple triangulation to establish range measurements. If the receiver measures the
     position of the reflection along a single axis, we call the sensor an optical triangulation sensor in 1D. If the receiver
     measures the position of the reflection along two orthogonal axes, we call the sensor a structured light sensor.
      <a id="subsubsection*.16">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-28000">
      </a>
      Optical  Triangulation  (1D  Sensor)
     </h5>
     <p class="noindent">
      The principle of optical triangulation in 1D is straightforward, as depicted in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-28001r10">
       1.10
      </a>
      .
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/single-dimension-laser.png" width="150%"/>
       <a id="x3-28001r10">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.10:
       </span>
       <span class="content">
        Principle of 1D laser triangulation.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      A collimated beam is transmitted toward the target. The reflected light is collected by a lens and projected onto a position sensitive
     device
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Perception/raster/position-sensing-device.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          13
         </sup>
        </span>
       </alert>
       <span id="textcolor25">
        A position sensitive device and/or position sensitive detector is an optical position sensor which can measure a
       position of a light spot in one or two-dimensions on a sensor surface.
       </span>
      </span>
      or linear camera. Given the geometry of
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-28001r10">
       1.10
      </a>
      the distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        D
       </mi>
      </math>
      is given by:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          D
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          f
         </mi>
         <mfrac>
          <mrow>
           <mi>
            L
           </mi>
          </mrow>
          <mrow>
           <mi>
            x
           </mi>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      The distance is proportional to
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mfrac>
        <mrow>
         <mn>
          1
         </mn>
        </mrow>
        <mrow>
         <mi>
          x
         </mi>
        </mrow>
       </mfrac>
      </math>
      ,
     therefore the sensor resolution is best for close objects and becomes worse as distance increases. Sensors based on this
     principle are used in range sensing up to one or two
     m
     , but also in high precision industrial measurements with
     resolutions far below one
     ţ
     m
     . Optical triangulation devices can provide relatively high accuracy with very good
     resolution for close objects. However, the operating range of such a device is normally fairly limited by
      <alert style="color: #821131;">
       geometry
      </alert>
      . For
     example, an off-the-shelf optical triangulation sensor can operate over a distance range of between 8
     cm
     and 80
     cm
     .
     </p>
     <div class="knowledge">
      <p class="noindent">
       It is inexpensive compared to ultrasonic and laser rangefinder sensors.
      </p>
     </div>
     <p class="noindent">
      Although more limited in range than sonar, the optical triangulation sensor has high bandwidth
     and does not suffer from cross-sensitivities that are more common in the sound domain.
      <a id="subsubsection*.17">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-29000">
      </a>
      Structured  Light  (2D  Sensor)
     </h5>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/structured-light-sensor.jpg" width="100%"/>
      <a id="x3-29001r11">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.11:
       </span>
       <span class="content">
        Structured       light
       sources on display at
       the
       2014 Machine Vision
       Show in Boston <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Reinhold2014">[5]</a></span>.
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      If one replaced the linear camera or
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:psd">
       Position Sensing Device (PSD)
      </a>
      of an optical triangulation sensor with a two-dimensional
     receiver such as a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      or
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       Complimentary MOS (CMOS)
      </a>
      camera, then one can recover distance to a large set of
     points instead of to only one point. The emitter must project a known pattern, or structured light, onto the
     environment. Many systems exist which either project light textures, which can be seen in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-29003r12">
       1.12
      </a>
      , or emit
     collimated light by means of a rotating mirror. Yet another popular alternative is to project a laser stripe by turning a
     laser beam into a plane using a prism. Regardless of how it is created, the projected light has a known structure,
     and therefore the image taken by the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      or
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      receiver can be filtered to identify the pattern’s reflection.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/2d-light-structure.png" width="150%"/>
       <a id="x3-29003r12">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.12:
       </span>
       <span class="content">
        a) Principle of active two dimensional triangulation b) Other possible light structures c) One-dimensional
       schematic of the principle
       </span>
      </figcaption>
     </div>
     <div class="warning">
      <p class="noindent">
       The problem of recovering depth here far simpler than the problem of passive image analysis.
      </p>
     </div>
     <p class="noindent">
      In
     passive
     image
     analysis,
     as
     we
     discuss
     later,
     existing
     features
     in
     the
     environment
     must
     be
     used
     to
     perform
     correlation,
     while
     the
     present
     method
     projects
     a
      <alert style="color: #821131;">
       known
pattern
upon
the
environment
      </alert>
      and
     thereby
     avoids
     the
     standard
     correlation
     problem
     altogether.
     Furthermore,
     the
     structured
     light
     sensor
     is
     an
     active
     device;
     so,
     it
     will
     continue
     to
     work
     in
     dark
     environments
     as
     well
     as
     environments
     in
     which
     the
     objects
     are
     featureless
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          14
         </sup>
        </span>
       </alert>
       <span id="textcolor26">
        e.g.,
       uniformly
       coloured
       and
       possess
       no
       visible
       edge.
       </span>
      </span>
      .
     In
     contrast,
     stereo
     vision
     would
     fail
     in
     such
     texture-free
     circumstances.
     Figure
     4.15c
     shows
     a
     one-dimensional
     active
     triangulation
     geometry.
     We
     can
     examine
     the
     trade-off
     in
     the
     design
     of
     triangulation
     systems
     by
     examining
     the
     geometry
     in
     figure
     4.15c.
     The
     measured
     values
     in
     the
     system
     are
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        α
       </mi>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        u
       </mi>
      </math>
      ,
     the
     distance
     of
     the
     illuminated
     point
     from
     the
     origin
     in
     the
     imaging
     sensor.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          15
         </sup>
        </span>
       </alert>
       <span id="textcolor27">
        The
       imaging
       sensor
       here
       can
       be
       a
       camera
       or
       an
       array
       of
       photo
       diodes
       of
       a
       position
       sensitive
       device
       (e.g.
       a
       2D
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:psd">
         PSD
        </a>
       </span>
      </span>
      From
     figure
     4.15c,
     simple
     geometry
     shows
     that:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          x
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            a
           </mi>
           <mi>
            u
           </mi>
          </mrow>
          <mrow>
           <mi>
            f
           </mi>
           <mi class="loglike">
            cot
           </mi>
           <mo>
            ⁡
           </mo>
           <mi>
            α
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mi>
            u
           </mi>
          </mrow>
         </mfrac>
         <mspace class="qquad" width="2em">
         </mspace>
         <mstyle class="text">
          <mtext>
           and
          </mtext>
         </mstyle>
         <mspace class="qquad" width="2em">
         </mspace>
         <mi>
          z
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            a
           </mi>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            f
           </mi>
           <mi class="loglike">
            cot
           </mi>
           <mo>
            ⁡
           </mo>
           <mi>
            α
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mi>
            u
           </mi>
          </mrow>
         </mfrac>
         <mo class="MathClass-punc" stretchy="false">
          .
         </mo>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        f
       </mi>
      </math>
      is
     the
     distance
     of
     the
     lens
     to
     the
     imaging
     plane.
     In
     the
     limit,
     the
     ratio
     of
     image
     resolution
     to
     range
     resolution
     is
     defined
     as
     the
     triangulation
     gain
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          G
         </mi>
        </mrow>
        <mrow>
         <mi>
          p
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      and
     from
     equation
     4.12
     is
     given
     by:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle displaystyle="true">
          <mfrac>
           <mrow>
            <msup>
             <mrow>
              <mi>
               ∂
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mi>
             u
            </mi>
           </mrow>
           <mrow>
            <mi>
             ∂
            </mi>
            <msup>
             <mrow>
              <mi>
               z
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msup>
           </mrow>
          </mfrac>
         </mstyle>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msubsup>
          <mrow>
           <mi>
            G
           </mi>
          </mrow>
          <mrow>
           <mi>
            p
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            a
           </mi>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <msubsup>
            <mrow>
             <mi>
              z
             </mi>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mn>
              2
             </mn>
            </mrow>
           </msubsup>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      This
     shows
     that
     the
     ranging
     accuracy,
     for
     a
     given
     image
     resolution,
     is
     proportional
     to
     source/
     detector
     separation
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        b
       </mi>
      </math>
      and
     focal
     length
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        f
       </mi>
      </math>
      ,
     and
     decreases
     with
     the
     square
     of
     the
     range
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        z
       </mi>
      </math>
      .
     In
     a
     scanning
     ranging
     system,
     there
     is
     an
     additional
     effect
     on
     the
     ranging
     accuracy,
     caused
     by
     the
     measurement
     of
     the
     projection
     angle
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        α
       </mi>
      </math>
      .
     From
     equation
     4.12
     we
     see
     that:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle displaystyle="true">
          <mfrac>
           <mrow>
            <msup>
             <mrow>
              <mi>
               ∂
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mi>
             α
            </mi>
           </mrow>
           <mrow>
            <mi>
             ∂
            </mi>
            <msup>
             <mrow>
              <mi>
               z
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msup>
           </mrow>
          </mfrac>
         </mstyle>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msubsup>
          <mrow>
           <mi>
            G
           </mi>
          </mrow>
          <mrow>
           <mi>
            α
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            b
           </mi>
           <mi class="loglike">
            sin
           </mi>
           <mo>
            ⁡
           </mo>
           <msubsup>
            <mrow>
             <mi>
              α
             </mi>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mn>
              2
             </mn>
            </mrow>
           </msubsup>
          </mrow>
          <mrow>
           <msubsup>
            <mrow>
             <mi>
              z
             </mi>
            </mrow>
            <mrow>
            </mrow>
            <mrow>
             <mn>
              2
             </mn>
            </mrow>
           </msubsup>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      We
     can
     summarise
     the
     effects
     of
     the
     parameters
     on
     the
     sensor
     accuracy
     as
     follows:
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Baseline Length (b)
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        the
       smaller
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          b
         </mi>
        </math>
        is
       the
       more
       compact
       the
       sensor
       can
       be.The
       larger
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          b
         </mi>
        </math>
        is
       the
       better
       the
       range
       resolution
       will
       be.
       Note
       also
       that
       although
       these
       sensors
       do
       not
       suffer
       from
       the
       correspondence
       problem,
       the
       disparity
       problem
       still
       occurs.
       As
       the
       baseline
       length
       b
       is
       increased,
       one
       introduces
       the
       chance
       that,
       for
       close
       objects,
       the
       illuminated
       point(s)
       may
       not
       be
       in
       the
       receiver’s
       field
       of
       view.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Detector length and focal length f
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        A
       larger
       detector
       length
       can
       provide
       either
       a
       larger
       field
       of
       view
       or
       an
       improved
       range
       resolution
       or
       partial
       benefits
       for
       both.
       Increasing
       the
       detector
       length
       however
       means
       a
       larger
       sensor
       head
       and
       worse
       electrical
       characteristics
       (increase
       in
       random
       error
       and
       reduction
       of
       bandwidth).
       Also,
       a
       short
       focal
       length
       gives
       a
       large
       field
       of
       view
       at
       the
       expense
       of
       accuracy
       and
       vice
       versa.
       </p>
      </dd>
     </dl>
     <p class="noindent">
      At
     one
     time,
     laser
     stripe-based
     structured
     light
     sensors
     were
     common
     on
     several
     mobile
     robot
     bases
     as
     an
     inexpensive
     alternative
     to
     laser
     range-finding
     devices.
     However,
     with
     the
     in-
     creasing
     quality
     of
     laser
     range-finding
     sensors
     in
     the
     1990’s
     the
     structured
     light
     system
     has
     become
     relegated
     largely
     to
     vision
     research
     rather
     than
     applied
     mobile
     robotics.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.2.2
      </small>
      <a id="x3-300001.2.2">
      </a>
      Motion
     and
     Speed
     Sensors
     </h3>
     <p class="noindent">
      Some
     sensors
     directly
     measure
     the
     relative
     motion
     between
     the
     robot
     and
     its
     environment.
     Since
     such
     motion
     sensors
     detect
      <alert style="color: #821131;">
       relative
motion
      </alert>
      ,
     so
     long
     as
     an
     object
     is
     moving
     relative
     to
     the
     robot’s
     reference
     frame,
     it
     will
     be
     detected
     and
     its
     speed
     can
     be
     estimated.
     There
     are
     a
     number
     of
     sensors
     that
     inherently
     measure
     some
     aspect
     of
     motion
     or
     change.
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example,
      a
      pyroelectric
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          16
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <img alt="PIC" height="" src="figures/Perception/raster/pyroelectric-detectors.jpg" width="100%"/>
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           16
          </sup>
         </span>
        </alert>
        <span id="textcolor28">
         An
        example
        of
        a
        pyroelectric
        sensor.
        </span>
       </span>
       sensor
      detects
      change
      in
      heat.
      </p>
     </div>
     <p class="noindent">
      When
     someone
     walks
     across
     the
     sensor’s
     field
     of
     view,
     his
     motion
     triggers
     a
     change
     in
     heat
     in
     the
     sensor’s
     reference
     frame.
     In
     the
     next
     subsection,
     we
     describe
     an
     important
     type
     of
     motion
     detector
     based
     on
     the
      <alert style="color: #821131;">
       Doppler
effect
      </alert>
      .
     These
     sensors
     represent
     a
     well-known
     technology
     with
     decades
     of
     general
     applications
     behind
     them.
     </p>
     <div class="knowledge">
      <p class="noindent">
       For
      fast-moving
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       s
      such
      as
      autonomous
      highway
      vehicles
      and
      unmanned
      flying
      vehicles,
      Doppler-based
      motion
      detectors
      are
      the
      obstacle
      detection
      sensor
      of
      choice.
      </p>
     </div>
     <a id="subsubsection*.18">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x3-31000">
      </a>
      Doppler
     Effect
     </h5>
     <p class="noindent">
      Anyone
     who
     has
     noticed
     the
     change
     in
     siren
     pitch
     when
     an
     ambulance
     approaches
     and
     then
     passes
     by
     is
     familiar
     with
     the
     Doppler
     effect.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         17
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          17
         </sup>
        </span>
       </alert>
       <span id="textcolor29">
        For
       anyone
       who
       needs
       a
       bit
       more
       information,
       it
       is
       the
       change
       in
       the
       frequency
       of
       a
       wave
       in
       relation
       to
       an
       observer
       who
       is
       moving
       relative
       to
       the
       source
       of
       the
       wave.
       The
       Doppler
       effect
       is
       named
       after
       the
       physicist
       Christian
       Doppler,
       who
       described
       the
       phenomenon
       in
       1842.
       A
       common
       example
       of
       Doppler
       shift
       is
       the
       change
       of
       pitch
       heard
       when
       a
       vehicle
       sounding
       a
       horn
       approaches
       and
       recedes
       from
       an
       observer.
       Compared
       to
       the
       emitted
       frequency,
       the
       received
       frequency
       is
       higher
       during
       the
       approach,
       identical
       at
       the
       instant
       of
       passing
       by,
       and
       lower
       during
       the
       recession.
       </span>
      </span>
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/doppler-effect.png" width="150%"/>
       <a id="x3-31001r13">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.13:
       </span>
       <span class="content">
        Doppler effect between two moving objects (a) or a moving and a stationary object(b)
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      A
     transmitter
     emits
     an
     electromagnetic
     or
     sound
     wave
     with
     a
     frequency
     f
     t.
     It
     is
     either
     received
     by
     a
     receiver
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-31001r13">
       1.13
      </a>
      (a)
     or
     reflected
     from
     an
     object
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-31001r13">
       1.13
      </a>
      (b).
     The
     measured
     frequency
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          f
         </mi>
        </mrow>
        <mrow>
         <mi>
          r
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      at
     the
     receiver
     is
     a
     function
     of
     the
     relative
     speed
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        v
       </mi>
      </math>
      between
     transmitter
     and
     receiver
     according
     to
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            r
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mfrac>
          <mrow>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mn>
            1
           </mn>
           <mo class="MathClass-bin" stretchy="false">
            +
           </mo>
           <mstyle displaystyle="true">
            <mfrac>
             <mrow>
              <mi>
               υ
              </mi>
             </mrow>
             <mrow>
              <mi>
               c
              </mi>
             </mrow>
            </mfrac>
           </mstyle>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      if
     the
     transmitter
     is
     moving
     and
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            r
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mn>
              1
             </mn>
             <mo class="MathClass-bin" stretchy="false">
              +
             </mo>
             <mfrac>
              <mrow>
               <mi>
                υ
               </mi>
              </mrow>
              <mrow>
               <mi>
                c
               </mi>
              </mrow>
             </mfrac>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      if
     the
     receiver
     is
     moving.
     In
     the
     case
     of
     a
     reflected
     wave
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-31001r13">
       1.13
      </a>
      (b)
     there
     is
     a
     factor
     of
     two
     introduced,
     since
     any
     change
     x
     in
     relative
     separation
     affects
     the
     round-trip
     path
     length
     by
     2
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        x
       </mi>
      </math>
      .
     </p>
     <div class="knowledge">
      <p class="noindent">
       In
      such
      situations
      it
      is
      generally
      more
      convenient
      to
      consider
      the
      change
      in
      frequency
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi mathvariant="normal">
         Δ
        </mi>
        <mi>
         f
        </mi>
       </math>
       ,
      known
      as
      the
      Doppler
      shift,
      as
      opposed
      to
      the
      Doppler
      frequency
      notation
      above.
      </p>
     </div>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi mathvariant="normal">
          Δ
         </mi>
         <mi>
          f
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-bin" stretchy="false">
          −
         </mo>
         <msubsup>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
          <mrow>
           <mi>
            r
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mn>
            2
           </mn>
           <msubsup>
            <mrow>
             <mi>
              f
             </mi>
            </mrow>
            <mrow>
             <mi>
              t
             </mi>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <mi>
            υ
           </mi>
           <mi class="loglike">
            cos
           </mi>
           <mo>
            ⁡
           </mo>
           <mi>
            𝜃
           </mi>
          </mrow>
          <mrow>
           <mi>
            c
           </mi>
          </mrow>
         </mfrac>
         <mspace class="qquad" width="2em">
         </mspace>
         <mstyle class="text">
          <mtext>
           and
          </mtext>
         </mstyle>
         <mspace class="qquad" width="2em">
         </mspace>
         <mi>
          υ
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi mathvariant="normal">
            Δ
           </mi>
           <mi>
            f
           </mi>
           <mi>
            c
           </mi>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
           <msubsup>
            <mrow>
             <mi>
              f
             </mi>
            </mrow>
            <mrow>
             <mi>
              t
             </mi>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <mi class="loglike">
            cos
           </mi>
           <mo>
            ⁡
           </mo>
           <mi>
            𝜃
           </mi>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      A
     current
     application
     area
     is
     both
     autonomous
     and
     manned
     highway
     vehicles.
     Both
     micro-
     wave
     and
     laser
     radar
     systems
     have
     been
     designed
     for
     this
     environment.
     Both
     systems
     have
     equivalent
     range,
     but
     laser
     can
     suffer
     when
     visual
     signals
     are
     deteriorated
     by
     environmental
     conditions
     such
     as
     rain,
     fog,
     etc.
     Commercial
     microwave
     radar
     systems
     are
     already
     avail-
     able
     for
     installation
     on
     highway
     trucks.
     These
     systems
     are
     called
     VORAD
     (vehicle
     on-board
     radar)
     and
     have
     a
     total
     range
     of
     approximately
     150m.
     With
     an
     accuracy
     of
     approximately
     97%,
     these
     systems
     report
     range
     rate
     from
     0
     to
     160
     km/hr
     with
     a
     resolution
     of
     1
     km/
     hr.
     The
     beam
     is
     approximately
     4ř
     wide
     and
     5ř
     in
     elevation.
     One
     of
     the
     key
     limitations
     of
     radar
     technology
     is
     its
     bandwidth.
     Existing
     systems
     can
     provide
     information
     on
     multiple
     targets
     at
     approximately
     2
     Hz.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-3" sec="section-3">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.1.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.3
      </small>
      <a id="x3-320001.3">
      </a>
      Vision
     Based
     Sensors
     </h2>
     <p class="noindent">
      Vision
     is
     our
     most
     powerful
     sense.
     It
     provides
     us
     with
     an
     enormous
     amount
     of
     information
     about
     the
     environment
     and
     enables
     rich,
     intelligent
     interaction
     in
     dynamic
     environments.
     It
     is
     therefore
     not
     at
     all
     surprising
     that
     a
     great
     deal
     of
     effort
     has
     been
     devoted
     to
     providing
     machines
     with
     sensors
     which
     can
     at
     least
     try
     to
     mimic
     the
     capabilities
     of
     the
     human
     vision
     system.
     </p>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Perception/raster/ccd-chip.jpg" width="100%"/>
      <a id="x3-32001r14">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.14:
       </span>
       <span class="content">
        Sony    ICX493AQA
       10.14-megapixel
       APS-C (23.4 Œ 15.6
       mm)
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        from  digital  camera
       Sony DSLR-A200 or
       DSLR-A300,  sensor
       side <span class="cite"><a href="MobileRoboticsLectureBookad1.html#CCDchip2014">[6]</a></span>.
       </span>
      </figcaption>
     </aside>
     <p class="noindent">
      The
     first
     step
     in
     this
     process
     is
     the
     creation
     of
     sensing
     devices
     that
     capture
     the
     same
     raw
     information
     which
     is
     the
     light
     the
     human
     vision
     system
     uses.
     The
     main
     topics
     which
     will
     be
     described
     are
     the
     two
     <alert style="color: #821131;">(2)</alert>
     current
     technologies
     for
     creating
     vision
     sensors:
     </p>
     <dl class="enumerate-enumitem">
      <dt class="enumerate-enumitem">
       1.
      </dt>
      <dd class="enumerate-enumitem">
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
        CCD
       </a>
       ,
      </dd>
      <dt class="enumerate-enumitem">
       2.
      </dt>
      <dd class="enumerate-enumitem">
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
        CMOS
       </a>
       .
      </dd>
     </dl>
     <p class="noindent">
      Of
     course,
     these
     sensors
     have
     specific
     limitations
     in
     performance
     compared
     to
     the
     human
     eye,
     and
     it
     is
     important
     to
     understand
     these
     limitations.
     Later
     sections
     describe
     vision-based
     sensors
     which
     are
     commercially
     available,
     similar
     to
     the
     sensors
     discussed
     previously,
     along
     with
     their
     disadvantages
     and
     most
     popular
     applications.
      <a id="subsubsection*.19">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-33000">
      </a>
      CCD
     and
     CMOS
     Sensors
     </h5>
     <p class="noindent">
      When
     it
     comes
     to
     the
     marketplace,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      is
     the
     most
     popular
     fundamental
     ingredient
     for
     robotic
     vision
     systems.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         18
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          18
         </sup>
        </span>
       </alert>
       <span id="textcolor30">
        Willard
       Boyle
       and
       George
       E.
       Smith
       invented
       the
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        in
       1969
       at
       AT&amp;T
       Bell
       Labs.
       Their
       original
       idea
       was
       to
       create
       a
       memory
       device.
       However,
       with
       its
       publication
       in
       1970,
       other
       scientists
       began
       experimenting
       with
       the
       technology
       on
       a
       range
       of
       applications.
       Astronomers
       discovered
       that
       they
       could
       produce
       high-resolution
       images
       of
       distant
       objects,
       because
       CCDs
       offered
       a
       photo-sensitivity
       one
       hundred
       times
       greater
       than
       film <a id="x3-33001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#TeledyneCCD2020">[7]</a></span>.
       </span>
      </span>
      The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chip,
     which
     you
     can
     see
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-32001r14">
       1.14
      </a>
      is
     an
     array
     of
     light-sensitive
     picture
     elements,
     or
     pixels,
     usually
     with
     between
     20000
     and
     2
     million
     pixels
     total.
     </p>
     <p class="noindent">
      Each
     pixel
     can
     be
     thought
     of
     as
     a
      <alert style="color: #821131;">
       light-sensitive,
discharging
capacitor
      </alert>
      that
     is
     5
     to
     25
     ţ
     m
     in
     size.
     First,
     the
     capacitors
     of
     all
     pixels
     are
     fully
     charged,
     then
     the
     integration
     period
     begins.
     As
     photons
     of
     light
     strike
     each
     pixel,
     the
     electrons
     are
     liberated,
     which
     are
     captured
     by
     electric
     fields
     and
     retained
     at
     the
     pixel.
     Over
     time,
     each
     pixel
     accumulates
     a
     varying
     level
     of
     charge
     based
     on
     the
     total
     number
     of
     photons
     that
     have
     struck
     it.
     After
     the
     integration
     period
     is
     complete,
     the
     relative
     charges
     of
     all
     pixels
     need
     to
     be
      <alert style="color: #821131;">
       frozen
and
read
      </alert>
      .
     </p>
     <p class="noindent">
      In
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      ,
     the
     reading
     process
     is
     performed
     at
     one
     corner
     of
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chip.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         19
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          19
         </sup>
        </span>
       </alert>
       <span id="textcolor31">
        Because
       the
       entire
       array
       is
       read
       through
       a
       single
       amplifier
       the
       output
       can
       be
       highly
       optimised
       to
       give
       very
       low
       noise
       and
       extremely
       high
       dynamic
       range.
       CCDs
       can
       have
       over
       100
       dB
       dynamic
       range
       with
       less
       than
       2e
       of
       noise <a id="x3-33002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#TeledyneCCD2020">[7]</a></span>.
       </span>
      </span>
      The
     bottom
     row
     of
     pixel
     charges
     are
     transported
     to
     this
     corner
     and
     read,
     then
     the
     rows
     above
     shift
     down
     and
     the
     process
     repeats.
     This
     means
     that
     each
     charge
      <alert style="color: #821131;">
       must
be
transported
across
the
chip
      </alert>
      ,
     and
     it
     is
     critical
     the
     value
     be
     preserved.
     </p>
     <div class="warning">
      <p class="noindent">
       This
      requires
      specialised
      control
      circuitry
      and
      custom
      fabrication
      techniques
      to
      ensure
      the
      stability
      of
      transported
      charges.
      </p>
     </div>
     <p class="noindent">
      The
     photo-diodes
     used
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chips
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         20
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          20
         </sup>
        </span>
       </alert>
       <span id="textcolor32">
        This
       also
       includes
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        as
       well.
       </span>
      </span>
      are
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      <alert style="color: #821131;">
       equally
sensitive
to
all
frequencies
of
light
      </alert>
      .
     They
     are
     sensitive
     to
     light
     between
     400
     nm
     and
     1000
     nm
     wavelength.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         21
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          21
         </sup>
        </span>
       </alert>
       <span id="textcolor33">
        This
       number
       range
       is
       usually
       given
       for
       easier
       numbers
       as
       both
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        and
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        have
       sensitivity
       values
       at
       approximately
       350
       -
       1050
       nm
       .
       </span>
      </span>
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/ccd-frequency-response.jpg" width="150%"/>
       <a id="x3-33003r15">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.15:
       </span>
       <span class="content">
        Normalized Spectral Response of a Typical Monochrome CCD.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      It
     is
     important
     to
     remember
     that
     photodiodes
     are
      <span id="bold" style="font-weight:bold;">
       less
sensitive
to
the
ultraviolet
      </span>
      part
     of
     the
     spectrum
     and
     are
     overly
      <span id="bold" style="font-weight:bold;">
       sensitive
to
the
infrared
      </span>
      portion
     (e.g.
     heat)
     which
     you
     can
     see
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-33003r15">
       1.15
      </a>
      .
     You
     can
     see
     that
     the
     basic
     light-measuring
     process
     is
     colourless.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         22
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          22
         </sup>
        </span>
       </alert>
       <span id="textcolor34">
        It
       is
       just
       measuring
       the
       total
       number
       of
       photons
       that
       strike
       each
       pixel
       in
       the
       integration
       period
       regardless
       whether
       the
       light
       hitting
       it
       is
       blue,
       red
       or
       green.
       </span>
      </span>
     </p>
     <p class="noindent">
      There
     are
     two
     <alert style="color: #821131;">(2)</alert>
     common
     approaches
     for
     creating
     color
     images.
     If
     the
     pixels
     on
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chip
     are
     grouped
     into
     2-by-2
     sets
     of
     four
     <alert style="color: #821131;">(4)</alert>
     ,
     then
     red,
     green
     and
     blue
     dyes
     can
     be
     applied
     to
     a
     colour
     filter
     so
     each
     individual
     pixel
     receives
     only
     light
     of
     just
     one
     color.
     </p>
     <div class="figure">
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/Bayer.png" width=""/>
        <a id="x3-33004r1">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (a)
        </span>
        <span class="content">
         Bayer Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/CYGM.png" width=""/>
        <a id="x3-33005r2">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (b)
        </span>
        <span class="content">
         CYGM Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/CYYM.png" width=""/>
        <a id="x3-33006r3">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (c)
        </span>
        <span class="content">
         CYYM Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/RCCB.png" width=""/>
        <a id="x3-33007r4">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (d)
        </span>
        <span class="content">
         RCCB Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/RCCC.png" width=""/>
        <a id="x3-33008r5">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (e)
        </span>
        <span class="content">
         RCCC Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/RGBE.png" width=""/>
        <a id="x3-33009r6">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (f)
        </span>
        <span class="content">
         RGBE Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/RGBW.png" width=""/>
        <a id="x3-33010r7">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (g)
        </span>
        <span class="content">
         RGBW Filter
        </span>
       </div>
      </div>
      <div class="subfigure" style="width:24%;">
       <p class="noindent">
       </p>
       <p class="noindent">
        <img alt="PIC" height="" src="figures/Perception/raster/RYYB.png" width=""/>
        <a id="x3-33011r8">
        </a>
       </p>
       <div class="caption">
        <span class="id">
         (h)
        </span>
        <span class="content">
         RYYB Filter
        </span>
       </div>
      </div>
      <a id="x3-33012r16">
      </a>
      <div class="caption">
       <span class="id">
        Figure
       1.16:
       </span>
       <span class="content">
        Types of colour filter used in commercial and industrial applications
       </span>
      </div>
     </div>
     <p class="noindent">
      Normally, two <alert style="color: #821131;">(2)</alert> pixels measure green while one pixel each measures red and blue light
     intensity. Of course, this 1-chip color
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      has a geometric resolution disadvantage.
     </p>
     <div class="warning">
      <p class="noindent">
       The number of pixels in the system has been effectively cut by a factor of 4, and therefore the image resolution output by the
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
        CCD
       </a>
       camera will be sacrificed.
      </p>
     </div>
     <p class="noindent">
      The
     3-chip
     color
     camera
     avoids
     these
     problems
     by
     splitting
     the
     incoming
     light
     into
     three
     <alert style="color: #821131;">(3)</alert>
     complete
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         23
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          23
         </sup>
        </span>
       </alert>
       <span id="textcolor35">
        Albeit,
       with
       lower
       resolution.
       </span>
      </span>
      copies.
     Three
     separate
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chips
     receive
     the
     light,
     with
     one
     red,
     green
     or
     blue
     filter
     over
     each
     entire
     chip.
     Thus,
     in
     parallel,
     each
     chip
     measures
     light
     intensity
     for
     just
     one
     color,
     and
     the
     camera
     must
     combine
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chips’
     outputs
     to
     create
     a
     joint
     color
     image.
     </p>
     <div class="knowledge">
      <p class="noindent">
       Resolution is preserved in this solution, although the 3-chip color cameras are, as one would
      expect, significantly more expensive and therefore more rarely used in mobile robotics.
      </p>
     </div>
     <p class="noindent">
      Both
     3-chip
     and
     single
     chip
     color
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      cameras
     suffer
     from
     the
     fact
     that
     photo-diodes
     are
     much
     more
     sensitive
     to
     the
     near-infrared
     end
     of
     the
     spectrum.
     This
     means
     that
     the
     overall
     system
     detects
     blue
     light
     much
     more
     poorly
     than
     red
     and
     green.
     To
     compensate,
     the
     gain
     must
     be
     increased
     on
     the
     blue
     channel,
     and
     this
     introduces
     greater
     absolute
     noise
     on
     blue
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         24
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          24
         </sup>
        </span>
       </alert>
       <span id="textcolor36">
        This
       is
       generally
       defined
       as
       the
       amplifier
       noise.
       </span>
      </span>
      than
     on
     red
     and
     green.
     It
     is
     not
     uncommon
     to
     assume
     at
     least
     1
     -
     2
     bits
     of
     additional
     noise
     on
     the
     blue
     channel.
     </p>
     <p class="noindent">
      The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera
     has
     several
     camera
     parameters
     that
     affect
     its
     behavior.
     In
     some
     cameras,
     these
     parameter
     values
     are
     fixed.
     In
     others,
     the
     values
     are
     constantly
     changing
     based
     on
     built-in
     feedback
     loops.
     In
     higher-end
     cameras,
     the
     user
     can
     modify
     the
     values
     of
     these
     parameters
     via
     software
     embedded
     into
     the
     device.
     The
     iris
     position
     and
     shutter
     speed
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         25
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          25
         </sup>
        </span>
       </alert>
       <span id="textcolor37">
        It’s
       the
       speed
       at
       which
       the
       shutter
       of
       the
       camera
       closes.
       A
       fast
       shutter
       speed
       creates
       a
       shorter
       exposure
       -
       the
       amount
       of
       light
       the
       camera
       takes
       in
       -
       and
       a
       slow
       shutter
       speed
       gives
       a
       longer
       exposure.
       </span>
      </span>
      regulate
     the
     amount
     of
     light
     being
     measured
     by
     the
     camera.
     The
     iris
     is
     simply
     a
     mechanical
     aperture
     that
     constricts
     incoming
     light,
     just
     as
     in
     standard
     35mm
     cameras.
     Shutter
     speed
     regulates
     the
     integration
     period
     of
     the
     chip.
     In
     higher-end
     cameras,
     the
     effective
     shutter
     speed
     can
     be
     as
     brief
     at
     1/30,000s
     and
     as
     long
     as
     2s.
     Camera
     gain
     controls
     the
     overall
     amplification
     of
     the
     analog
     signal,
     prior
     to
     A/D
     conversion.
     However,
     it
     is
     very
     important
     to
     understand
     that,
     even
     though
     the
     image
     may
     appear
     brighter
     after
     setting
     high
     gain,
     the
     shutter
     speed
     and
     iris
     may
     not
     have
     changed
     at
     all.
     Thus
     gain
     merely
     amplifies
     the
     signal,
     and
     amplifies
     along
     with
     the
     signal
     all
     of
     the
     associated
     noise
     and
     error.
     Although
     useful
     in
     applications
     where
     imaging
     is
     done
     for
     human
     consumption
     (e.g.
     photography,
     television),
     gain
     is
     of
     little
     value
     to
     a
     mobile
     roboticist.
     </p>
     <p class="noindent">
      In
     colour
     cameras,
     an
     additional
     control
     exists
     for
     white
     balance.
     Depending
     on
     the
     source
     of
     illumination
     in
     a
     scene
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         26
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          26
         </sup>
        </span>
       </alert>
       <span id="textcolor38">
        For
       example
       this
       could
       be
       fluorescent
       lamps,
       incandescent
       lamps,
       sunlight,
       underwater
       filtered
       light,
       etc.
       </span>
      </span>
      the
     relative
     measurements
     of
     red,
     green
     and
     blue
     light
     which
     combine
     to
     define
     pure
     white
     light
     will
     change
     dramatically
     which
     can
     be
     seen
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-33014r17">
       1.17
      </a>
      which
     can
     also
     be
     adjusted
     with
     algorithms <a id="x3-33013"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#1530559">[8]</a></span>.
     The
     human
     eyes
     compensate
     for
     all
     such
     effects
     in
     ways
     that
     are
     not
     fully
     understood,
     however,
     the
     camera
     can
     demonstrate
     glaring
     inconsistencies
     in
     which
     the
     same
     table
     looks
     blue
     in
     one
     image,
     taken
     during
     the
     night,
     and
     yellow
     in
     another
     image,
     taken
     during
     the
     day.
     White
     balance
     controls
     enable
     the
     user
     to
     change
     the
     relative
     gain
     for
     red,
     green
     and
     blue
     in
     order
     to
     maintain
     more
     consistent
     color
     definitions
     in
     varying
     contexts.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/white-balance.jpg" width="150%"/>
       <a id="x3-33014r17">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.17:
       </span>
       <span class="content">
        Example of white balance. Here the same scene is emulated to be shot under different light conditions <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Fstoppers2022">[9]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The key disadvantages of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      cameras are primarily in the areas of inconstancy and
      <span id="bold" style="font-weight:bold;">
       dynamic range
      </span>
      .
     </p>
     <div class="informationblock" id="tcolobox-1">
      <div class="title">
       <p class="noindent">
        <span id="bold" style="font-weight:bold;">
         Information
        </span>
        : Dynamic Range
       </p>
      </div>
      <div class="box-content">
       <p class="noindent">
        Dynamic  range  in  photography  describes  the  ratio  between  the  maximum  and  minimum  measurable  light
       intensities (white and black, respectively). In the real world, one never encounters true white or black - only
       varying degrees of light source intensity and subject reflectivity. Therefore the concept of dynamic range becomes
       more complicated, and depends on whether you are describing a capture device (such as a camera or scanner),
       a display device (such as a print or computer display), or the subject itself.
       </p>
      </div>
     </div>
     <p class="noindent">
      As mentioned above, a number of parameters can change the brightness and colours with which a camera creates its image.
     </p>
     <div class="knowledge">
      <p class="noindent">
       Manipulating these parameters in a way to provide consistency over time and over environments, for example ensuring a
      green shirt always looks green, and something dark grey is always dark grey, remains an open problem <a id="x3-33016"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#ilie2005ensuring">[10]</a></span>.
      </p>
     </div>
     <p class="noindent">
      The
     second
     type
     of
     disadvantages
     relates
     to
     the
     behavior
     of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chip
     in
     environments
     with
      <alert style="color: #821131;">
       extreme
illumination
      </alert>
      .
     In
     cases
     of
     very
     low
     illumination,
     each
     pixel
     will
     receive
     only
     a
     small
     number
     of
     photons.
     The
     longest
     possible
     shutter
     speed
     and
     camera
     optics
     (i.e.
     pixel
     size,
     chip
     size,
     lens
     focal
     length
     and
     diameter)
     will
     determine
     the
     minimum
     level
     of
     light
     for
     which
     the
     signal
     is
     stronger
     than
     random
     error
     noise.
     In
     cases
     of
     very
     high
     illumination,
     a
     pixel
     fills
     its
     well
     with
     free
     electrons
     and,
     as
     the
     well
     reaches
     its
     limit,
     the
     probability
     of
     trapping
     additional
     electrons
     falls
     and
     therefore
     the
     linearity
     between
     incoming
     light
     and
     electrons
     in
     the
     well
     degrades.
     This
     is
     termed
     saturation
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         27
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Perception/raster/ccd-saturation.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          27
         </sup>
        </span>
       </alert>
       <span id="textcolor39">
        Example
       of
       blooming
       caused
       by
       saturation
       of
       a
       sensor
       pixel.
       The
       sun
       is
       so
       bright
       in
       the
       image
       that
       there
       is
       blooming
       on
       the
       sun
       itself,
       leaking
       into
       the
       surrounding
       pixels,
       and
       a
       vertical
       smear
       across
       the
       whole
       image <a id="x3-33017"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#TeledyneSat2025">[11]</a></span>.
       </span>
      </span>
      and
     can
     indicate
     the
     existence
     of
     a
     further
     problem
     related
     to
     cross-sensitivity <a id="x3-33018"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#zhang2013analysis">[12]</a></span>.
     When
     a
     well
     has
     reached
     its
     limit,
     then
     additional
     light
     within
     the
     remainder
     of
     the
     integration
     period
     may
     cause
     further
     charge
     to
     leak
     into
     neighbouring
     pixels,
     causing
     them
     to
     report
     incorrect
     values
     or
     even
     reach
     secondary
     saturation.
     This
     effect,
     called
     blooming,
     means
     that
     individual
     pixel
     values
     are
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      truly
      <alert style="color: #821131;">
       independent
      </alert>
      .
     The
     camera
     parameters
     may
     be
     adjusted
     for
     an
     environment
     with
     a
     particular
     light
     level,
     but
     the
     problem
     remains
     that
     the
     dynamic
     range
     of
     a
     camera
     is
     limited
     by
     the
     well
     capacity
     of
     the
     individual
     pixels.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example,
      a
      high
      quality
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
        CCD
       </a>
       may
      have
      pixels
      that
      can
      hold
      40000
      electrons.
      The
      noise
      level
      for
      reading
      the
      well
      may
      be
      11
      electrons,
      and
      therefore
      the
      dynamic
      range
      will
      be
      40,000:11,
      or
      3,600:1,
      which
      is
      35
      dB
      .
      </p>
     </div>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.3.1
      </small>
      <a id="x3-340001.3.1">
      </a>
      CMOS  Technology
     </h3>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/cmos-sensor.jpg" width="150%"/>
       <a id="x3-34001r18">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.18:
       </span>
       <span class="content">
        A close-up view of a
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        sensor and its circuitry <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Baumer2025">[13]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The Complementary Metal Oxide Semiconductor (
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      ) chip is a significant departure from the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      . Similar to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      , it too
     has an array of pixels, but located alongside each pixel are
      <alert style="color: #821131;">
       several transistors specific to that pixel
      </alert>
      . Just as in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chips, all
     of the pixels accumulate charge during the integration period. During the data collection step, the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      takes a new approach:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       The
      pixel-specific
      circuitry
      next
      to
      every
      pixel
      measures
      and
      amplifies
      the
      pixel’s
      signal,
      all
      in
      parallel
      for
      every
      pixel
      in
      the
      array.
      </p>
     </div>
     <p class="noindent">
      Using
     more
     traditional
     traces
     from
     general
     semiconductor
     chips,
     the
     resulting
     pixel
     values
     are
     all
     carried
     to
     their
     destinations.
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      has
     a
     number
     of
     advantages
     over
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      technologies.
     First
     and
     foremost,
     there
     is
     no
     need
     for
     the
     specialized
     clock
     drivers
     and
     circuitry
     required
     in
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      to
     transfer
     each
     pixel’s
     clock
     down
     all
     of
     the
     array
     columns
     and
     across
     all
     of
     its
     rows.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         28
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Perception/raster/cmos-camera-sensor.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          28
         </sup>
        </span>
       </alert>
       <span id="textcolor40">
        -CAM80CUNX
       is
       an
       8MP
       Ultra-lowlight
       MIPI
       CSI-2
       camera
       capable
       of
       streaming
       4K
       @
       44
       fps.
       This
       8MP
       camera
       is
       based
       on
       SONY
       STARVIS
       IMX415
       CMOS
       image
       sensor <a id="x3-34003"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#Econ2025">[14]</a></span>
       </span>
      </span>
     </p>
     <div class="warning">
      <p class="noindent">
       This
      also
      means
      that
      specialized
      semiconductor
      manufacturing
      processes
      are
      not
      required
      to
      create
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
        CMOS
       </a>
       chips.
      </p>
     </div>
     <p class="noindent">
      Therefore,
     the
     same
     production
     lines
     that
     create
     microchips
     can
     create
     inexpensive
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      chips
     as
     well.
     The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      chip
     is
     so
     much
     simpler
     that
     it
     consumes
     significantly
     less
     power,
     it
     operates
     with
     a
     power
     consumption
     a
     tenth
     the
     power
     consumption
     of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      chip <a id="x3-34004"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#holst2007cmos">[15]</a></span>.
     </p>
     <div class="knowledge">
      <p class="noindent">
       In
      a
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       ,
      power
      is
      a
      scarce
      resource
      and
      therefore
      this
      is
      an
      important
      advantage.
      </p>
     </div>
     <p class="noindent">
      On
     the
     other
     hand,
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
       CMOS
      </a>
      chip
     also
     faces
     several
     disadvantages.
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        Most
       importantly,
       the
       circuitry
       next
       to
       each
       pixel
       consumes
       valuable
       real
       estate
       on
       the
       face
       of
       the
       light-detecting
       array.
       Many
       photons
       hit
       the
       transistors
       rather
       than
       the
       photodiode,
       making
       the
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        chip
       significantly
       less
       sensitive
       than
       an
       equivalent
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        chip.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        ,
       compared
       to
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        is
       still
       finding
       ground
       in
       the
       marketplace,
       and
       as
       a
       result,
       the
       best
       resolution
       that
       one
       can
       purchase
       in
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        format
       continues
       to
       be
       far
       inferior
       to
       the
       best
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
         CCD
        </a>
        chips
       available.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        sensors
       have
       a
       lower
       dynamic
       range,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:cmos">
         CMOS
        </a>
        sensors
       have
       higher
       levels
       of
       noise.
       </p>
      </li>
     </ul>
     <p class="noindent">
      Compared
     to
     the
     human
     eye,
     these
     chips
     all
     have
     worse
     performance,
     cross-sensitivity
     and
     a
     limited
     dynamic
     range.
     As
     a
     result,
     vision
     sensors
     today
     continue
     to
     be
     fragile.
     Only
     over
     time,
     as
     the
     underlying
     performance
     of
     imaging
     chips
     improves,
     will
     significantly
     more
     robust
     vision-based
     sensors
     for
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     be
     available.
     </p>
     <div class="informationblock" id="tcolobox-2">
      <div class="title">
       <p class="noindent">
        <span id="bold" style="font-weight:bold;">
         Information
        </span>
        :
       Shot
       Noise
       </p>
      </div>
      <div class="box-content">
       <p class="noindent">
        Shot noise or Poisson noise is a type of noise which can be modeled by a Poisson process.
       </p>
       <p class="noindent">
        In electronics shot noise originates from the discrete nature of electric charge. Shot noise also occurs in photon counting
       in optical devices, where shot noise is associated with the particle nature of light.
       </p>
       <div class="figure">
        <p class="noindent">
         <img alt="PIC" height="" src="figures/Perception/raster/shot-noise.jpg" width="150%"/>
         <a id="x3-34005r19">
         </a>
        </p>
        <figcaption class="caption">
         <span class="id">
          Figure
         1.19:
         </span>
         <span class="content">
          Photon noise simulation. Number of photons per pixel increases from left to right and from upper
         row to bottom row <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Mdf2010">[16]</a></span>.
         </span>
        </figcaption>
       </div>
      </div>
     </div>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.3.2
      </small>
      <a id="x3-350001.3.2">
      </a>
      Visual
     Ranging
     Sensors
     </h3>
     <p class="noindent">
      Range
     sensing
     is
     extremely
     important
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      as
     it
     is
     a
     basic
     input
     for
     successful
     obstacle
     avoidance.
     As
     we
     have
     seen
     earlier,
     a
     number
     of
     sensors
     are
     popular
     in
     robotics
     specifically
     for
     their
     ability
     to
     recover
     depth
     estimates:
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       ultrasonic,
      laser
      rangefinder,
      optical
      rangefinder,
      etc.
      </p>
     </div>
     <p class="noindent">
      It
     is
     natural
     to
     attempt
     to
     implement
     ranging
     functionality
     using
     vision
     chips
     as
     well.
     However,
     a
     fundamental
     problem
     with
     visual
     images
     makes
     rangefinding
     relatively
     difficult.
     </p>
     <p class="noindent">
      Any
     vision
     chip
     collapses
     the
     three-dimensional
     world
     into
     a
     two-dimensional
     image
     plane,
     thereby
     losing
     depth
     information.
     If
     one
     can
     make
     strong
     assumptions
     regarding
     the
     size
     of
     objects
     in
     the
     world,
     or
     their
     particular
     colour
     and
     reflectance,
     then
     one
     can
     directly
     interpret
     the
     appearance
     of
     the
     two-dimensional
     image
     to
     recover
     depth.
     But
     such
     assumptions
     are
     rarely
     possible
     in
     real-world
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications.
     </p>
     <div class="warning">
      <p class="noindent">
       Without
      such
      assumptions,
      a
      single
      picture
      does
      not
      provide
      enough
      information
      to
      recover
      spatial
      information.
      </p>
     </div>
     <p class="noindent">
      The
     general
     solution
     is
     to
     recover
     depth
     by
     looking
     at
     several
     images
     of
     the
     scene
     to
     gain
     more
     information,
     which
     will
     be
     hopefully
     enough
     to
     at
     least
     partially
     recover
     depth.
     The
     images
     used
      <alert style="color: #821131;">
       must
be
different
      </alert>
      ,
     so
     that
     taken
     together
     they
     provide
     additional
     information.
     They
     could
     differ
     in
     viewpoint,
     which
     would
     allow
     the
     use
     of
     stereo
     or
     motion
     algorithms.
     </p>
     <p class="noindent">
      An
     alternative
     is
     to
     create
     different
     images,
     not
     by
     changing
     the
     viewpoint,
     but
     by
     changing
     the
     camera
     geometry,
     such
     as
     the
     focus
     position
     or
     lens
     iris.
     This
     is
     the
     fundamental
     idea
     behind
     depth
     from
     focus
     and
     depth
     from
     defocus
     techniques.
     We
     will
     now
     look
     into
     the
     general
     approach
     to
     the
     depth
     from
     focus
     techniques
     as
     it
     presents
     a
     straightforward
     and
     efficient
     way
     to
     create
     a
     vision-based
     range
     sensor.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.3.3
      </small>
      <a id="x3-360001.3.3">
      </a>
      Depth
     from
     Focus
     </h3>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/camera-optics.png" width="150%"/>
       <a id="x3-36001r20">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.20:
       </span>
       <span class="content">
        Depiction   of   the   camera   optics   and   its   impact   on   the   image.   To   get   a   sharp   image,
       the   image   plane   must   coincide   with   the   focal   plane.   Otherwise   the   image   of   the   point
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              x
             </mi>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <mi>
              y
             </mi>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <mi>
              z
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
        will be blurred in the image as can be seen in the drawing above.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The
     depth
     from
     focus
     class
     of
     techniques
     relies
     on
     the
     fact
     that
     image
     properties
     not
     only
     change
     as
     a
     function
     of
     the
      <alert style="color: #821131;">
       scene
      </alert>
      ,
     but
     also
     as
     a
     function
     of
     the
      <alert style="color: #821131;">
       camera
parameters
      </alert>
      .
     The
     relationship
     between
     camera
     parameters
     and
     image
     properties
     is
     depicted
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-36001r20">
       1.20
      </a>
      .
     The
     fundamental
     formula
     governing
     image
     formation
     relates
     the
     distance
     of
     the
     object
     from
     the
     lens,
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        d
       </mi>
      </math>
      in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-36001r20">
       1.20
      </a>
      ,
     to
     the
     distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        e
       </mi>
      </math>
      from
     the
     lens
     to
     the
     focal
     point,
     based
     on
     the
     focal
     length
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        f
       </mi>
      </math>
      of
     the
     lens:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mfrac>
          <mrow>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mi>
            f
           </mi>
          </mrow>
         </mfrac>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mi>
            d
           </mi>
          </mrow>
         </mfrac>
         <mo class="MathClass-bin" stretchy="false">
          +
         </mo>
         <mfrac>
          <mrow>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mi>
            e
           </mi>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      If the image plane is located at distance
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        e
       </mi>
      </math>
      from the lens, then for the specific object voxel
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         29
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          29
         </sup>
        </span>
       </alert>
       <span id="textcolor45">
        A three-dimensional counterpart to a pixel.
       </span>
      </span>
      depicted, all light will be focused
     at a single point on the image plane and the object voxel will be focused. However, when the image plane is
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <span id="bold" style="font-weight:bold;">
        <mstyle class="text">
         <mtext>
          NOT
         </mtext>
        </mstyle>
       </span>
      </math>
      at
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        e
       </mi>
      </math>
      , as
     is seen in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x3-36001r20">
       1.20
      </a>
      , then the light from the object voxel will be cast on the image plane as a
      <alert style="color: #821131;">
       blur circle
      </alert>
      . To
     a first approximation, the light is homogeneously distributed throughout this blur circle, and the radius
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        R
       </mi>
      </math>
      of the circle can be characterized according to the equation:
     </p>
     <table class="equation-star">
      <tr>
       <td>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mi>
          R
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            L
           </mi>
           <mi>
            δ
           </mi>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
           <mi>
            e
           </mi>
          </mrow>
         </mfrac>
        </math>
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        L
       </mi>
      </math>
      is the diameter of
     the lens or aperture and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        δ
       </mi>
      </math>
      is the displacement of the image plan from the focal point.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Perception/raster/camera-focus.jpg" width="150%"/>
       <a id="x3-36002r21">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       1.21:
       </span>
       <span class="content">
        Three images of the same scene taken with a camera at three different focusing positions. Note the
       significant change in texture sharpness between the near surface and far surface <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Mark-j2018">[17]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      Given these formulae, several basic optical effects are clear.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       For
      example,
      if
      the
      aperture
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          30
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           30
          </sup>
         </span>
        </alert>
        <span id="textcolor46">
         The
        aperture
        is
        the
        opening
        in
        the
        lens
        that
        allows
        light
        to
        enter
        the
        camera
        and
        onto
        the
        sensor
        or
        film.
        </span>
       </span>
       or
      lens
      is
      reduced
      to
      a
      point,
      as
      in
      a
      pin-hole
      camera,
      then
      the
      radius
      of
      the
      blur
      circle
      approaches
      zero.
      </p>
     </div>
     <p class="noindent">
      This is consistent with the fact that decreasing the iris aperture opening causes the depth of field to increase until all objects
     are in focus. Of course, the disadvantage of doing so is that we are allowing less light to form the image on the
     image plane and so this is practical only in bright circumstances The second property to be deduced from these
     optics equations relates to the sensitivity of blurring as a function of the distance from the lens to the object.
     </p>
     <p class="noindent">
      Suppose the image plane is at a fixed distance 1.2 from a lens with diameter L = 0.2 and focal length f = 0.5. We can see from
     Equation (4.20) that the size of the blur circle R changes proportionally with the image plane displacement . If the object is at
     distance d = 1, then from Equation (4.19) we can compute e=1 and therefore = 0.2. Increase the object distance to d = 2 and
     as a result = 0.533. Using Equation (4.20) in each case we can compute R = 0.02 R = 0.08 respectively. This
     demonstrates high sensitivity for defocusing when the object is close to the lens. In contrast suppose the object is
     at d = 10. In this case we compute e = 0.526. But if the object is again moved one unit, to d = 11, then we
     compute e = 0.524. Then resulting blur circles are R = 0.117 and R = 0.129, far less than the quadrupling in R
     when the obstacle is 1/10 the distance from the lens. This analysis demonstrates the fundamental limitation
     of depth from focus techniques: they lose sensitivity as objects move further away (given a fixed focal length).
     Interestingly, this limitation will turn out to apply to virtually all visual ranging techniques, including depth
     from stereo and depth from motion. Nevertheless, camera optics can be customised for the depth range of the
     intended application. For example, a "zoom" lens with a very large focal length f will enable range resolution at
     significant distances, of course at the expense of field of view. Similarly, a large lens diameter, coupled with a very
     fast shutter speed, will lead to larger, more detectable blur circles. Given the physical effects summarised by the
     above equations, one can imagine a visual ranging sensor that makes use of multiple images in which camera
     optics are varied (e.g. image plane displacement ) and the same scene is captured (see Fig. 4.20). In fact this
     approach is not a new invention. The human visual system uses an abundance of cues and techniques, and one
     system demonstrated in humans is depth from focus. Humans vary the focal length of their lens continuously
     at a rate of about 2 Hz. Such approaches, in which the lens optics are actively searched in order to maximise
     focus, are technically called depth from focus. In contrast, depth from defocus means that depth is recovered
     using a series of images that have been taken with different camera geometries. Depth from focus methods are
     one of the simplest visual ranging techniques. To determine the range to an object, the sensor simply moves the
     image plane (via focusing) until maximizing the sharpness of the object. When the sharpness is maximised, the
     corresponding position of the image plane directly reports range. Some autofocus cameras and virtually all autofocus
     video cameras use this technique. Of course, a method is required for measuring the sharpness of an image or an
     object within the image. The most common techniques are approximate measurements of the sub-image gradient:
     </p>
     <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
      <mtable class="align" columnalign="left" displaystyle="true">
       <mtr>
        <mtd class="align-odd" columnalign="right">
         <msub>
          <mrow>
           <mstyle class="text">
            <mtext>
             sharpness
            </mtext>
           </mstyle>
          </mrow>
          <mrow>
           <mn>
            1
           </mn>
          </mrow>
         </msub>
        </mtd>
        <mtd class="align-even">
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <munder class="msub">
          <mrow>
           <mo>
            ∑
           </mo>
          </mrow>
          <mrow>
           <mi>
            x
           </mi>
           <mo class="MathClass-punc" stretchy="false">
            ,
           </mo>
           <mspace class="thinspace" width="0.17em">
           </mspace>
           <mi>
            y
           </mi>
          </mrow>
         </munder>
         <mrow class="mathinner">
          <mspace class="negthinspace" width="-0.17em">
          </mspace>
          <mrow>
           <mo fence="true" form="prefix">
            |
           </mo>
           <mrow>
            <mi>
             I
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <mi>
                 x
                </mi>
                <mo class="MathClass-punc" stretchy="false">
                 ,
                </mo>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mi>
                 y
                </mi>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
            <mo class="MathClass-bin" stretchy="false">
             −
            </mo>
            <mi>
             I
            </mi>
            <msup>
             <mrow>
              <mrow>
               <mo fence="true" form="prefix">
                (
               </mo>
               <mrow>
                <mi>
                 x
                </mi>
                <mo class="MathClass-bin" stretchy="false">
                 −
                </mo>
                <mn>
                 1
                </mn>
                <mo class="MathClass-punc" stretchy="false">
                 ,
                </mo>
                <mspace class="thinspace" width="0.17em">
                </mspace>
                <mi>
                 y
                </mi>
               </mrow>
               <mo fence="true" form="postfix">
                )
               </mo>
              </mrow>
             </mrow>
             <mrow>
             </mrow>
            </msup>
           </mrow>
           <mo fence="true" form="postfix">
            |
           </mo>
          </mrow>
         </mrow>
         <mspace width="2em">
         </mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
         <mstyle class="label" id="x3-36004r3">
         </mstyle>
         <mstyle class="maketag">
          <mtext>
           (1.3)
          </mtext>
         </mstyle>
         <mspace class="nbsp" width="0.33em">
         </mspace>
        </mtd>
       </mtr>
       <mtr>
        <mtd class="align-odd" columnalign="right">
         <msub>
          <mrow>
           <mstyle class="text">
            <mtext>
             sharpness
            </mtext>
           </mstyle>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
          </mrow>
         </msub>
        </mtd>
        <mtd class="align-even">
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <munder class="msub">
          <mrow>
           <mo>
            ∑
           </mo>
          </mrow>
          <mrow>
           <mi>
            x
           </mi>
           <mo class="MathClass-punc" stretchy="false">
            ,
           </mo>
           <mspace class="thinspace" width="0.17em">
           </mspace>
           <mi>
            y
           </mi>
          </mrow>
         </munder>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              I
             </mi>
             <msup>
              <mrow>
               <mrow>
                <mo fence="true" form="prefix">
                 (
                </mo>
                <mrow>
                 <mi>
                  x
                 </mi>
                 <mo class="MathClass-punc" stretchy="false">
                  ,
                 </mo>
                 <mspace class="thinspace" width="0.17em">
                 </mspace>
                 <mi>
                  y
                 </mi>
                </mrow>
                <mo fence="true" form="postfix">
                 )
                </mo>
               </mrow>
              </mrow>
              <mrow>
              </mrow>
             </msup>
             <mo class="MathClass-bin" stretchy="false">
              −
             </mo>
             <mi>
              I
             </mi>
             <msup>
              <mrow>
               <mrow>
                <mo fence="true" form="prefix">
                 (
                </mo>
                <mrow>
                 <mi>
                  x
                 </mi>
                 <mo class="MathClass-bin" stretchy="false">
                  −
                 </mo>
                 <mn>
                  2
                 </mn>
                 <mo class="MathClass-punc" stretchy="false">
                  ,
                 </mo>
                 <mspace class="thinspace" width="0.17em">
                 </mspace>
                 <mi>
                  y
                 </mi>
                 <mo class="MathClass-bin" stretchy="false">
                  −
                 </mo>
                 <mn>
                  2
                 </mn>
                </mrow>
                <mo fence="true" form="postfix">
                 )
                </mo>
               </mrow>
              </mrow>
              <mrow>
              </mrow>
             </msup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
           <mn>
            2
           </mn>
          </mrow>
         </msup>
         <mspace width="2em">
         </mspace>
        </mtd>
        <mtd class="align-label" columnalign="right">
         <mstyle class="label" id="x3-36005r4">
         </mstyle>
         <mstyle class="maketag">
          <mtext>
           (1.4)
          </mtext>
         </mstyle>
         <mspace class="nbsp" width="0.33em">
         </mspace>
        </mtd>
       </mtr>
      </mtable>
     </math>
     <p class="noindent">
      A significant advantage of the horizontal sum of differences technique (Equation (4.21)) is that the calculation
     can be implemented in analog circuitry using just a rectifier, a low-pass filter and a high-pass filter. This is a
     common approach in commercial cameras and video recorders. Such systems will be sensitive to contrast along
     one particular axis, although in practical terms this is rarely an issue. However depth from focus is an active
     search method and will be slow because it takes time to change the focusing parameters of the camera, using for
     example a servo-controlled focusing ring. For this reason this method has not been applied to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s. A variation
     of the depth from focus technique has been applied to a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      , demonstrating obstacle avoidance in a variety
     of environments as well as avoidance of concave obstacles such as steps and ledges [95]. This robot uses three
     monochrome cameras placed as close together as possible with different, fixed lens focus positions (Fig. 4.21).
     </p>
     <p class="noindent">
      Several times each second, all three frame-synchronised cameras simultaneously capture three images of the same scene. The
     images are each divided into five columns and three rows, or 15 subregions. The approximate sharpness of each region is
     computed using a variation of Equation (4.22), leading to a total of 45 sharpness values. Note that Equation 22 calculates
     sharpness along diagonals but skips one row. This is due to a subtle but important issue. Many cameras produce images in
     interlaced mode. This means that the odd rows are captured first, then afterwards the even rows are captured. When
     such a camera is used in dynamic environments, for example on a moving robot, then adjacent rows show the
     dynamic scene at two different time points, differing by up to 1/30 seconds. The result is an artificial blurring due
     to motion and not optical defocus. By comparing only even-number rows we avoid this interlacing side effect.
     </p>
     <p class="noindent">
      Recall that the three images are each taken with a camera using a different focus position. Based on the focusing position, we call
     each image close, medium or far. A 5x3 coarse depth map of the scene is constructed quickly by simply comparing the
     sharpness values of each three corresponding regions. Thus, the depth map assigns only two bits of depth information
     to each region using the values close, medium and far. The critical step is to adjust the focus positions of all
     three cameras so that flat ground in front of the obstacle results in medium readings in one row of the depth
     map. Then, unexpected readings of either close or far will indicate convex and concave obstacles respectively,
     enabling basic obstacle avoidance in the vicinity of objects on the ground as well as drop-offs into the ground.
     Although sufficient for obstacle avoidance, the above depth from focus algorithm presents unsatisfyingly coarse
     range information. The alternative is depth from defocus, the most desirable of the focus-based vision techniques.
     Depth from defocus methods take as input two or more images of the same scene, taken with different, known
     camera geometry. Given the images and the camera geometry settings, the goal is to recover the depth information
     of the three-dimensional scene represented by the images. We begin by deriving the relationship between the
     actual scene properties (irradiance and depth), camera geometry settings and the image g that is formed at the
     image plane. The focused image f(x,y) of a scene is defined as follows. Consider a pinhole aperture (L=0) in lieu
     of the lens. For every point p at position (x,y) on the image plane, draw a line through the pinhole aperture
     to the corresponding, visible point P in the actual scene. We define f(x,y) as the irradiance (or light intensity)
     at p due to the light from P. Intuitively, f(x,y) rep- resents the intensity image of the scene perfectly in focus
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-4" sec="section-4">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.1.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.4
      </small>
      <a id="x3-370001.4">
      </a>
      Feature  Extraction
     </h2>
     <p class="noindent">
      An
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      must be able to
      <alert style="color: #821131;">
       determine its relationship
      </alert>
      to the environment by
      <alert style="color: #821131;">
       making measurements with its sensors
      </alert>
      and then using those
     measured signals. A wide variety of sensing technologies are available, as we discussed previously. But every sensor we have presented is imperfect:
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       measurements
      always
      have
      error
      and,
      therefore,
      un-
      certainty
      associated
      with
      them.
      </p>
     </div>
     <p class="noindent">
      Therefore, sensor inputs must be used in a way that enables the robot to interact with its environment successfully in
     spite of measurement uncertainty. There are two <alert style="color: #821131;">(2)</alert>
     strategies for using uncertain sensor input to guide the
     robot’s behavior. One strategy is to use each sensor measurement as a raw and individual value. Such raw sensor
     values could for example be tied directly to robot behavior, whereby the robot’s actions are a function of its
     sensor inputs. Alternatively, the raw sensors values could be used to update an intermediate model, with the
     robot’s actions being triggered as a function of this model rather than the individual sensor measurements.
     </p>
     <p class="noindent">
      The second strategy is to extract information from one or more sensor readings first, gener- ating a higher-level percept that can
     then be used to inform the robot’s model and perhaps the robot’s actions directly. We call this process feature extraction,
     and it is this next, op- tional step in the perceptual interpretation pipeline (Fig. 4.34) that we will now discuss.
     </p>
     <p class="noindent">
      In practical terms, mobile robots do not necessarily use feature extraction and scene inter- pretation for every activity. Instead,
     robots will interpret sensors to varying degrees depend- ing on each specific functionality. For example, in order to guarantee
     emergency stops in the face of immediate obstacles, the robot may make direct use of raw forward-facing range readings to stop
     its drive motors. For local obstacle avoidance, raw ranging sensor strikes may be combined in an occupancy grid model, enabling
     smooth avoidance of obstacles meters away. For map-building and precise navigation, the range sensor values and even vision
     sensor measurements may pass through the complete perceptual pipeline, being sub- jected to feature extraction followed by
     scene interpretation to minimize the impact of indi- vidual sensor uncertainty on the robustness of the robot’s map-making and
     navigation skills. The pattern that thus emerges is that, as one moves into more sophisticated, long-term per-
     ceptual tasks, the feature extraction and scene interpretation aspects of the perceptual pipe- line become essential.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.4.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.4.1
      </small>
      <a id="x3-380001.4.1">
      </a>
      Defining  Feature
     </h3>
     <p class="noindent">
      Features are recognizable structures of elements in the environment. They usually can be extracted from measurements and
     mathematically described. Good features are always perceivable and easily detectable from the environment.
     We distinguish between low-level fea- tures (geometric primitives) like lines, circles or polygons and high-level
     features (objects) such as edges, doors, tables or a trash can. At one extreme, raw sensor data provides a large
     volume of data, but with low distinctiveness of each individual quantum of data. Making use of raw data has the
     potential advantage that every bit of information is fully used, and thus there is a high conservation of information.
     Low level features are abstractions of raw data, and as such provide a lower volume of data while increasing the
     distinctiveness of each fea- ture. The hope, when one incorporates low level features, is that the features are filtering
     out poor or useless data, but of course it is also likely that some valid information will be lost as a result of the
     feature extraction process. High level features provide maximum ab- straction from the raw data, thereby reducing
     the volume of data as much as possible while providing highly distinctive resulting features. Once again, the
     abstraction process has the risk of filtering away important information, potentially lowering data utilization.
     </p>
     <p class="noindent">
      Although features must have some spatial locality, their geometric extent can range widely. For example, a corner
     feature inhabits a specific coordinate location in the geometric world. In contract, a visual "fingerprint" identifying
     a specific room in an office building applies to the entire room, but has a location that is spatially limited to
     the one, particular room. In mobile robotics, features play an especially important role in the creation of
     environmen- tal models. They enable more compact and robust descriptions of the environment, helping a mobile
     robot during both map-building and localization. When designing a mobile robot, a critical decision revolves
     around choosing the appropriate features for the robot to use. A number of factors are essential to this decision:
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Target Environment
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        For
       geometric
       features
       to
       be
       useful,
       the
       target
       geometries
       must
       be
       readily
       detected
       in
       the
       actual
       environment.
       For
       example,
       line
       features
       are
       extremely
       useful
       in
       office
       building
       environments
       due
       to
       the
       abundance
       of
       straight
       walls
       segments
       while
       the
       same
       feature
       is
       virtually
       useless
       when
       navigating
       Mars.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Available Sensors
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Obviously
       the
       specific
       sensors
       and
       sensor
       uncertainty
       of
       the
       robot
       im-
       pacts
       the
       appropriateness
       of
       various
       features.
       Armed
       with
       a
       laser
       rangefinder,
       a
       robot
       is
       well
       qualified
       to
       use
       geometrically
       detailed
       features
       such
       as
       corner
       features
       due
       to
       the
       high
       qual-
       ity
       angular
       and
       depth
       resolution
       of
       the
       laser
       scanner.
       In
       contrast,
       a
       sonar-equipped
       robot
       may
       not
       have
       the
       appropriate
       tools
       for
       corner
       feature
       extraction.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Computational Power
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Vision-based
       feature
       extraction
       can
       effect
       a
       significant
       computa-
       tional
       cost,
       particularly
       in
       robots
       where
       the
       vision
       sensor
       processing
       is
       performed
       by
       one
       of
       the
       robot’s
       main
       processors.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Environment representation
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Feature
       extraction
       is
       an
       important
       step
       toward
       scene
       inter-
       pretation,
       and
       by
       this
       token
       the
       features
       extracted
       must
       provide
       information
       that
       is
       consonant
       with
       the
       representation
       used
       for
       the
       environment
       model.
       For
       example,
       non-geometric
       vi-
       sion-based
       features
       are
       of
       little
       value
       in
       purely
       geometric
       environment
       models
       but
       can
       be
       of
       great
       value
       in
       topological
       models
       of
       the
       environment.
       Figure
       4.35
       shows
       the
       application
       of
       two
       different
       representations
       to
       the
       task
       of
       modeling
       an
       office
       building
       hallway.
       Each
       ap-
       proach
       has
       advantages
       and
       disadvantages,
       but
       extraction
       of
       line
       and
       corner
       features
       has
       much
       more
       relevance
       to
       the
       representation
       on
       the
       left.
       Refer
       to
       Chapter
       5,
       Section
       5.5
       for
       a
       close
       look
       at
       map
       representations
       and
       their
       relative
       tradeoffs.
       In
       the
       following
       two
       sections,
       we
       present
       specific
       feature
       extraction
       techniques
       based
       on
       the
       two
       most
       popular
       sensing
       modalitites
       of
       mobile
       robotics:
       range
       sensing
       and
       visual
       appear-
       ance-based
       sensing.
       </p>
      </dd>
     </dl>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.1.4.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       1.4.2
      </small>
      <a id="x3-390001.4.2">
      </a>
      Using  Range  Data
     </h3>
     <p class="noindent">
      Most of today’s features extracted from ranging sensors are geometric primitives such as line segments or circles. The main reason
     for this is that for most other geometric primitives the parametric description of the features becomes too complex and no closed form
     solution exists. Here we will describe line extraction in detail, demonstrating how the uncertainty models presented above can be
     applied to the problem of combining multiple sensor mea- surements. Afterwards, we briefly present another very successful feature for
     indoor mobile robots, the corner feature, and demonstrate how these features can be combined in a single representation.
      <a id="subsubsection*.20">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-40000">
      </a>
      Line  Extraction
     </h5>
     <p class="noindent">
      Geometric feature extraction is usually the process of comparing and matching measured sensor data against a predefined
     description, or template, of the expect feature. Usually, the system is overdetermined in that the number of sensor measurements
     exceeds the number of feature parameters to be estimated. Since the sensor measurements all have some error, there is
     no perfectly consistent solution and, instead, the problem is one of optimization. One can, for example, extract
     the feature that minimizes the discrepancy with all sensor measurements used (e.g. least squares estimation).
     In this section we present an optimization-based solution to the problem of extracting a line feature from a set
     of uncertain sensor measurements. For greater detail than is presented be- low, refer to [19], pp. 15 and 221.
      <a id="subsubsection*.21">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x3-41000">
      </a>
      Probabilistic  Line  Extraction
     </h5>
     <p class="noindent">
      4.36. There is uncertainty associated with each of the noisy range sensor measurements, and so there is no single line that passes
     through the set. Instead, we wish to select the best pos- sible match, given some optimization criterion. More formally, suppose n
     ranging measurement points in polar coordinates x = ( ,  ) iii are produced by the robot’s sensors. We know that there is
     uncertainty associated with each measurement, and so we can model each measurement using two random variables X = ( P , Q )
     . In this analysis we assume that uncertainty with respect to the actual value iii of P and Q are independent. Based on
     Equation (4.56) we can state this formally: Furthermore, we will assume that each random variable is subject to
     a Gaussian probability density curve, with a mean at the true value and with some specified variance: Given
     some measurement point (, ) , we can calculate the corresponding Euclidean co- ordinatesasx = cos andy = sin.
     Iftherewerenoerror,wewouldwanttofinda line for which all measurements lie on that line: Of course there is measurement error,
     and so this quantity will not be zero. When it is non- zero, this is a measure of the error between the measurement point (, ) and
     the line, spe- cifically in terms of the minimum orthogonal distance between the point and the line. It is always
     important to understand how the error that shall be minimized is being measured. For example a number of line
     extraction techniques do not minimize this orthogonal point- line distance, but instead the distance parallel to the
     y-axis between the point and the line. A good illustration of the variety of optimization criteria is available in [18]
     where several algorithms for fitting circles and ellipses are presented which minimize algebraic and geo- metric
     distances. For each specific ( ,  ) , we can write the orthogonal distance d between ( ,  ) and iiiii the line as:
     </p>
    </div>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="MobileRoboticsLectureBookch2.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="MobileRoboticsLectureBookli1.html" style="float: left;">
       ← Previous Chapter
      </a>
     </p>
    </div>
   </footer>
  </div>
  <div class="page">
   <article class="chapter">
    <p class="noindent">
     <a id="tailMobileRoboticsLectureBookch1.html">
     </a>
    </p>
   </article>
  </div>
 </body>
</html>
