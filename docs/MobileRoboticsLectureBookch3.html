<!DOCTYPE html>
<html lang="en-US" xml:lang="en-US">
 <head>
  <title>
   3 Mobile Robot Localisation
  </title>
  <meta charset="utf-8"/>
  <meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
  <meta content="width=device-width,initial-scale=1" name="viewport"/>
  <link href="MobileRoboticsLectureBook.css" rel="stylesheet" type="text/css"/>
  <meta content="MobileRoboticsLectureBook.tex" name="src"/>
  <script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript">
  </script>
  <link href="flatweb.css" rel="stylesheet" type="text/css"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js">
  </script>
  <script src="flatjs.js">
  </script>
  <link href="http://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" type="text/css"/>
  <link href="https://unpkg.com/nord-highlightjs@0.1.0/dist/nord.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.3.1/highlight.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/dockerfile.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/cmake.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/cpp.min.js">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/makefile.min.js">
  </script>
  <script src="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.js">
  </script>
  <link href="https://unpkg.com/highlightjs-copy/dist/highlightjs-copy.min.css" rel="stylesheet"/>
  <script src="flatjs.js">
  </script>
  <header>
  </header>
 </head>
 <body id="top">
  <nav class="wide">
   <div class="contents">
    <h3 style="font-weight:lighter; padding: 20px 0 20px 0;">
     3
   
     
   

   Mobile Robot Localisation
    </h3>
    <h4 style="font-weight:lighter">
     Chapter Table of Contents
    </h4>
    <ul>
     <div class="chapterTOCS">
      <li>
       <span class="sectionToc">
        <small>
         3.1
        </small>
        <a href="#x5-680003.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.2
        </small>
        <a href="#x5-690003.2">
         The problems of Noise and Aliasing
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.1
        </small>
        <a href="#x5-700003.2.1">
         Sensor Noise
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.2
        </small>
        <a href="#x5-710003.2.2">
         Sensor Aliasing
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.2.3
        </small>
        <a href="#x5-720003.2.3">
         Effector Noise
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.3
        </small>
        <a href="#x5-730003.3">
         Localisation v. Hard-Coded Navigation
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.4
        </small>
        <a href="#x5-740003.4">
         Representing Belief
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.4.1
        </small>
        <a href="#x5-750003.4.1">
         Single Hypothesis Belief
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.4.2
        </small>
        <a href="#x5-760003.4.2">
         Multiple Hypothesis Belief
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.5
        </small>
        <a href="#x5-770003.5">
         Representing Maps
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.5.1
        </small>
        <a href="#x5-780003.5.1">
         Continuous Representation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.5.2
        </small>
        <a href="#x5-790003.5.2">
         Decomposition Methods
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.5.3
        </small>
        <a href="#x5-800003.5.3">
         Current Challenges
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.6
        </small>
        <a href="#x5-810003.6">
         Probabilistic Map-Based Localisation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.6.1
        </small>
        <a href="#x5-820003.6.1">
         Introduction
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.6.2
        </small>
        <a href="#x5-830003.6.2">
         Markov Localisation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.6.3
        </small>
        <a href="#x5-860003.6.3">
         Kalman Filter Localisation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.6.4
        </small>
        <a href="#x5-870003.6.4">
         An Implementation of Kalman Filter
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.7
        </small>
        <a href="#x5-890003.7">
         Other Examples of Localisation Methods
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.7.1
        </small>
        <a href="#x5-900003.7.1">
         Landmark-based Navigation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.7.2
        </small>
        <a href="#x5-910003.7.2">
         Globally Unique Localisation
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.7.3
        </small>
        <a href="#x5-920003.7.3">
         Positioning Beacon systems
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.7.4
        </small>
        <a href="#x5-930003.7.4">
         Route-Based Localisation
        </a>
       </span>
      </li>
      <li>
       <span class="sectionToc">
        <small>
         3.8
        </small>
        <a href="#x5-940003.8">
         Building Maps
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.8.1
        </small>
        <a href="#x5-950003.8.1">
         Stochastic Map Technique
        </a>
       </span>
      </li>
      <li>
       <span class="subsectionToc" style="font-size:12px;margin:0px;">
        <small>
         3.8.2
        </small>
        <a href="#x5-960003.8.2">
         Other Mapping Techniques
        </a>
       </span>
      </li>
     </div>
    </ul>
    <div class="prev-next" style="text-align: center">
     <p class="noindent">
      <a href="MobileRoboticsLectureBookch4.html" style="float:right; font-size:10px">
       NEXT →
      </a>
      <a href="MobileRoboticsLectureBookch2.html" style="float:left; font-size:10px">
       ← PREV
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
    <div id="author-info" style="bottom: 0; padding-bottom: 10px;">
     <div id="author-logo">
      <img src="figures/logo.svg" style="margin: 10px 0;"/>
      <div>
       <div>
        <h4 style="color:#7aa0b8; font-weight:lighter;">
         WebBook | D. T. McGuiness, P.hD
        </h4>
       </div>
      </div>
     </div>
    </div>
   </div>
  </nav>
  <div class="page">
   <article class="chapter">
    <h1 class="chapterHead">
     <div class="number">
      3
     </div>
     <a id="x5-670003">
     </a>
     Mobile Robot Localisation
    </h1>
    <div class="section-control" style="text-align: center">
     <a id="prev-section" onclick="goPrev()" style="float:left;">
      ← Previous Section
     </a>
     <a id="next-section" onclick="goNext()" style="float:right;">
      Next Section →
     </a>
    </div>
    <div class="section-head" id="section-1" sec="section-1">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.1
      </small>
      <a id="x5-680003.1">
      </a>
      Introduction
     </h2>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/robot-navigation.jpg" width="150%"/>
       <a id="x5-68001r1">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.1:
       </span>
       <span class="content">
        Navigation  is  one  if  not  the  most  demanding  and  complicated  task  in
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>.  However  a  successful
       implementation will result in a versatile
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        which can find its way in unknown environments such as
       exploring other planets <span class="cite"><a href="MobileRoboticsLectureBookad1.html#UoW2023">[49]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      Navigation is one of, if not, the most challenging problem faced by an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      and for the robot to be
able to successfully navigate its environment, it requires four <alert style="color: #821131;">(4)</alert> functions:
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10">
        Perception
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        the robot must be able to interpret its sensors to extract meaningful data,
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10">
        Localisation
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        the robot must be able to determine its position within the environment,
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10">
        Cognition
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        the robot must be able to decide how to act to achieve its goals,
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10">
        Motion control
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        the robot must be able to modulate its motor outputs to achieve the desired trajectory.
       </p>
      </dd>
     </dl>
     <div class="figure">
      <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/localisation-scheme.png" width="150%"/>
      <a id="x5-68003r2">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.2:
       </span>
       <span class="content">
        General schematic for mobile robot localisation.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      Of these four <alert style="color: #821131;">(4)</alert> aforementioned components, localisation has received the greatest research attention
in the past and, as a result, significant advances have been made on this front, presented in <a id="x5-68004"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#bailey2002mobile">[50]</a>, <a id="x5-68005"></a><a href="MobileRoboticsLectureBookad1.html#campbell2020localization">[51]</a></span>,
and <a id="x5-68006"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#hodge2023sensors">[52]</a></span>. In this chapter, we will explore the successful localisation methodologies and techniques used
in academic research and industrial application <a id="x5-68007"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#jaradat2017challenges">[53]</a></span>.
     </p>
     <p class="noindent">
      The structure of the chapter is as follows:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        We will describe how sensor and effector uncertainty is responsible for the difficulties of
localisation in Section
        <a href="#x5-690003.2">
         3.2
        </a>,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Then, in Section
        <a href="#x5-730003.3">
         3.3
        </a>, we will have a look at the two <alert style="color: #821131;">(2)</alert> extreme approaches to dealing with the
challenge of robot localisation <a id="x5-68008"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#filliat2003map">[54]</a></span>:
       </p>
       <ul class="itemize2">
        <li class="itemize">
         <p class="noindent">
          Avoiding localisation altogether,
         </p>
        </li>
        <li class="itemize">
         <p class="noindent">
          Performing explicit map-based localisation
         </p>
        </li>
       </ul>
      </li>
      <li class="itemize">
       <p class="noindent">
        The remainder of the chapter discusses the question of representation, which we will have a look
at different case studies of successful localisation systems using a variety of representations and
techniques to achieve
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        localisation.
       </p>
      </li>
     </ul>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-2" sec="section-2">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.2
      </small>
      <a id="x5-690003.2">
      </a>
      The problems of Noise and Aliasing
     </h2>
     <p class="noindent">
      If one could attach an accurate
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      sensor to an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>, much of the localisation problem would be
obviated.
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      would then inform the robot of its
      <alert style="color: #821131;">
       exact
      </alert>
      position and orientation, indoors and
outdoors, so the answers to the questions,
     </p>
     <div class="quoteblock">
      <p class="noindent">
       <italic>
        Where am I?
       </italic>
       ,
       <italic>
        Where am I going?
       </italic>
       , and,
       <italic>
        How should I get there?
       </italic> <a id="x5-69001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#leonard2012directed">[55]</a></span>
      </p>
     </div>
     <p class="noindent">
      would
      <alert style="color: #821131;">
       always be immediately available
      </alert>
      .
     </p>
     <p class="noindent">
      Unfortunately,
such
a
sensor
is
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      currently
practical.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         1
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          1
         </sup>
        </span>
       </alert>
       <span id="textcolor81">
        Of
       course,
       this
       misleading
       statement
       as
       we
       have
       technology
       which
       allows
       the
       shrinking
       of
       errors
       down
       to
       cm
       using
       real-time
       kinematic
       positioning
       which
       is
       used
       to
       correct
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gnss">
         Global
        Navigation
        Satellite
        System
        (GNSS)
        </a>,
       which
       transmits
       the
       robot’s
       location
       by
       longitude,
       latitude,
       altitude,
       and
       a
       timestamp <a id="x5-69002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#hodge2023sensors">[52]</a></span>.
       </span>
      </span>
      The
     existing
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      network
     provides
     accuracy
     to
     within
     several
     m <a id="x5-69003"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#wing2005consumer">[56]</a></span>,
     which
     is
     still
     not
     the
     optimal
     accuracy
     for
     localising
     human-scale
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     as
     well
     as
     miniature
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     such
     as
     desk
     robots
     and
     the
     body-navigating
     nano-robots
     of
     the
     future.
     </p>
     <p class="noindent">
      In
     addition,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      cannot
     function
     indoors
     or
     in
     obstructed
     areas
     and
     are
     therefore
     limited
     in
     their
     workspace.
     But,
     looking
     beyond
     the
     limitations
     of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>,
     localisation
     implies
     more
     than
     knowing
     one’s
     absolute
     position
     in
     the
     Earth’s
     reference
     frame.
     </p>
     <p class="noindent">
      Consider
     a
     robot
     which
     is
     interacting
     with
     humans.
     This
     robot
     may
     need
     to
     identify
     its
     absolute
     position,
     but
     its
     relative
     position
     with
     respect
     to
     target
     humans
     is
     also
     equally
     important.
     Its
     localisation
     task
     can
     include:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        identifying humans using its sensor array <a id="x5-69004"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#bellotto2007people">[57]</a></span>,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        then computing its relative position to the humans.
       </p>
      </li>
     </ul>
     <p class="noindent">
      Furthermore,
     during
     operation
     a
     robot
     will
     select
     a
     strategy
     for
     achieving
     its
     goals.
     If
     it
     intends
     to
     reach
     a
     particular
     location,
     then
     localisation
     may
     not
     be
     enough.
     The
     robot
     may
     need
     to
     acquire
     or
     build
     an
     environmental
     model,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         2
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          2
         </sup>
        </span>
       </alert>
       <span id="textcolor84">
        i.e.,
       a
       map
       representing
       2D
       space
       if
       it
       is
       an
       indoor
       space
       which
       is
       level,
       or
       a
       3D
       space
       if
       it
       is
       navigating
       rough
       terrain.
       </span>
      </span>
      which
     aids
     it
     in
     planning
     a
     path
     to
     the
     goal.
     </p>
     <div class="warning">
      <p class="noindent">
       Localisation means more than simply determining an absolute pose in space. It means building a map, then identifying the robot’s position relative to that map.
      </p>
     </div>
     <div class="knowledge">
      <p class="noindent">
       Clearly, the robot’s sensors and effectors play an integral role in all the above forms of localisation. It is because
      of the inaccuracy and incompleteness of these sensors and effectors localisation poses difficult challenges.
      </p>
     </div>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.2.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.2.1
      </small>
      <a id="x5-700003.2.1">
      </a>
      Sensor  Noise
     </h3>
     <p class="noindent">
      Sensors are the fundamental robot input for the process of perception, and therefore the degree to which sensors can
     discriminate world state is critical. Sensor noise produces a
      <alert style="color: #821131;">
       limitation on the consistency of sensor readings
      </alert>
      in
     the same environmental state and, therefore, on the number of useful bits available from each sensor reading.
     </p>
     <div class="warning">
      <p class="noindent">
       Often, the source of sensor noise problems is that some environmental features are not captured by the robot’s representation and are thus overlooked.
      </p>
     </div>
     <p class="noindent">
      For
     example,
     a
     vision
     system
     used
     for
     indoor
     navigation
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         3
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          3
         </sup>
        </span>
       </alert>
       <span id="textcolor85">
        For
       example,
       this
       could
       be
       indoor
       office
       building,
       or
       a
       warehouse.
       </span>
      </span>
      may
     use
     the
     colour
     values
     detected
     by
     its
     colour
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera.
     When
     the
     Sun
     is
     hidden
     by
     clouds,
     the
     illumination
     of
     the
     building’s
     interior
     changes
     due
     to
     windows
     throughout
     the
     building.
     As
     a
     result,
     hue
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         4
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/color-wheel.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          4
         </sup>
        </span>
       </alert>
       <span id="textcolor86">
        One
       of
       the
       properties
       of
       a
       colour,
       defined
       as
       the
       degree
       to
       which
       a
       stimulus
       can
       be
       described
       as
       similar
       to
       or
       different
       from
       stimuli
       that
       are
       described
       as
       red,
       orange,
       yellow,
       green,
       blue,
       violet
       within
       certain
       theories
       of
       colour
       vision.
       </span>
      </span>
      values
     are
     not
     constant.
     The
     colour
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      appears
     noisy
     from
     the
     robot’s
     perspective
     as
     if
     subject
     to
      <alert style="color: #821131;">
       random
error
      </alert>
      ,
     and
     the
     hue
     values
     obtained
     from
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera
     will
     be
     unusable,
     unless
     the
     robot
     is
     able
     to
     note
     the
     position
     of
     the
     Sun
     and
     clouds
     in
     its
     representation.
     </p>
     <div class="knowledge">
      <p class="noindent">
       Illumination dependency is only one example of the apparent noise in a vision-based sensor system <a id="x5-70001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#sridharan2009color">[58]</a></span>. Picture jitter, signal gain,
      blooming and blurring are all additional sources of noise, potentially reducing the useful content of a colour video image.
      </p>
     </div>
     <p class="noindent">
      Consider the noise level of ultrasonic range-measuring sensors, such as sonars, as we discussed previously. When a sonar transducer
     emits sound towards a relatively smooth and angled surface, much of the signal will coherently reflect away, failing to generate a
     return echo. Depending on the material characteristics, a small amount of energy may return nonetheless. When this level is close
     to the gain threshold of the sonar sensor, then the sonar will, at times, succeed and, at other times, fail to detect the object.
     From the robot’s perspective, a virtually unchanged environmental state will result in two <alert style="color: #821131;">(2)</alert>
     different possible sonar readings:
     </p>
     <div class="warning">
      <p class="noindent">
       one short, and one long which causes an nondeterministic behaviour.
      </p>
     </div>
     <p class="noindent">
      The
     poor
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:snr">
       Signal-to-Noise
      Ratio
      (SNR)
      </a>
      of
     a
     sonar
     sensor
     is
     further
     confounded
     by
     interference
     between
     multiple
     sonar
     emitters.
     Often,
     research
     robots
     have
     between
     12
     to
     48
     sonars
     on
     a
     single
     platform.
     In
     acoustically
     reflective
     environments,
     multipath
     interference
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         5
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          5
         </sup>
        </span>
       </alert>
       <span id="textcolor87">
        The
       propagation
       phenomenon
       resulting
       in
       signals
       reaching
       the
       receiver
       by
       two
       <alert style="color: #821131;">(2)</alert>
       or
       more
       paths.
       Causes
       of
       multipath
       can
       be
       atmospheric
       ducting,
       ionospheric
       reflection
       and
       refraction,
       and
       reflection
       from
       water
       bodies
       and
       terrestrial
       objects
       such
       as
       mountains
       and
       buildings.
       </span>
      </span>
      is
     possible
     between
     the
     sonar
     emissions
     of
     one
     transducer
     and
     the
     echo
     detection
     circuitry
     of
     another
     transducer.
     The
     result
     can
     be
     dramatically
     large
     errors
     in
     ranging
     values
     due
     to
     a
     set
     of
     coincidental
     angles.
     Such
     errors
     occur
     rarely,
     less
     than
     1%
     of
     the
     time,
     and
     are
     virtually
     random
     from
     the
     robot’s
     perspective.
     </p>
     <p class="noindent">
      In
     conclusion,
     sensor
     noise
     reduces
     the
     useful
     information
     content
     of
     sensor
     readings.
     Clearly,
     the
     solution
     is
     to
     take
     multiple
     readings
     into
     account,
     employing
     temporal
     fusion
     or
     multi-sensor
     fusion
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         6
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          6
         </sup>
        </span>
       </alert>
       <span id="textcolor88">
        Sensor
       fusion
       is
       the
       process
       of
       using
       information
       from
       several
       different
       sensors
       to
       estimate
       the
       state
       of
       a
       dynamic
       system.
       The
       resulting
       estimate
       is,
       in
       some
       senses,
       better
       than
       it
       would
       be
       if
       the
       sensors
       were
       used
       individually <a id="x5-70002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#GALAR20171">[59]</a></span>.
       </span>
      </span>
      to
     increase
     the
     overall
     information
     content
     of
     the
     robot’s
     inputs.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.2.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.2.2
      </small>
      <a id="x5-710003.2.2">
      </a>
      Sensor
     Aliasing
     </h3>
     <p class="noindent">
      Aliasing
     is
     the
     second
     major
     shortcoming
     of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      sensors
     which
     cause
     them
     to
     give
     little
     information
     content,
     further
     amplifying
     the
     problem
     of
      <alert style="color: #821131;">
       perception
      </alert>
      and
      <alert style="color: #821131;">
       localisation
      </alert>
      .
     </p>
     <div class="informationblock" id="tcolobox-3">
      <div class="title">
       <p class="noindent">
        <span id="bold" style="font-weight:bold;">
         Information
        </span>
        :
       The
       Human
       Experience
       </p>
      </div>
      <div class="box-content">
       <p class="noindent">
        The problem, known as sensor aliasing, is a phenomenon that humans seldom encounter. The human sensory
       system, particularly the visual system, tends to receive unique inputs in each unique local state within normal
       usage <a id="x5-71001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#williams1985aliasing">[60]</a></span>. In other words, every different place looks different. The power of this unique mapping is only apparent
       when one considers situations where this fails to hold.
       </p>
       <p class="noindent">
        Consider moving through an unfamiliar building that is completely dark. When the visual system sees only black,
       one’s localisation system quickly degrades. Another useful example is that of a human-sized maze made from tall
       hedges. Such mazes have been created for centuries, and humans find them extremely difficult to solve without
       landmarks or clues because, without visual uniqueness, human localisation competence degrades rapidly.
       </p>
      </div>
     </div>
     <p class="noindent">
      In
     robots,
     the
     non-uniqueness
     of
     sensors
     readings,
     or
     sensor
     aliasing,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         7
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/sensor-aliasing.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          7
         </sup>
        </span>
       </alert>
       <span id="textcolor89">
        Sensor
       aliasing
       in
       multiple
       types
       of
       sensors.
       One
       of
       the
       most
       apparent
       one
       is
       usually
       seen
       in
       digital
       images.
       For
       example,
       in
       the
       image
       above,
       due
       to
       low
       sampling,
       moire
       pattern
       starts
       to
       be
       seen <a id="x5-71002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#maksim2006">[61]</a></span>.
       </span>
      </span>
      is
     the
     norm
     and
     not
     the
     exception.
     Consider
     a
     narrow-beam
     rangefinder
     such
     as
     ultrasonic
     or
     infrared
     rangefinders.
     This
     sensor
     provides
     range
     information
     in
     a
     single
     direction
     without
     any
     additional
     data
     regarding
     material
     composition
     such
     as
      <alert style="color: #821131;">
       color
      </alert>
      ,
      <alert style="color: #821131;">
       texture
      </alert>
      and
      <alert style="color: #821131;">
       hardness
      </alert>
      .
     Even
     for
     a
     robot
     with
     several
     such
     sensors
     in
     an
     array,
     there
     are
     a
     variety
     of
     environmental
     states
     that
     would
     trigger
     the
     same
     sensor
     values
     across
     the
     array.
     Formally,
     there
     is
     a
     many-to-one
     mapping
     from
     environmental
     states
     to
     the
     robot’s
     perceptual
     inputs.
     Therefore,
     the
     robot’s
     sensors
     cannot
     distinguish
     from
     among
     these
     many
     states.
     </p>
     <div class="knowledge">
      <p class="noindent">
       A
      classical
      problem
      with
      sonar-based
      robots
      involves
      distinguishing
      between
      humans
      and
      inanimate
      objects
      in
      an
      indoor
      setting
      [
       <a id="x5-71003">
       </a>
       <a href="MobileRoboticsLectureBookad1.html#blumrosen2014noncontact">
        62
       </a>,
       <a id="x5-71004">
       </a>
       <a href="MobileRoboticsLectureBookad1.html#sabatini1998method">
        63
       </a>
       ].
      </p>
      <div class="quoteblock">
       <p class="noindent">
        When
       facing
       an
       apparent
       obstacle
       in
       front
       of
       itself,
       should
       the
       robot
       say
       “Excuse
       me”
       because
       the
       obstacle
       may
       be
       a
       moving
       human,
       or
       should
       the
       robot
       plan
       a
       path
       around
       the
       object
       because
       it
       may
       be
       a
       cardboard
       box?
       </p>
      </div>
      <p class="noindent">
       With
      sonar
      alone,
      these
      states
      are
      aliased
      and
      differentiation
      is
      impossible.
      </p>
     </div>
     <p class="noindent">
      The
     navigation
     problem
     due
     to
     sensor
     aliasing
     is
     that,
     even
     with
     noise-free
     sensors,
     the
     amount
     of
     information
     is
     generally
      <alert style="color: #821131;">
       insufficient
      </alert>
      to
     identify
     the
     robot’s
     accurate
     position
     from
     a
     single
     sensor’s
     reading.
     Therefore
     techniques
     needs
     to
     be
     employed
     by
     the
     robot
     programmer
     which
     base
     the
     robot’s
     localisation
     on
     a
     series
     of
     readings
     and
      <alert style="color: #821131;">
       sufficient
information
      </alert>
      to
     recover
     the
     robot’s
     position
     over
     time.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.2.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.2.3
      </small>
      <a id="x5-720003.2.3">
      </a>
      Effector
     Noise
     </h3>
     <p class="noindent">
      The
     challenges
     of
     localisation
     does
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      lie
     with
     sensor
     technologies
     alone.
     Just
     as
     robot
     sensors
     are
     noisy,
     limiting
     the
     information
     content
     of
     the
     signal,
     so
     do
     the
     robot
     effectors.
     </p>
     <div class="warning">
      <p class="noindent">
       A
      single
      action
      taken
      by
      a
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       may
      have
      several
      different
      possible
      results,
      even
      though
      from
      the
      robot’s
      point
      of
      view
      the
      initial
      state
      before
      the
      action
      was
      taken
      is
      well-known.
      </p>
     </div>
     <p class="noindent">
      In
     short,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      effectors
     introduce
     uncertainty
     about
     future
     state.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         8
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/figures-2.svg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          8
         </sup>
        </span>
       </alert>
       <span id="textcolor90">
        An
       over-exaggerated
       example
       of
       effector
       noise
       where
       the
       motion
       is
       severely
       affected
       by
       the
       uncertainty
       caused
       by
       the
       deterministic
       error.
       </span>
      </span>
      The
     simple
     act
     of
     moving
     tends
     to
      <alert style="color: #821131;">
       increase
the
uncertainty
      </alert>
      of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>.
     There
     are,
     of
     course,
     exceptions.
     Using
     filters
     and
     predictive
     modelling,
     the
     motion
     can
     be
     carefully
     planned
     so
     as
     to
     minimise
     this
     effect,
     and
     indeed
     sometimes
     to
     actually
     result
     in
     more
     certainty.
     Furthermore,
     when
     the
     robot
     actions
     are
     taken
     in
     concert
     with
     careful
     interpretation
     of
     sensory
     feedback,
     it
     can
     compensate
     for
     the
     uncertainty
     introduced
     by
     noisy
     actions
     using
     the
     information
     provided
     by
     the
     sensors.
     </p>
     <p class="noindent">
      First,
     however,
     it
     is
     important
     to
     understand
     the
     precise
     nature
     of
     the
     effector
     noise
     that
     impacts
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>.
     It
     is
     important
     to
     note
     that,
     from
     the
     robot’s
     point
     of
     view,
     this
     error
     in
     motion
     is
     viewed
     as
      <alert style="color: #821131;">
       error
in
the
odometer
      </alert>
      ,
     or
     the
     robot’s
     inability
     to
     estimate
     its
     own
     position
     over
     time
     using
     knowledge
     of
     its
     kinematics
     and
     dynamics.
     The
     true
     source
     of
     error
     generally
     lies
     in
     an
      <alert style="color: #821131;">
       incomplete
model
      </alert>
      of
     the
     environment.
     </p>
     <div class="knowledge">
      <p class="noindent">
       For instance, the robot does
       <span id="bold" style="font-weight:bold;">
        NOT
       </span>
       model the fact that the floor may be sloped, the wheels may slip, and a human may push the robot.
      </p>
     </div>
     <p class="noindent">
      All  of  these  unmodeled  sources  of  error  result  in:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        inaccuracy
       between
       the
       physical
       motion
       of
       the
       robot,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        the
       intended
       motion
       of
       the
       robot,
       and
       the
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        proprioceptive
       sensor
       estimates
       of
       motion.
       </p>
      </li>
     </ul>
     <p class="noindent">
      In
     odometry
     and
     dead
     reckoning
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         9
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          9
         </sup>
        </span>
       </alert>
       <span id="textcolor94">
        The
       process
       of
       calculating
       the
       current
       position
       of
       a
       moving
       object
       by
       using
       a
       previously
       determined
       position,
       or
       fix,
       and
       incorporating
       estimates
       of
       speed,
       heading
       (or
       direction
       or
       course),
       and
       elapsed
       time.
       </span>
      </span>
      the
     position
     update
     is
     based
     on
     proprioceptive
     sensors.
     The
     movement
     of
     the
     robot,
     sensed
     with
     wheel
     encoders
     and
     /or
     heading
     sensors
     is
     integrated
     to
     compute
     position.
     Because
     the
     sensor
     measurement
     errors
     are
     integrated,
     the
     position
     error
     accumulates
     over
     time.
     Thus
     the
     position
     has
     to
     be
     updated
     from
     time
     to
     time
     by
     other
     localisation
     mechanisms.
     Otherwise
     the
     robot
     is
     not
     able
     to
     maintain
     a
     meaningful
     position
     estimate
     in
     long
     run.
     </p>
     <div class="warning">
      <p class="noindent">
       In
      the
      following
      we
      will
      concentrate
      on
      odometry
      based
      on
      the
      wheel
      sensor
      readings
      of
      a
      differential
      drive
      robot
      only <a id="x5-72001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#batavia2000path">[64]</a></span>.
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          10
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           10
          </sup>
         </span>
        </alert>
        <span id="textcolor95">
         Using
        additional
        heading
        sensors
        (e.g.
        gyroscope)
        can
        help
        to
        reduce
        the
        cumulative
        errors,
        but
        the
        main
        problems
        remain
        the
        same.
        </span>
       </span>
      </p>
     </div>
     <p class="noindent">
      There
     are
     many
     sources
     of
     odometric
     error,
     from
     environmental
     factors
     to
     resolution:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        Limited
       resolution
       during
       integration
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           11
          </sup>
         </span>
        </alert>
        <span class="marginnote">
         <alert style="color: #821131;">
          <span id="bold" style="font-weight:bold;">
           <sup class="textsuperscript">
            11
           </sup>
          </span>
         </alert>
         <span id="textcolor97">
          time
         increments,
         measurement
         resolution
         </span>
        </span>
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Misalignment
       of
       wheels
       causing
        <alert style="color: #821131;">
         deterministic
        </alert>
        error,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Unequal
       wheel
       diameter,
       which
       again,
       causing
        <alert style="color: #821131;">
         deterministic
        </alert>
        error,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Unequal
       floor
       contact,
       which
       can
       cause
        <alert style="color: #821131;">
         slipping
        </alert>
        during
       operation.
       </p>
      </li>
     </ul>
     <p class="noindent">
      Some
     of
     the
     errors
     might
     be
     deterministic
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         12
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          12
         </sup>
        </span>
       </alert>
       <span id="textcolor101">
        To
       reiterate,
       deterministic
       errors
       are
       any
       errors
       which
       can
       be
       avoided
       and
       are
       generally
       caused
       by
       bad
       design
       or
       poorly
       calibrated
       sensors.
       </span>
      </span>
      (systematic).
     However,
     there
     are
     still
     a
     number
     of
     non-deterministic
     (random)
     errors
     which
     remain,
     leading
     to
     uncertainties
     in
     position
     estimation
     over
     time.
     From
     a
     geometric
     point
     of
     view
     one
     can
     classify
     the
     errors
     into
     three
     <alert style="color: #821131;">(3)</alert>
     types:
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Range error
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Integrated
       path
       length
       of
       the
       robot
       movement,
       as
       in
       the
       sum
       of
       wheel
       motion.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Turn error
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Similar
       to
       range
       error,
       but
       for
       turns
       which
       are
       difference
       of
       the
       wheel
       motions.
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Drift error
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        difference
       in
       the
       error
       of
       the
       wheels
       leads
       to
       an
       error
       in
       the
       robot’s
       angular
       orientation.
       </p>
      </dd>
     </dl>
     <p class="noindent">
      Over
     long
     periods
     of
     time,
     turn
     and
     drift
     errors
     far
     outweigh
     range
     errors,
     as
     their
     contribute
     to
     the
     overall
     position
     error
     is
     non-linear.
     Consider
     a
     robot,
     whose
     position
     is
     initially
     perfectly
     well-known,
     moving
     forward
     in
     a
     straight
     line
     along
     the
      <span id="bold" style="font-weight:bold;">
       <span id="textcolor102">
        x
       </span>
      </span>
      axis.
     The
     error
     in
     the
      <span id="bold" style="font-weight:bold;">
       <span id="textcolor103">
        y
       </span>
      </span>
      -position
     introduced
     by
     a
     move
     of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        d
       </mi>
      </math>
      meters
     will
     have
     a
     component
     of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        d
       </mi>
       <mi class="loglike">
        sin
       </mi>
       <mo>
        ⁡
       </mo>
       <mi mathvariant="normal">
        Δ
       </mi>
       <mi>
        𝜃
       </mi>
      </math>
      ,
     which
     can
     be
     quite
     large
     as
     the
     angular
     error
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi mathvariant="normal">
        Δ
       </mi>
       <mi>
        𝜃
       </mi>
      </math>
      grows.
     Over
     time,
     as
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      moves
     about
     the
     environment,
     the
     rotational
     error
     between
     its
     internal
     reference
     frame
     and
     its
     original
     reference
     frame
     grows
     quickly.
     As
     the
     robot
     moves
     away
     from
     the
     origin
     of
     these
     reference
     frames,
     the
     resulting
     linear
     error
     in
     position
     grows
     quite
     large.
     It
     is
     instructive
     to
     establish
     an
     error
     model
     for
     odometric
     accuracy
     and
     see
     how
     the
     errors
     propagate
     over
     time.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-3" sec="section-3">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.3
      </small>
      <a id="x5-730003.3">
      </a>
      Localisation
     v.
     Hard-Coded
     Navigation
     </h2>
     <p class="noindent">
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-73001r3">
       3.3
      </a>
      depicts
     a
     standard
     indoor
     environment
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      is
     set
     to
     navigate.
     Now,
     suppose
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      in
     question
     must
     deliver
     messages
     between
     two
     <alert style="color: #821131;">(2)</alert>
     specific
     rooms
     in
     this
     environment:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       These
      are
      rooms
       <alert style="color: #821131;">
        A
       </alert>
       and
       <alert style="color: #821131;">
        B
       </alert>
       .
      </p>
     </div>
     <p class="noindent">
      In
     creating
     a
     navigation
     system
     for
     this
     task,
     it
     is
     clear
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      will
     need
     sensors
     and
     a
     motion
     control
     system.
     Sensors
     are
     required
     to
     avoid
     hitting
     moving
     obstacles
     such
     as
     humans,
     and
     some
     motion
     control
     system
     is
     required
     so
     that
     the
     robot
     can
     actively
     move.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/a-sample-environment.png" width="150%"/>
       <a id="x5-73001r3">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.3:
       </span>
       <span class="content">
        A sample environment.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      It
     is
     less
     evident,
     however,
     whether
     or
     not
     this
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      will
     require
     a
     localisation
     system.
     Localisation
     may
     seem
     mandatory
     to
     successfully
     navigate
     between
     the
     two
     <alert style="color: #821131;">(2)</alert>
     rooms.
     It
     is
     through
     localising
     on
     a
     map,
     after
     all,
     which
     the
     robot
     can
     hope
     to
     recover
     its
     position
     and
     detect
     when
     it
     has
     arrived
     at
     the
     goal
     location.
     It
     is
     true
     that,
     at
     the
     least,
     the
     robot
     must
     have
     a
     way
     of
     detecting
     the
     goal
     location.
     However,
     explicit
     localisation
     with
     reference
     to
     a
     map
     is
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      the
     only
     strategy
     that
     qualifies
     as
     a
     goal
     detector.
     </p>
     <p class="noindent">
      An
     alternative,
     adopted
     by
     the
     behaviour-based
     community,
     suggests
     that,
     since
     sensors
     and
     effectors
     are
     noisy
     and
     information-limited,
     one
     should
      <alert style="color: #821131;">
       avoid
      </alert>
      creating
     a
     geometric
     map
     for
     localisation.
     Instead,
     they
     suggest
     designing
     sets
     of
     behaviours
     which
     together
     result
     in
     the
      <alert style="color: #821131;">
       desired
robot
motion
      </alert>
      .
     </p>
     <div class="warning">
      <p class="noindent">
       In
      its
      essence,
      this
      approach
      avoids
      explicit
      reasoning
      about
      localisation
      and
      position,
      and
      therefore
      generally
      avoids
      explicit
      path
      planning
      as
      well.
      </p>
     </div>
     <p class="noindent">
      This
     technique
     is
     based
     on
     a
     idea
     that,
     there
     exists
     a
     procedural
     solution
     to
     the
     particular
     navigation
     problem
     at
     hand.
     For
     example,
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-73001r3">
       3.3
      </a>,
     the
     behavioralist
     approach
     to
     navigating
     from
     Room
      <alert style="color: #821131;">
       A
      </alert>
      to
     Room
      <alert style="color: #821131;">
       B
      </alert>
      might
     be
     to
     design
     a
     left-wall-following
     behavior
     and
     a
     detector
     for
     Room
     B
     that
     is
     triggered
     by
     some
     unique
     queue
     in
     Room
     B,
     such
     as
     the
     color
     of
     the
     carpet.
     Then,
     the
     robot
     can
     reach
     Room
     B
     by
     engaging
     the
     left
     wall
     follower
     with
     the
     Room
     B
     detector
     as
     the
     termination
     condition
     for
     the
     program.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/behaviour-based-navigation.png" width="150%"/>
       <a id="x5-73002r4">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.4:
       </span>
       <span class="content">
        An Architecture for Behavior-based Navigation
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The
     architecture
     of
     this
     solution
     to
     a
     specific
     navigation
     problem
     is
     shown
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-73002r4">
       3.4
      </a>.
     The
     key
     advantage
     of
     this
     method
     is
     that,
     when
     possible,
     it
     may
     be
     implemented
     very
     quickly
     for
     a
     single
     environment
     with
     a
     small
     number
     of
     goal
     positions.
     It
     suffers
     from
     some
     disadvantages,
     however.
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        The
       method
       does
       not
       directly
       scale
       to
       other
       environments
       or
       to
       larger
       environments.
       Often,
       the
       navigation
       code
       is
       location-specific,
       and
       the
       same
       degree
       of
       coding
       and
       debugging
       is
       required
       to
       move
       the
       robot
       to
       a
       new
       environment.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        The
       underlying
       procedures,
       such
       as
       left-wall-follow,
       must
       be
       carefully
       designed
       to
       produce
       the
       desired
       behaviour.
       This
       task
       may
       be
       time-consuming
       and
       is
       heavily
       dependent
       on
       the
       specific
       robot
       hardware
       and
       environmental
       characteristics.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        A
       behaviour-based
       system
       may
       have
       multiple
       active
       behaviors
       at
       any
       one
       time.
       Even
       when
       individual
       behaviours
       are
       tuned
       to
       optimise
       performance,
       this
       fusion
       and
       rapid
       switching
       between
       multiple
       behaviors
       can
       negate
       that
       fine-tuning.
       Often,
       the
       addition
       of
       each
       new
       incremental
       behavior
       forces
       the
       robot
       designer
       to
       re-tune
       all
       of
       the
       existing
       behaviors
       again
       to
       ensure
       that
       the
       new
       interactions
       with
       the
       freshly
       introduced
       behavior
       are
       all
       stable
       </p>
      </li>
     </ul>
     <p class="noindent">
      In
     contrast
     to
     the
     behaviour-based
     approach,
     the
     map-based
     approach
     includes
     both
     localisation
     and
     cognition
     modules
     shown
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-73003r5">
       3.5
      </a>.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/model-based-navigation.png" width="150%"/>
       <a id="x5-73003r5">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.5:
       </span>
       <span class="content">
        An Architecture for Map-based (or model-based) Navigation
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      In
     map-based
     navigation,
     the
     robot
      <alert style="color: #821131;">
       explicitly
      </alert>
      attempts
     to
     localise
     by
     collecting
     sensor
     data,
     then
     updating
     some
     belief
     about
     its
     position
     with
     respect
     to
     a
     map
     of
     the
     environment.
     The
     key
     advantages
     of
     the
     map-based
     approach
     for
     navigation
     are
     as
     follows:
     </p>
     <aside class="wrapfig-r">
      <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/spirit-xkcd.png" width="100%"/>
      <a id="x5-73004r6">
      </a>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.6:
       </span>
       <span class="content">
        On  January  26th,  2274  Mars  days  into  the  mission,
       NASA declared Spirit a ’stationary research station’, expected to stay
       operational for several more months until the dust buildup on its solar
       panels forces a final shutdown <span class="cite"><a href="MobileRoboticsLectureBookad1.html#xkcd2025">[65]</a></span>.
       </span>
      </figcaption>
     </aside>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        The
       explicit,
       map-based
       concept
       of
       position
       makes
       the
       system’s
       belief
       about
       position
       transparently
       available
       to
       the
       human
       operators.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        The
       existence
       of
       the
       map
       itself
       represents
       a
       medium
       for
       communication
       between
       human
       and
       robot
       as
       the
       human
       can
       simply
       give
       the
       robot
       a
       new
       map
       if
       the
       robot
       goes
       to
       a
       new
       environment.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        The
       map,
       if
       created
       by
       the
       robot,
       can
       be
       used
       by
       humans
       as
       well,
       achieving
       two
       uses.
       </p>
      </li>
     </ul>
     <p class="noindent">
      The
     map-based
     approach
     will
     require
     more
     up-front
     development
     effort
     to
     create
     a
     navigating
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>.
     The
     hope
     is
     that
     the
     development
     effort
     results
     in
     an
     architecture
     which
     can
     successfully
     map
     and
     navigate
     a
     variety
     of
     environments,
     thereby
     compensating
     for
     the
     up-front
     design
     cost
     over
     time.
     </p>
     <p class="noindent">
      Of
     course
     the
     primary
     risk
     of
     the
     map-based
     approach
     is
     that
     an
     internal
     representation,
     rather
     than
     the
     real
     world
     itself,
     is
     being
     constructed
     and
     trusted
     by
     the
     robot.
     If
     that
     model
     diverges
     from
     reality,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         13
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          13
         </sup>
        </span>
       </alert>
       <span id="textcolor110">
        As
       in
       if
       the
       robot
       gets
       the
       wrong
       idea
       about
       its
       environment
       and
       draws
       the
       wrong
       map.
       </span>
      </span>
      then
     the
     robot’s
     behaviour
     may
     be
     undesirable
     at
     best
     or
     wrong
     at
     worst,
     even
     if
     the
     raw
     sensor
     values
     of
     the
     robot
     are
     only
     transiently
     incorrect.
     </p>
     <p class="noindent">
      In
     the
     remainder
     of
     this
     chapter,
     we
     focus
     on
     a
     discussion
     of
     map-based
     approaches
     and,
     specifically,
     the
     localisation
     component
     of
     these
     techniques.
     These
     approaches
     are
     particularly
     appropriate
     for
     study
     given
     their
     significant
     recent
     successes
     in
     enabling
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      to
     navigate
     a
     variety
     of
     environments,
     from
     academic
     research
     buildings
     to
     factory
     floors
     and
     museums
     around
     the
     world.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-4" sec="section-4">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.4
      </small>
      <a id="x5-740003.4">
      </a>
      Representing
     Belief
     </h2>
     <p class="noindent">
      The
     fundamental
     issue
     which
     differentiates
     different
     types
     of
     map-based
     localisation
     systems
     is
     the
     issue
     of
      <alert style="color: #821131;">
       representation
      </alert>
      .
     There
     are
     two
     <alert style="color: #821131;">(2)</alert>
     specific
     concepts
     which
     the
     robot
     must
     represent,
     and
     each
     has
     its
     own
     unique
     possible
     solutions.
     </p>
     <dl class="enumerate-enumitem">
      <dt class="enumerate-enumitem">
       1.
      </dt>
      <dd class="enumerate-enumitem">
       Representation
      of
      the
      environment,
      </dd>
      <dt class="enumerate-enumitem">
       2.
      </dt>
      <dd class="enumerate-enumitem">
       The
      map.
      </dd>
     </dl>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       What
      aspects
      of
      the
      environment
      are
      contained
      in
      this
      map?
      At
      what
      level
      of
      fidelity
      does
      the
      map
      represent
      the
      environment?
      These
      are
      the
      design
      questions
      for
      map
      representation.
      </p>
     </div>
     <p class="noindent">
      The
     robot
     must
     also
     have
     a
     representation
     of
     its
      <alert style="color: #821131;">
       belief
      </alert>
      regarding
     its
     position
     on
     the
     map.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       Does
      the
      robot
      identify
      a
      single
      unique
      position
      as
      its
      current
      position,
      or
      does
      it
      describe
      its
      position
      in
      terms
      of
      a
      set
      of
      possible
      positions?
      If
      multiple
      possible
      positions
      are
      expressed
      in
      a
      single
      belief,
      how
      are
      those
      multiple
      positions
      ranked,
      if
      at
      all?
      </p>
     </div>
     <p class="noindent">
      These
     are
     the
     design
     questions
     for
     belief
     representation.
     Decisions
     along
     these
     two
     <alert style="color: #821131;">(2)</alert>
     design
     axes
     can
     result
     in
     varying
     levels
     of
     architectural
     complexity,
     computational
     complexity
     and
     overall
     localisation
     accuracy.
     </p>
     <p class="noindent">
      We
     will
     start
     by
     discussing
     belief
     representation.
     The
     first
     major
     branch
     in
     the
     classification
     of
     belief
     representation
     systems
     differentiates
     between
     single
     hypothesis
     and
     multiple
     hypothesis
     belief
     systems.
     </p>
     <dl class="description">
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Single
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Former
       covers
       solutions
       in
       which
       the
       robot
       postulates
       its
       unique
       position,
       </p>
      </dd>
      <dt class="description">
       <span class="ec-lmssbx-10x-x-70">
        Multiple
       </span>
      </dt>
      <dd class="description">
       <p class="noindent">
        Enables
       a
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
         AMR
        </a>
        to
       describe
       the
       degree
       to
       which
       it
       is
       uncertain
       about
       its
       position.
       </p>
      </dd>
     </dl>
     <p class="noindent">
      A
     sampling
     of
     different
     belief
     and
     map
     representations
     is
     shown
     in
     figure
     5.9.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.4.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.4.1
      </small>
      <a id="x5-750003.4.1">
      </a>
      Single
     Hypothesis
     Belief
     </h3>
     <p class="noindent">
      The
     single
     hypothesis
     belief
     representation
     is
     the
     most
     direct
     possible
     postulation
     of
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     position <a id="x5-75001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#reuter1999scan">[66]</a></span>.
     </p>
     <div class="warning">
      <p class="noindent">
       Given
      some
      environmental
      map,
      the
      robot’s
      belief
      about
      position
      is
      expressed
      as
      a
      single
      unique
      point
      on
      the
      map.
      </p>
     </div>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/single-belief.png" width="150%"/>
       <a id="x5-75002r7">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.7:
       </span>
       <span class="content">
        The three <alert style="color: #821131;">(3)</alert>
       examples of single hypotheses of position using different map representation.
        <span id="bold" style="font-weight:bold;">
         a)
        </span>
        real map
       with walls, doors and furniture
        <span id="bold" style="font-weight:bold;">
         b)
        </span>
        line-based map -&gt; around 100 lines with two parameters
        <span id="bold" style="font-weight:bold;">
         c)
        </span>
        occupancy
       grid based map -&gt; around 3000 gird cells sizing 50x50 cm
        <span id="bold" style="font-weight:bold;">
         d)
        </span>
        topological map using line features (Z/S-lines)
       and doors -&gt; around 50 features and 18 nodes
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      In
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-75002r7">
       3.7
      </a>,
     three
     <alert style="color: #821131;">(3)</alert>
     examples
     of
     a
     single
     hypothesis
     belief
     are
     shown
     using
     three
     different
     map
     representations
     of
     the
     same
     actual
     environment
     shown
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-75002r7">
       3.7
      </a>
      a.
     In
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-75002r7">
       3.7
      </a>
      b,
     a
     single
     point
     is
     geometrically
     annotated
     as
     the
     robot’s
     position
     in
     a
     continuous
     two-dimensional
     geometric
     map.
     In
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-75002r7">
       3.7
      </a>
      c,
     the
     map
     is
     a
     discrete,
     tessellated
     map,
     and
     the
     position
     is
     noted
     at
     the
     same
     level
     of
     fidelity
     as
     the
     map
     cell
     size.
     In
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-75002r7">
       3.7
      </a>
      d,
     the
     map
     is
     not
     geometrical
     at
     all
     but
     abstract
     and
     topological.
     In
     this
     case,
     the
     single
     hypothesis
     of
     position
     involves
     identifying
     a
     single
     node
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        i
       </mi>
      </math>
      in
     the
     topological
     graph
     as
     the
     robot’s
     position.
     </p>
     <p class="noindent">
      The
     principal
     advantage
     of
     the
     single
     hypothesis
     representation
     of
     position
     stems
     from
     the
     fact
     that,
     given
     a
     unique
     belief,
     there
     is
      <alert style="color: #821131;">
       no
position
ambiguity
      </alert>
      .
     The
     unambiguous
     nature
     of
     this
     representation
     facilitates
     decision-making
     at
     the
     robot’s
     cognitive
     level
     (e.g.
     path
     planning).
     The
     robot
     can
     simply
     assume
     that
     its
     belief
     is
     correct,
     and
     can
     then
     select
     its
     future
     actions
     based
     on
     its
     unique
     position.
     </p>
     <p class="noindent">
      Just
     as
     decision-making
     is
     facilitated
     by
     a
     single-position
     hypothesis,
     so
     updating
     the
     robot’s
     belief
     regarding
     position
     is
     also
     facilitated,
     since
     the
     single
     position
     must
     be
     updated
     by
     definition
     to
     a
     new,
     single
     position.
     The
     challenge
     with
     this
     position
     update
     approach,
     which
     ultimately
     is
     the
     principal
     disadvantage
     of
     single-hypothesis
     representation,
     is
     that
     robot
     motion
     often
     induces
     uncertainty
     due
     to
     effectory
     and
     sensory
     noise.
     </p>
     <div class="warning">
      <p class="noindent">
       Forcing
      the
      position
      update
      process
      to
      always
      generate
      a
      single
      hypothesis
      of
      position
      is
      challenging
      and,
      often,
      impossible.
      </p>
     </div>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.4.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.4.2
      </small>
      <a id="x5-760003.4.2">
      </a>
      Multiple
     Hypothesis
     Belief
     </h3>
     <p class="noindent">
      In
     the
     case
     of
     multiple
     hypothesis
     beliefs
     regarding
     position,
     the
     robot
     tracks
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      just
     a
     single
     possible
     position
     but
     a
     possibly
      <alert style="color: #821131;">
       infinite
set
of
positions
      </alert>
      .
     In
     one
     simple
     example
     originating
     in
     the
     work
     of
     Jean-Claude
     Latombe
     [5,
     89],
     the
     robot’s
     position
     is
     described
     in
     terms
     of
     a
     convex
     polygon
     positioned
     in
     a
     two-dimensional
     map
     of
     the
     environment.
     </p>
     <p class="noindent">
      This
     multiple
     hypothesis
     representation
     communicates
     the
     set
     of
     possible
     robot
     positions
     geometrically,
     with
     no
     preference
     ordering
     over
     the
     positions.
     Each
     point
     in
     the
     map
     is
     simply
     either
     contained
     by
     the
     polygon
     and,
     therefore,
     in
     the
     robot’s
     belief
     set,
     or
     outside
     the
     polygon
     and
     thereby
     excluded.
     Mathematically,
     the
     position
     polygon
     serves
     to
     partition
     the
     space
     of
     possible
     robot
     positions.
     Such
     a
     polygonal
     representation
     of
     the
     multiple
     hypothesis
     belief
     can
     apply
     to
     a
     continuous,
     geometric
     map
     of
     the
     environment
     or,
     alternatively,
     to
     a
     tessellated,
     discrete
     approximation
     to
     the
     continuous
     environment.
     </p>
     <p class="noindent">
      It
     may
     be
     useful,
     however,
     to
     incorporate
     some
     ordering
     on
     the
     possible
     robot
     positions,
     capturing
     the
     fact
     that
     some
     robot
     positions
     are
     likelier
     than
     others.
     A
     strategy
     for
     representing
     a
     continuous
     multiple
     hypothesis
     belief
     state
     along
     with
     a
     preference
     ordering
     over
     possible
     positions
     is
     to
     model
     the
     belief
     as
     a
     mathematical
     distribution.
     For
     example,
     [42,47]
     notate
     the
     robot’s
     position
     belief
     using
     an
     X,Y
     point
     in
     the
     two-dimensional
     environment
     as
     the
     mean
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        μ
       </mi>
      </math>
      plus
     a
     standard
     deviation
     parameter
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        σ
       </mi>
      </math>
      ,
     thereby
     defining
     a
     Gaussian
     distribution.
     The
     intended
     interpretation
     is
     that
     the
     distribution
     at
     each
     position
     represents
     the
     probability
     assigned
     to
     the
     robot
     being
     at
     that
     location.
     This
     representation
     is
     particularly
     amenable
     to
     mathematically
     defined
     tracking
     functions,
     such
     as
     the
     Kalman
     Filter,
     that
     are
     designed
     to
     operate
     efficiently
     on
     Gaussian
     distributions.
     </p>
     <p class="noindent">
      An
     alternative
     is
     to
     represent
     the
     set
     of
     possible
     robot
     positions,
     not
     using
     a
     single
     Gaussian
     probability
     density
     function,
     but
     using
     discrete
     markers
     for
     each
     possible
     position.
     In
     this
     case,
     each
     possible
     robot
     position
     is
     individually
     noted
     along
     with
     a
     confidence
     or
     probability
     parameter
     (See
     Fig.
     (5.11)).
     In
     the
     case
     of
     a
     highly
     tessellated
     map
     this
     can
     result
     in
     thousands
     or
     even
     tens
     of
     thousands
     of
     possible
     robot
     positions
     in
     a
     single
     belief
     state.
     </p>
     <p class="noindent">
      The
     key
     advantage
     of
     the
     multiple
     hypothesis
     representation
     is
     that
     the
     robot
     can
     explicitly
     maintain
     uncertainty
     regarding
     its
     position.
     If
     the
     robot
     only
     acquires
     partial
     information
     regarding
     position
     from
     its
     sensors
     and
     effectors,
     that
     information
     can
     conceptually
     be
     incorporated
     in
     an
     updated
     belief.
     </p>
     <p class="noindent">
      A
     more
     subtle
     advantage
     of
     this
     approach
     revolves
     around
     the
     robot’s
     ability
     to
     explicitly
     measure
     its
     own
     degree
     of
     uncertainty
     regarding
     position.
     This
     advantage
     is
     the
     key
     to
     a
     class
     of
     localisation
     and
     navigation
     solutions
     in
     which
     the
     robot
     not
     only
     reasons
     about
     reaching
     a
     particular
     goal,
     but
     reasons
     about
     the
     future
     trajectory
     of
     its
     own
     belief
     state.
     For
     instance,
     a
     robot
     may
     choose
     paths
     that
     minimise
     its
     future
     position
     uncertainty.
     An
     example
     of
     this
     approach
     is
     [90],
     in
     which
     the
     robot
     plans
     a
     path
     from
     point
     A
     to
     B
     that
     takes
     it
     near
     a
     series
     of
     landmarks
     in
     order
     to
     mitigate
     localisation
     difficulties.
     This
     type
     of
     explicit
     reasoning
     about
     the
     effect
     that
     trajectories
     will
     have
     on
     the
     quality
     of
     localisation
     requires
     a
     multiple
     hypothesis
     representation.
     </p>
     <p class="noindent">
      One
     of
     the
     fundamental
     disadvantages
     of
     the
     multiple
     hypothesis
     approaches
     involves
     decision-making.
     If
     the
     robot
     represents
     its
     position
     as
     a
     region
     or
     set
     of
     possible
     positions,
     then
     how
     shall
     it
     decide
     what
     to
     do
     next?
     Figure
     5.11
     provides
     an
     example.
     At
     position
     3,
     the
     robot’s
     belief
     state
     is
     distributed
     among
     5
     hallways
     separately.
     If
     the
     goal
     of
     the
     robot
     is
     to
     travel
     down
     one
     particular
     hallway,
     then
     given
     this
     belief
     state
     what
     action
     should
     the
     robot
     choose?
     </p>
     <p class="noindent">
      The
     challenge
     occurs
     because
     some
     of
     the
     robot’s
     possible
     positions
     imply
     a
     motion
     trajectory
     that
     is
     inconsistent
     with
     some
     of
     its
     other
     possible
     positions.
     One
     approach
     that
     we
     will
     see
     in
     the
     case
     studies
     below
     is
     to
     assume,
     for
     decision-making
     purposes,
     that
     the
     robot
     is
     physically
     at
     the
     most
     probable
     location
     in
     its
     belief
     state,
     then
     to
     choose
     a
     path
     based
     on
     that
     current
     position.
     But
     this
     approach
     demands
     that
     each
     possible
     position
     have
     an
     associated
     probability.
     </p>
     <p class="noindent">
      In
     general,
     the
     right
     approach
     to
     such
     a
     decision-making
     problems
     would
     be
     to
     decide
     on
     trajectories
     that
     eliminate
     the
     ambiguity
     explicitly.
     But
     this
     leads
     us
     to
     the
     second
     major
     disadvantage
     of
     the
     multiple
     hypothesis
     approaches.
     In
     the
     most
     general
     case,
     they
     can
     be
     computationally
     very
     expensive.
     When
     one
     reasons
     in
     a
     three
     dimensional
     space
     of
     discrete
     possible
     positions,
     the
     number
     of
     possible
     belief
     states
     in
     the
     single
     hypothesis
     case
     is
     limited
     to
     the
     number
     of
     possible
     positions
     in
     the
     3D
     world.
     Consider
     this
     number
     to
     be
     N.
     When
     one
     moves
     to
     an
     arbitrary
     multiple
     hypothesis
     representation,
     then
     the
     number
     of
     posN
     sible
     belief
     states
     is
     the
     power
     set
     of
     N,
     which
     is
     far
     larger:
     2
     .
     Thus
     explicit
     reasoning
     about
     the
     possible
     trajectory
     of
     the
     belief
     state
     over
     time
     quickly
     becomes
     computationally
     untenable
     as
     the
     size
     of
     the
     environment
     grows.
     There
     are,
     however,
     specific
     forms
     of
     multiple
     hypothesis
     representations
     that
     are
     somewhat
     more
     constrained,
     thereby
     avoiding
     the
     computational
     explosion
     while
     allowing
     a
     limited
     type
     of
     multiple
     hypothesis
     belief.
     For
     example,
     if
     one
     assumes
     a
     Gaussian
     distribution
     of
     probability
     centered
     at
     a
     single
     position,
     then
     the
     problem
     of
     representation
     and
     tracking
     of
     belief
     becomes
     equivalent
     to
     Kalman
     Filtering,
     a
     straightforward
     mathematical
     process
     described
     below.
     Alternatively,
     a
     highly
     tessellated
     map
     representation
     combined
     with
     a
     limit
     of
     10
     possible
     positions
     in
     the
     belief
     state,
     results
     in
     a
     discrete
     update
     cycle
     that
     is,
     at
     worst,
     only
     10x
     more
     computationally
     expensive
     than
     single
     hypothesis
     belief
     update.
     </p>
     <p class="noindent">
      In
     conclusion,
     the
     most
     critical
     benefit
     of
     the
     multiple
     hypothesis
     belief
     state
     is
     the
     ability
     to
     maintain
     a
     sense
     of
     position
     while
     explicitly
     annotating
     the
     robot’s
     uncertainty
     about
     its
     own
     position.
     This
     powerful
     representation
     has
     enabled
     robots
     with
     limited
     sensory
     information
     to
     navigate
     robustly
     in
     an
     array
     of
     environments,
     as
     we
     shall
     see
     in
     the
     case
     studies
     below.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-5" sec="section-5">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.5" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.5
      </small>
      <a id="x5-770003.5">
      </a>
      Representing
     Maps
     </h2>
     <p class="noindent">
      The
     problem
     of
     representing
     the
     environment
     in
     which
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      moves
     is
     a
     dual
     of
     the
     problem
     of
     representing
     the
     robot’s
     possible
     position
     or
     positions.
     Decisions
     made
     regarding
     the
     environmental
     representation
     can
     have
     impact
     on
     the
     choices
     available
     for
     robot
     position
     representation.
     </p>
     <div class="knowledge">
      <p class="noindent">
       Often
      the
      fidelity
      of
      the
      position
      representation
      is
      bounded
      by
      the
      fidelity
      of
      the
      map.
      </p>
     </div>
     <p class="noindent">
      There
     are
     three
     <alert style="color: #821131;">(3)</alert>
     fundamental
     relationships
     which
     must
     be
     understood
     when
     choosing
     a
     particular
     map
     representation:
     </p>
     <p class="noindent">
     </p>
     <dl class="enumerate-enumitem">
      <dt class="enumerate-enumitem">
       1.
      </dt>
      <dd class="enumerate-enumitem">
       The
      precision
      of
      the
      map
      must
      appropriately
      match
      the
      precision
      with
      which
      the
      robot
      needs
      to
      achieve
      its
      goals.
      </dd>
      <dt class="enumerate-enumitem">
       2.
      </dt>
      <dd class="enumerate-enumitem">
       The
      precision
      of
      the
      map
      and
      the
      type
      of
      features
      represented
      must
      match
      the
      precision
      and
      data
      types
      returned
      by
      the
      robot’s
      sensors.
      </dd>
      <dt class="enumerate-enumitem">
       3.
      </dt>
      <dd class="enumerate-enumitem">
       The
      complexity
      of
      the
      map
      representation
      has
      direct
      impact
      on
      the
      computational
      complexity
      of
      reasoning
      about
      mapping,
      localisation
      and
      navigation.
      </dd>
     </dl>
     <p class="noindent">
      Using
     the
     aforementioned
     criteria,
     we
     identify
     and
     discuss
     critical
     design
     choices
     in
     creating
     a
     map
     representation.
     Each
     such
     choice
     has
     great
     impact
     on
     the
     relationships,
     and
     on
     the
     resulting
     robot
     localisation
     architecture.
     As
     we
     will
     see,
     the
     choice
     of
     possible
     map
     representations
     is
     broad,
     if
     not
     expansive.
     Selecting
     an
     appropriate
     representation
     requires
     understanding
     all
     of
     the
     trade-offs
     inherent
     in
     that
     choice
     as
     well
     as
     understanding
     the
     specific
     context
     in
     which
     a
     particular
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      implementation
     must
     perform
     localisation.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/robot-centric-mapping.png" width="150%"/>
       <a id="x5-77004r8">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.8:
       </span>
       <span class="content">
        The presented robot-centric mapping framework enables mobile robots to create consistent elevation maps
       of the terrain. Mapping does not necessarily need to be done only in 2D as robots which will be used in
       outdoor environment would need the height of the map as well <span class="cite"><a href="MobileRoboticsLectureBookad1.html#fankhauser2018probabilistic">[67]</a></span>.
       </span>
      </figcaption>
     </div>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.5.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.5.1
      </small>
      <a id="x5-780003.5.1">
      </a>
      Continuous
     Representation
     </h3>
     <p class="noindent">
      A
     continuous-valued
     map
     is
     one
     method
     for <span id="textcolor111"><code class="verb">exact</code></span>
      decomposition
     of
     the
     environment.
     The
     position
     of
     environmental
     features
     can
     be
     mapped
     precisely
     in
     continuous
     space.
     </p>
     <div class="knowledge">
      <p class="noindent">
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       implementations
      to
      date
      use
      continuous
      maps
      only
      in
      two
      <alert style="color: #821131;">(2)</alert>
      dimensional
      representations,
      as
      increasing
      the
      number
      of
      dimensions
      can
      result
      in
      high
      computational
      load
      on
      the
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       navigation
      computer.
      </p>
     </div>
     <p class="noindent">
      A
     common
     approach
     is
     to
     combine
     the
     exactness
     of
     a
     continuous
     representation
     with
     the
     compactness
     of
     the
     closed
     world
     assumption.
     This
     means
     that
     one
     assumes
     the
     representation
     will
     specify
     all
     environmental
     objects
     in
     the
     map,
     and
     that
     any
     area
     in
     the
     map
     which
     is
     devoid
     of
     objects
     has
     no
     objects
     in
     the
     corresponding
     portion
     of
     the
     environment.
     Therefore,
     the
     total
     storage
     needed
     in
     the
     map
     is
     proportional
     to
     the
     density
     of
     objects
     in
     the
     environment,
     and
     a
     sparse
     environment
     can
     be
     represented
     by
     a
     low-memory
     map.
     </p>
     <p class="noindent">
      One
     example
     of
     such
     a
     representation,
     shown
     in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-78001r9">
       3.9
      </a>,
     is
     a
     2D
     representation
     in
     which
     polygons
     represent
     all
     obstacles
     in
     a
     continuos-valued
     coordinate
     space.
     This
     is
     similar
     to
     the
     method
     used
     by
     Latombe
     [5,
     113]
     and
     others
     to
     represent
     environments
     for
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      path
     planning
     techniques.
     In
     the
     case
     of
     [5,
     113],
     most
     of
     the
     experiments
     are
     in
     fact
     simulations
     run
     exclusively
     within
     the
     computer’s
     memory.
     Therefore,
     no
     real
     effort
     would
     have
     been
     expended
     to
     attempt
     to
     use
     sets
     of
     polygons
     to
     describe
     a
     real-world
     environment,
     such
     as
     a
     park
     or
     office
     building.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/polygon-representation.png" width="150%"/>
       <a id="x5-78001r9">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.9:
       </span>
       <span class="content">
        A continous representation using polygons as environmental obstacles.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      In
     other
     work
     in
     which
     real
     environments
     must
     be
     captured
     by
     the
     maps,
     there
     seems
     to
     be
     a
     trend
     towards
      <alert style="color: #821131;">
       selectivity
      </alert>
      and
      <alert style="color: #821131;">
       abstraction
      </alert>
      .
     The
     human
     map-maker
     tends
     to
     capture
     on
     the
     map,
     for
     localisation
     purposes,
     only
     objects
     that
     can
     be
     detected
     by
     the
     robot’s
     sensors
     and,
     furthermore,
     only
     a
     subset
     of
     the
     features
     of
     real-world
     objects.
     </p>
     <div class="knowledge">
      <p class="noindent">
       It
      should
      be
      immediately
      apparent
      that
      geometric
      maps
      can
      capably
      represent
      the
      physical
      locations
      of
      objects
      without
      referring
      to
      their
      texture,
      colour,
      elasticity,
      or
      any
      other
      such
      secondary
      features
      that
      do
      not
      related
      directly
      to
      position
      and
      space.
      </p>
     </div>
     <p class="noindent">
      In
     addition
     to
     this
     level
     of
     abstraction,
     an
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      map
     can
     further
     reduce
     memory
     usage
     by
     capturing
     only
      <alert style="color: #821131;">
       aspects
of
object
geometry
      </alert>
      which
     are
      <alert style="color: #821131;">
       immediately
relevant
      </alert>
      to
     localisation.
     For
     example
     all
     objects
     may
     be
     approximated
     using
     very
     simple
     convex
     polygons,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         14
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          14
         </sup>
        </span>
       </alert>
       <span id="textcolor112">
        A
       convex
       polygon
       is
       any
       shape
       that
       has
       all
       interior
       angles
       that
       measure
       less
       than
       180
       degrees
       </span>
      </span>
      sacrificing
     map
     felicity
     for
     the
     sake
     of
     computational
     speed.
     </p>
     <p class="noindent">
      One
     excellent
     example
     involves
      <alert style="color: #821131;">
       line
extraction
      </alert>
      .
     Many
     indoor
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      rely
     upon
     laser
     range-finding
     devices
     to
     recover
     distance
     readings
     to
     nearby
     objects.
     Such
     robots
     can
     automatically
     extract
     best-fit
     lines
     from
     the
     dense
     range
     data
     provided
     by
     thousands
     of
     points
     of
     laser
     strikes.
     Given
     such
     a
     line
     extraction
     sensor,
     an
     appropriate
     continuous
     mapping
     approach
     is
     to
     populate
     the
     map
     with
     a
     set
     of
     infinite
     lines.
     The
     continuous
     nature
     of
     the
     map
     guarantees
     that
     lines
     can
     be
     positioned
     at
     arbitrary
     positions
     in
     the
     plane
     and
     at
     arbitrary
     angles.
     The
     abstraction
     of
     real
     environmental
     objects
     such
     as
     walls
     and
     intersections
     captures
     only
     the
     information
     in
     the
     map
     representation
     that
     matches
     the
     type
     of
     information
     recovered
     by
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     rangefinding
     sensor.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/epfl.png" width="150%"/>
       <a id="x5-78002r10">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.10:
       </span>
       <span class="content">
        Example of a continuous-valued line representation of EPFL. left: real map right: representation with a
       set of infinite lines.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-78002r10">
       3.10
      </a>
      shows
     a
     map
     of
     an
     indoor
     environment
     at
     EPFL
     using
     a
     continuous
     line
     representation.
     Note
     that
     the
     only
     environmental
     features
     captured
     by
     the
     map
     are
     straight
     lines,
     such
     as
     those
     found
     at
     corners
     and
     along
     walls.
     This
     represents
     not
     only
     a
     sampling
     of
     the
     real
     world
     of
     richer
     features,
     but
     also
     a
     simplification,
     for
     an
     actual
     wall
     may
     have
     texture
     and
     relief
     that
     is
     not
     captured
     by
     the
     mapped
     line.
     The
     impact
     of
     continuos
     map
     representations
     on
     position
     representation
     is
     primarily
     positive.
     In
     the
     case
     of
     single
     hypothesis
     position
     representation,
     that
     position
     may
     be
     specified
     as
     any
     continuous-valued
     point
     in
     the
     coordinate
     space,
     and
     therefore
     extremely
     high
     accuracy
     is
     possible.
     In
     the
     case
     of
     multiple
     hypothesis
     position
     representation,
     the
     continuous
     map
     enables
     two
     types
     of
     multiple
     position
     representation.
     In
     one
     case,
     the
     possible
     robot
     position
     may
     be
     depicted
     as
     a
     geometric
     shape
     in
     the
     hyperplane,
     such
     that
     the
     robot
     is
     known
     to
     be
     within
     the
     bounds
     of
     that
     shape.
     This
     is
     shown
     in
     Figure
     5.30,
     in
     which
     the
     position
     of
     the
     robot
     is
     depicted
     by
     an
     oval
     bounding
     area.
     Yet,
     the
     continuous
     representation
     does
     not
     disallow
     representation
     of
     position
     in
     the
     form
     of
     a
     discrete
     set
     of
     possible
     positions.
     For
     instance,
     in
     [111]
     the
     robot
     position
     belief
     state
     is
     captured
     by
     sampling
     nine
     continuous-valued
     positions
     from
     within
     a
     region
     near
     the
     robot’s
     best
     known
     position.
     This
     algorithm
     captures,
     within
     a
     continuous
     space,
     a
     discrete
     sampling
     of
     possible
     robot
     positions.
     In
     summary,
     the
     key
     advantage
     of
     a
     continuous
     map
     representation
     is
     the
     potential
     for
     high
     accuracy
     and
     expressiveness
     with
     respect
     to
     the
     environmental
     configuration
     as
     well
     as
     the
     robot
     position
     within
     that
     environment.
     The
     danger
     of
     a
     continuous
     representation
     is
     that
     the
     map
     may
     be
     computationally
     costly.
     But
     this
     danger
     can
     be
     tempered
     by
     employing
     abstraction
     and
     capturing
     only
     the
     most
     relevant
     environmental
     features.
     Together
     with
     the
     use
     of
     the
     closed
     world
     assumption,
     these
     techniques
     can
     enable
     a
     continuous-valued
     map
     to
     be
     no
     more
     costly,
     and
     sometimes
     even
     less
     costly,
     than
     a
     standard
     discrete
     representation.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.5.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.5.2
      </small>
      <a id="x5-790003.5.2">
      </a>
      Decomposition
     Methods
     </h3>
     <p class="noindent">
      In
     previous
     section,
     we
     discussed
     one
     method
     of
     simplification,
     in
     which
     the
     continuous
     map
     representation
     contains
     a
     set
     of
     infinite
     lines
     which
     approximate
     real-world
     environmental
     lines
     based
     on
     a
     two-dimensional
     slice
     of
     the
     world.
     </p>
     <div class="warning">
      <p class="noindent">
       Basically
      this
      transformation
      from
      the
      real
      world
      to
      the
      map
      representation
      is
      a
      filter
      that
      removes
      all
      non-straight
      data
      and
      furthermore
      extends
      line
      segment
      data
      into
      infinite
      lines
      that
      require
      fewer
      parameters.
      </p>
     </div>
     <p class="noindent">
      A
     more
     dramatic
     form
     of
     simplification
     is
     abstraction:
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       a
      general
      decomposition
      and
      selection
      of
      environmental
      features.
      </p>
     </div>
     <p class="noindent">
      In
     this
     section,
     we
     explore
     decomposition
     as
     applied
     in
     its
     more
     extreme
     forms
     to
     the
     question
     of
     map
     representation.
     Why
     would
     one
     radically
     decompose
     the
     real
     environment
     during
     the
     design
     of
     a
     map
     representation?
     The
     immediate
     disadvantage
     of
     decomposition
     and
     abstraction
     is
     the
     loss
     of
     fidelity
     between
     the
     map
     and
     the
     real
     world.
     Both
     qualitatively,
     in
     terms
     of
     overall
     structure,
     and
     quantitatively,
     in
     terms
     of
     geometric
     precision,
     a
     highly
     abstract
     map
     does
     not
     compare
     favourably
     to
     a
     high-fidelity
     map.
     </p>
     <p class="noindent">
      Despite
     this
     disadvantage,
     decomposition
     and
     abstraction
     may
     be
     useful
     if
     the
     abstraction
     can
     be
     planned
     carefully
     so
     as
     to
     capture
     the
     relevant,
     useful
     features
     of
     the
     world
     while
     discarding
     all
     other
     features.
     The
     advantage
     of
     this
     approach
     is
     that
     the
     map
     representation
     can
     potentially
     be
     minimised.
     Furthermore,
     if
     the
     decomposition
     is
     hierarchical,
     such
     as
     in
     a
     pyramid
     of
     recursive
     abstraction,
     then
     reasoning
     and
     planning
     with
     respect
     to
     the
     map
     rep-
     resentation
     may
     be
     computationally
     far
     superior
     to
     planning
     in
     a
     fully
     detailed
     world
     model.
     </p>
     <p class="noindent">
      A
     standard,
     lossless
     form
     of
     opportunistic
     decomposition
     is
     termed
     exact
     cell
     decomposition.
     This
     method,
     introduced
     by
     [5],
     achieves
     decomposition
     by
     selecting
     boundaries
     between
     discrete
     cells
     based
     on
     geometric
     criticality.
     </p>
     <p class="noindent">
      Figure
     5.14
     depicts
     an
     exact
     decomposition
     of
     a
     planar
     workspace
     populated
     by
     polygonal
     obstacles.
     The
     map
     representation
     tessellates
     the
     space
     into
     areas
     of
     free
     space.
     The
     representation
     can
     be
     extremely
     compact
     because
     each
     such
     area
     is
     actually
     stored
     as
     a
     single
     node,
     shown
     in
     the
     graph
     at
     the
     bottom
     of
     Figure
     5.14.
     </p>
     <p class="noindent">
      The
     underlying
     assumption
     behind
     this
     decomposition
     is
     that
     the
     particular
     position
     of
     a
     robot
     within
     each
     area
     of
     free
     space
     does
     not
     matter.
     What
     matters
     is
     the
     robot’s
     ability
     to
     traverse
     from
     each
     area
     of
     free
     space
     to
     the
     adjacent
     areas.
     Therefore,
     as
     with
     other
     representations
     we
     will
     see,
     the
     resulting
     graph
     captures
     the
     adjacency
     of
     map
     locales.
     If
     indeed
     the
     assumptions
     are
     valid
     and
     the
     robot
     does
     not
     care
     about
     its
     precise
     position
     within
     a
     single
     area,
     then
     this
     can
     be
     an
     effective
     representation
     that
     nonetheless
     captures
     the
     connectivity
     of
     the
     environment.
     </p>
     <p class="noindent">
      Such
     an
     exact
     decomposition
     is
     not
     always
     appropriate.
     Exact
     decomposition
     is
     a
     function
     of
     the
     particular
     environment
     obstacles
     and
     free
     space.
     If
     this
     information
     is
     expensive
     to
     collect
     or
     even
     unknown,
     then
     such
     an
     approach
     is
     not
     feasible.
     </p>
     <p class="noindent">
      An
     alternative
     is
     fixed
     decomposition,
     in
     which
     the
     world
     is
     tessellated,
     transforming
     the
     continuos
     real
     environment
     into
     a
     discrete
     approximation
     for
     the
     map.
     Such
     a
     transforma-
     tion
     is
     demonstrated
     in
     Figure
     5.15,
     which
     depicts
     what
     happens
     to
     obstacle-filled
     and
     free
     areas
     during
     this
     transformation.
     The
     key
     disadvantage
     of
     this
     approach
     stems
     from
     its
     in-
     exact
     nature.
     It
     is
     possible
     for
     narrow
     passageways
     to
     be
     lost
     during
     such
     a
     transformation,
     as
     shown
     in
     Figure
     5.15.
     Formally
     this
     means
     that
     fixed
     decomposition
     is
     sound
     but
     not
     complete.
     Yet
     another
     approach
     is
     adaptive
     cell
     decomposition
     as
     presented
     in
     Figure
     5.16.
     </p>
     <p class="noindent">
      The
     concept
     of
     fixed
     decomposition
     is
     extremely
     popular
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ics;
     it
     is
     perhaps
     the
     single
     most
     common
     map
     representation
     technique
     currently
     utilised.
     One
     very
     popular
     </p>
     <p class="noindent">
      version
     of
     fixed
     decomposition
     is
     known
     as
     the
     occupancy
     grid
     representation
     [91].
     In
     an
     occupancy
     grid,
     the
     environment
     is
     represented
     by
     a
     discrete
     grid,
     where
     each
     cell
     is
     either
     filled
     (part
     of
     an
     obstacle)
     or
     empty
     (part
     of
     free
     space).
     This
     method
     is
     of
     particular
     value
     when
     a
     robot
     is
     equipped
     with
     range-based
     sensors
     because
     the
     range
     values
     of
     each
     sensor,
     combined
     with
     the
     absolute
     position
     of
     the
     robot,
     can
     be
     used
     directly
     to
     update
     the
     filled/
     empty
     value
     of
     each
     cell.
     In
     the
     occupancy
     grid,
     each
     cell
     may
     have
     a
     counter,
     whereby
     the
     value
     0
     indicates
     that
     the
     cell
     has
     not
     been
     "hit"
     by
     any
     ranging
     measurements
     and,
     therefore,
     it
     is
     likely
     free
     space.
     As
     the
     number
     of
     ranging
     strikes
     increases,
     the
     cell’s
     value
     is
     incremented
     and,
     above
     a
     cer-
     tain
     threshold,
     the
     cell
     is
     deemed
     to
     be
     an
     obstacle.
     By
     discounting
     the
     values
     of
     cells
     over
     time,
     both
     hysteresis
     and
     the
     possibility
     of
     transient
     obstacles
     can
     be
     represented
     using
     this
     occupancy
     grid
     approach.
     Figure
     5.17
     depicts
     an
     occupancy
     grid
     representation
     in
     which
     the
     darkness
     of
     each
     cell
     is
     proportional
     to
     the
     value
     of
     its
     counter.
     One
     commercial
     robot
     that
     uses
     a
     standard
     occupancy
     grid
     for
     mapping
     and
     navigation
     is
     the
     Cye
     robot
     [112].
     </p>
     <p class="noindent">
      There
     remain
     two
     main
     disadvantages
     of
     the
     occupancy
     grid
     approach.
     First,
     the
     size
     of
     the
     map
     in
     robot
     memory
     grows
     with
     the
     size
     of
     the
     environment
     and,
     if
     a
     small
     cell
     size
     is
     used,
     this
     size
     can
     quickly
     become
     untenable.
     This
     occupancy
     grid
     approach
     is
     not
     compatible
     with
     the
     closed
     world
     assumption,
     which
     enabled
     continuous
     representations
     to
     have
     poten-
     tially
     very
     small
     memory
     requirements
     in
     large,
     sparse
     environments.
     In
     contrast,
     the
     occu-
     pancy
     grid
     must
     have
     memory
     set
     aside
     for
     every
     cell
     in
     the
     matrix.
     Furthermore,
     any
     fixed
     decomposition
     method
     such
     as
     this
     imposes
     a
     geometric
     grid
     on
     the
     world
     a
     priori,
     regard-
     less
     of
     the
     environmental
     details.
     This
     can
     be
     inappropriate
     in
     cases
     where
     geometry
     is
     not
     the
     most
     salient
     feature
     of
     the
     environment.
     For
     these
     reasons,
     an
     alternative,
     called
     topological
     decomposition,
     has
     been
     the
     subject
     of
     some
     exploration
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ics.
     Topological
     approaches
     avoid
     direct
     measurement
     of
     geometric
     environmental
     qualities,
     instead
     concentrating
     on
     characteristics
     of
     the
     environ-
     ment
     that
     are
     most
     relevant
     to
     the
     robot
     for
     localisation.
     Formally,
     a
     topological
     representation
     is
     a
     graph
     that
     specifies
     two
     things:
     nodes
     and
     the
     connectivity
     between
     those
     nodes.
     Insofar
     as
     a
     topological
     representation
     is
     intended
     for
     the
     use
     of
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>,
     nodes
     are
     used
     to
     denote
     areas
     in
     the
     world
     and
     arcs
     are
     used
     to
     denote
     adjacency
     of
     pairs
     of
     nodes.
     When
     an
     arc
     connects
     two
     nodes,
     then
     the
     robot
     can
     traverse
     from
     one
     node
     to
     the
     other
     without
     requiring
     traversal
     of
     any
     other
     intermediary
     node.
     Adjacency
     is
     clearly
     at
     the
     heart
     of
     the
     topological
     approach,
     just
     as
     adjacency
     in
     a
     cell
     de-
     composition
     representation
     maps
     to
     geometric
     adjacency
     in
     the
     real
     world.
     However,
     the
     topological
     approach
     diverges
     in
     that
     the
     nodes
     are
     not
     of
     fixed
     size
     nor
     even
     specifications
     of
     free
     space.
     Instead,
     nodes
     document
     an
     area
     based
     on
     any
     sensor
     discriminant
     such
     that
     the
     robot
     can
     recognise
     entry
     and
     exit
     of
     the
     node.
     Figure
     5.18
     depicts
     a
     topological
     representation
     of
     a
     set
     of
     hallways
     and
     offices
     in
     an
     indoor
     </p>
     <p class="noindent">
      environment.
     In
     this
     case,
     the
     robot
     is
     assumed
     to
     have
     an
     intersection
     detector,
     perhaps
     us-
     ing
     sonar
     and
     vision
     to
     find
     intersections
     between
     halls
     and
     between
     halls
     and
     rooms.
     Note
     that
     nodes
     capture
     geometric
     space
     and
     arcs
     in
     this
     representation
     simply
     represent
     connec-
     tivity.
     Another
     example
     of
     topological
     representation
     is
     the
     work
     of
     Dudek
     [49],
     in
     which
     the
     goal
     is
     to
     create
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      that
     can
     capture
     the
     most
     interesting
     aspects
     of
     an
     area
     for
     human
     consumption.
     The
     nodes
     in
     Dudek’s
     representation
     are
     visually
     striking
     locales
     rather
     than
     route
     intersections.
     In
     order
     to
     navigate
     using
     a
     topological
     map
     robustly,
     a
     robot
     must
     satisfy
     two
     constraints.
     First,
     it
     must
     have
     a
     means
     for
     detecting
     its
     current
     position
     in
     terms
     of
     the
     nodes
     of
     the
     to-
     pological
     graph.
     Second,
     it
     must
     have
     a
     means
     for
     traveling
     between
     nodes
     using
     robot
     mo-
     tion.
     The
     node
     sizes
     and
     particular
     dimensions
     must
     be
     optimised
     to
     match
     the
     sensory
     discrimination
     of
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      hardware.
     This
     ability
     to
     "tune"
     the
     representation
     to
     the
     robot’s
     particular
     sensors
     can
     be
     an
     important
     advantage
     of
     the
     topological
     approach.
     How-
     ever,
     as
     the
     map
     representation
     drifts
     further
     away
     from
     true
     geometry,
     the
     expressiveness
     of
     the
     representation
     for
     accurately
     and
     precisely
     describing
     a
     robot
     position
     is
     lost.
     Therein
     lies
     the
     compromise
     between
     the
     discrete
     cell-based
     map
     representations
     and
     the
     topological
     representations.
     Interestingly,
     the
     continuous
     map
     representation
     has
     the
     potential
     to
     be
     both
     compact
     like
     a
     topological
     representation
     and
     precise
     as
     with
     all
     direct
     geometric
     rep-
     resentations.
     Yet,
     a
     chief
     motivation
     of
     the
     topological
     approach
     is
     that
     the
     environment
     may
     contain
     im-
     portant
     non-geometric
     features
     -
     features
     that
     have
     no
     ranging
     relevance
     but
     are
     useful
     for
     localisation.
     In
     Chapter
     4
     we
     described
     such
     whole-image
     vision-based
     features.
     In
     contrast
     to
     these
     whole-image
     feature
     extractors,
     often
     spatially
     localised
     landmarks
     are
     artificially
     placed
     in
     an
     environment
     to
     impose
     a
     particular
     visual-topological
     connectivity
     upon
     the
     environment.
     In
     effect,
     the
     artificial
     landmark
     can
     impose
     artificial
     structure.
     Ex-
     amples
     of
     working
     systems
     operating
     with
     this
     landmark-based
     strategy
     have
     also
     demon-
     strated
     success.
     Latombe’s
     landmark-based
     navigation
     research
     89
     has
     been
     implemented
     on
     real-world
     indoor
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     that
     employ
     paper
     landmarks
     attached
     to
     the
     ceiling
     as
     the
     locally
     observable
     features.
     Chips
     the
     museum
     robot
     is
     another
     robot
     that
     uses
     man-
     made
     landmarks
     to
     obviate
     the
     localisation
     problem.
     In
     this
     case,
     a
     bright
     pink
     square
     serves
     as
     a
     landmark
     with
     dimensions
     and
     color
     signature
     that
     would
     be
     hard
     to
     accidentally
     repro-
     duce
     in
     a
     museum
     environment
     [88].
     One
     such
     museum
     landmark
     is
     shown
     in
     Figure
     (5.19).
     In
     summary,
     range
     is
     clearly
     not
     the
     only
     measurable
     and
     useful
     environmental
     value
     for
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>.
     This
     is
     particularly
     true
     due
     to
     the
     advent
     of
     color
     vision
     as
     well
     as
     laser
     rangefinding,
     which
     provides
     reflectance
     information
     in
     addition
     to
     range
     information.
     Choosing
     a
     map
     representation
     for
     a
     particular
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      requires
     first
     understanding
     the
     sensors
     available
     on
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      and
     second
     understanding
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     function-
     al
     requirements
     (e.g.
     required
     goal
     precision
     and
     accuracy).
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.5.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.5.3
      </small>
      <a id="x5-800003.5.3">
      </a>
      Current
     Challenges
     </h3>
     <p class="noindent">
      Previous
     section
     describe
     major
     design
     decisions
     with
     regards
     to
     map
     representation
     choices.
     There
     are,
     however,
     fundamental
     real-world
     features
     which
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      map
     representations
     do
     not
     work
     as
     well.
     These
     continue
     to
     be
     the
     subject
     of
     open
     research,
     and
     several
     such
     challenges
     are
     described
     below.
     </p>
     <p class="noindent">
      The
     real
     world
     is
      <alert style="color: #821131;">
       dynamic
      </alert>
      .
     As
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s
     come
     to
     work
     and
     move
     in
     the
     same
     spaces
     as
     humans,
     they
     will
     encounter:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        moving
       people,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        cars,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        strollers,
       and
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        transient
       obstacles.
       </p>
      </li>
     </ul>
     <p class="noindent">
      This
     is
     particularly
     true
     when
     one
     considers
     a
     home
     setting
     with
     which
     domestic
     robots
     will
     someday
     need
     to
     contend.
     </p>
     <p class="noindent">
      The
     map
     representations
     described
     previously
     do
     not,
     in
     general,
     have
      <alert style="color: #821131;">
       explicit
methods
      </alert>
      for
     identifying
     and
     distinguishing
     between
     permanent
     obstacles
     (e.g.
     walls,
     doorways,
     etc.)
     and
     transient
     obstacles
     (e.g.,
     humans,
     shipping
     packages,
     etc.).
     The
     current
     state
     of
     the
     art
     in
     terms
     of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      sensors
     is
     partly
     to
     blame
     for
     this
     shortcoming.
     Although
     vision
     research
     is
     rapidly
     advancing,
     robust
     sensors
     that
     discriminate
     between
     moving
     animals
     and
     static
     structures
     from
     a
     moving
     reference
     frame
     are
     not
     yet
     available.
     Furthermore,
     estimating
     the
     motion
     vector
     of
     transient
     objects
     remains
     a
     research
     problem.
     </p>
     <p class="noindent">
      Usually,
     the
     assumption
     behind
     the
     above
     map
     representations
     is
     that
     all
     objects
     on
     the
     map
     are
     effectively
      <alert style="color: #821131;">
       static
      </alert>
      .
     Partial
     success
     can
     be
     achieved
     by
     discounting
     mapped
     objects
     over
     time.
     For
     example,
     occupancy
     grid
     techniques
     can
     be
     more
     robust
     to
     dynamic
     settings
     by
     introducing
     temporal
     discounting,
     effectively
     treating
     transient
     obstacles
     as
     noise.
     The
     more
     challenging
     process
     of
     map
     creation
     is
     particularly
     fragile
     to
     environment
     dynamics;
     most
     mapping
     techniques
     generally
     require
     that
     the
     environment
     be
     free
     of
     moving
     objects
     during
     the
     mapping
     process.
     One
     exception
     to
     this
     limitation
     involves
     topological
     representations.
     Because
     precise
     geometry
     is
     not
     important,
     transient
     objects
     have
     little
     effect
     on
     the
     map-
     ping
     or
     localisation
     process,
     subject
     to
     the
     critical
     constraint
     that
     the
     transient
     objects
     must
     not
     change
     the
     topological
     connectivity
     of
     the
     environment.
     Still,
     neither
     the
     occupancy
     grid
     representation
     nor
     a
     topological
     approach
     is
     actively
     recognizing
     and
     representing
     transient
     objects
     as
     distinct
     from
     both
     sensor
     error
     and
     permanent
     map
     features.
     </p>
     <p class="noindent">
      As
     vision
     sensing
     provides
     more
     robust
     and
     more
     informative
     content
     regarding
     the
     transience
     and
     motion
     details
     of
     objects
     in
     the
     world,
     researchers
     will
     in
     time
     propose
     representations
     that
     make
     use
     of
     that
     information.
     A
     classic
     example
     involves
     occlusion
     by
     human
     crowds.
     Museum
     tour
     guide
     robots
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         15
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/museum-tour-robot.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          15
         </sup>
        </span>
       </alert>
       <span id="textcolor117">
        An
       Example
       of
       a
       museum
       tour
       guide
       robot
       used
       in
       the
       National
       Museum
       of
       Korea <a id="x5-80001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#yonhap2018">[68]</a></span>.
       </span>
      </span>
      generally
     suffer
     from
     an
     extreme
     amount
     of
     occlusion.
     If
     the
     robot’s
     sensing
     suite
     is
     located
     along
     the
     robot’s
     body,
     then
     the
     robot
     is
     effectively
     blind
     when
     a
     group
     of
     human
     visitors
     completely
     surrounds
     the
     robot.
     This
     is
     because
     its
     map
     contains
     only
     environment
     features
     that
     are,
     at
     that
     point,
     fully
     hidden
     from
     the
     robot’s
     sensors
     by
     the
     wall
     of
     people.
     In
     the
     best
     case,
     the
     robot
     should
     recognise
     its
     occlusion
     and
     make
     no
     effort
     to
     localise
     using
     these
     invalid
     sensor
     readings.
     In
     the
     worst
     case,
     the
     robot
     will
     localise
     with
     the
     fully
     occluded
     data,
     and
     will
     update
     its
     location
     incorrectly.
     A
     vision
     sensor
     that
     can
     discriminate
     the
     local
     conditions
     of
     the
     robot
     (e.g.
     we
     are
     surrounded
     by
     people)
     can
     help
     eliminate
     this
     error
     mode.
     </p>
     <p class="noindent">
      A
     second
     open
     challenge
     in
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      localisation
     involves
     the
     traversal
     of
     open
     spaces.
     Existing
     localisation
     techniques
     generally
     depend
     on
     local
     measures
     such
     as
     range,
     thereby
     demanding
     environments
     that
     are
     somewhat
     densely
     filled
     with
     objects
     that
     the
     sensors
     can
     detect
     and
     measure.
     Wide
     open
     spaces
     such
     as
     parking
     lots,
     fields
     of
     grass
     and
     indoor
     open-spaces
     such
     as
     those
     found
     in
     convention
     centres
     or
     expos
     pose
     a
     difficulty
     for
     such
     systems
     due
     to
     their
     relative
     sparseness.
     Indeed,
     when
     populated
     with
     humans,
     the
     challenge
     is
     exacerbated
     because
     any
     mapped
     objects
     are
     almost
     certain
     to
     be
     occluded
     from
     view
     by
     the
     people.
     </p>
     <p class="noindent">
      Once
     again,
     more
     recent
     technologies
     provide
     some
     hope
     for
     overcoming
     these
     limitations.
     Both
     vision
     and
     state-of-the-art
     laser
     range-finding
     devices
     offer
     outdoor
     performance
     with
     ranges
     of
     up
     to
     a
     hundred
     meters
     and
     more.
     Of
     course,
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gps">
       GPS
      </a>
      performs
     even
     better.
     Such
     long-range
     sensing
     may
     be
     required
     for
     robots
     to
     localise
     using
     distant
     features.
     </p>
     <p class="noindent">
      This
     trend
     teases
     out
     a
     hidden
     assumption
     underlying
     most
     topological
     map
     representations.
     Usually,
     topological
     representations
     make
     assumptions
     regarding
     spatial
     locality:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       a
      node
      contains
      objects
      and
      features
      that
      are
      themselves
      within
      that
      node.
      </p>
     </div>
     <p class="noindent">
      The
     process
     of
     map
     creation
     therefore
     involves
     making
     nodes
     which
     are,
     in
     their
     own
     self-contained
     way,
     recognizable
     by
     virtue
     of
     the
     objects
     contained
     within
     the
     node.
     Therefore,
     in
     an
     indoor
     environment,
     each
     room
     can
     be
     a
     separate
     node.
     This
     is
     a
     reasonable
     assumption
     as
     each
     room
     will
     have
     a
     layout
     and
     a
     set
     of
     belongings
     that
     are
      <alert style="color: #821131;">
       unique
      </alert>
      to
     that
     room.
     </p>
     <p class="noindent">
      However,
     consider
     the
     outdoor
     world
     of
     a
     wide-open
     park.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       Where
      should
      a
      single
      node
      end
      and
      the
      next
      node
      begin?
      </p>
     </div>
     <p class="noindent">
      The
     answer
     is
     unclear
     as
     objects
     which
     are
     far
     away
     from
     the
     current
     node,
     or
     position,
     can
     give
     information
     for
     the
     localisation
     process.
     For
     example,
     the
     hump
     of
     a
     hill
     at
     the
     horizon,
     the
     position
     of
     a
     river
     in
     the
     valley
     and
     the
     trajectory
     of
     the
     Sun
     all
     are
     non-local
     features
     that
     have
     great
     bearing
     on
     one’s
     ability
     to
     infer
     current
     position.
     </p>
     <div class="warning">
      <p class="noindent">
       The
      spatial
      locality
      assumption
      is
      violated
      and,
      instead,
      replaced
      by
      a
      visibility
      criterion:
      </p>
      <div class="quoteblock">
       <p class="noindent">
        the
       node
       or
       cell
       may
       need
       a
       mechanism
       for
       representing
       objects
       that
       are
       measurable
       and
       visible
       from
       that
       cell.
       </p>
      </div>
     </div>
     <p class="noindent">
      Once
     again,
     as
     sensors
     and
     outdoor
     locomotion
     mechanisms
     improve,
     there
     will
     be
     greater
     urgency
     to
     solve
     problems
     associated
     with
     localisation
     in
     wide-open
     settings,
     with
     and
     without
     GPS-type
     global
     localisation
     sensors.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         16
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          16
         </sup>
        </span>
       </alert>
       <span id="textcolor118">
        Of
       course
       with
       the
       use
       of
       a
        <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:gnss">
         GNSS
        </a>,
       the
       localisation
       problem
       may
       completely
       be
       solved,
       however
       in
       cost
       saving
       measures
       one
       would
       wish
       to
       avoid
       the
       use
       of
       them
       as
       they
       can
       be
       expensive.
       </span>
      </span>
     </p>
     <p class="noindent">
      We
     end
     this
     section
     with
     one
     final
     open
     challenge
     that
     represents
     one
     of
     the
     fundamental
     ac-
     ademic
     research
     questions
     of
     robotics:
      <span id="bold" style="font-weight:bold;">
       sensor
fusion
      </span>
      .
     </p>
     <div class="informationblock" id="tcolobox-4">
      <div class="title">
       <p class="noindent">
        <span id="bold" style="font-weight:bold;">
         Information
        </span>
        :
       Sensor
       Fusion
       </p>
      </div>
      <div class="box-content">
       <p class="noindent">
        A variety of measurement types are possible using off-the-shelf robot sensors, including heat, range, acoustic
       and light-based reflectivity, color, texture, friction, etc. Sensor fusion is a research topic closely related to map
       representation. Just as a map must embody an environment in sufficient detail for a robot to perform localisation
       and reasoning, sensor fusion demands a representation of the world that is sufficiently general and expressive
       that a variety of sensor types can have their data correlated appropriately, strengthening the resulting percepts
       well beyond that of any individual sensor’s readings.
       </p>
      </div>
     </div>
     <p class="noindent">
      An
     implementation
     example
     implementation
     of
     sensor
     fusion
     to
     date
     is
     that
     of
     neural
     network
     classifier.
     Using
     this
     technique,
     any
     number
     and
     any
     type
     of
     sensor
     values
     may
     be
     jointly
     combined
     in
     a
     network
     that
     will
     use
     whatever
     means
     necessary
     to
     optimise
     its
     classification
     accuracy.
     For
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      that
     must
     use
     a
     human-readable
     internal
     map
     representation,
     no
     equally
     general
     sensor
     fusion
     scheme
     has
     yet
     been
     born.
     It
     is
     reasonable
     to
     expect
     that,
     when
     the
     sensor
     fusion
     problem
     is
     solved,
     integration
     of
     a
     large
     number
     of
     disparate
     sensor
     types
     may
     easily
     result
     in
     sufficient
     discriminatory
     power
     for
     robots
     to
     achieve
     real-world
     navigation,
     even
     in
     wide-open
     and
     dynamic
     circumstances
     such
     as
     a
     public
     square
     filled
     with
     people.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-6" sec="section-6">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.6" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.6
      </small>
      <a id="x5-810003.6">
      </a>
      Probabilistic
     Map-Based
     Localisation
     </h2>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.6.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.6.1
      </small>
      <a id="x5-820003.6.1">
      </a>
      Introduction
     </h3>
     <p class="noindent">
      As
     stated
     previously,
     multiple
     hypothesis
     position
     representation
     is
     advantageous
     because
     the
     robot
     can
     explicitly
     track
     its
     own
     beliefs
     regarding
     its
     possible
     positions
     in
     the
     environment.
     Ideally,
     the
     robot’s
     belief
     state
     will
     change,
     over
     time,
     as
     is
     consistent
     with
     its
     motor
     outputs
     and
     perceptual
     inputs.
     One
     geometric
     approach
     to
     multiple
     hypothesis
     representation,
     mentioned
     earlier,
     involves
     identifying
     the
     possible
     positions
     of
     the
     robot
     by
     specifying
     a
     polygon
     in
     the
     environmental
     representation
     [113].
     This
     method
     does
     not
     provide
     any
     indication
     of
     the
     relative
     chances
     between
     various
     possible
     robot
     positions.
     Probabilistic
     techniques
     differ
     from
     this
     because
     they
     explicitly
     identify
     probabilities
     with
     the
     possible
     robot
     positions,
     and
     for
     this
     reason
     these
     methods
     have
     been
     the
     focus
     of
     recent
     research.
     In
     the
     following
     sections
     we
     present
     two
     classes
     of
     probabilistic
     localisation.
     The
     first
     class,
     Markov
     localisation,
     uses
     an
     explicitly
     specified
     probability
     distribution
     across
     all
     possible
     robots
     positions.
     The
     second
     method,
     Kalman
     filter
     localisation,
     uses
     a
     Gaussian
     probability
     density
     representation
     of
     robot
     position
     and
     scan
     matching
     for
     localisation.
     Unlike
     Markov
     localisation,
     Kalman
     filter
     localisation
     does
     not
     independently
     consider
     each
     possible
     pose
     in
     the
     robot’s
     configuration
     space.
     Interestingly,
     the
     Kalman
     filter
     localization
     process
     results
     from
     the
     Markov
     localisation
     axioms
     if
     the
     robot’s
     position
     uncertainty
     is
     assumed
     to
     have
     a
     Gaussian
     form
     [28
     page
     43-44].
     Before
     discussing
     each
     method
     in
     detail,
     we
     present
     the
     general
     robot
     localisation
     problem
     and
     solution
     strategy.
     Consider
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      moving
     in
     a
     known
     environment.
     As
     it
     starts
     to
     move,
     say
     from
     a
     precisely
     known
     location,
     it
     can
     keep
     track
     of
     its
     motion
     using
     odometry.
     Due
     to
     odometry
     uncertainty,
     after
     some
     movement
     the
     robot
     will
     become
     very
     uncertain
     about
     its
     position
     (see
     section
     5.2.4).
     To
     keep
     position
     uncertainty
     from
     growing
     unbounded,
     the
     robot
     must
     localise
     itself
     in
     relation
     to
     its
     environment
     map.
     To
     localise,
     the
     robot
     might
     use
     its
     on-board
     sensors
     (ultrasonic,
     range
     sensor,
     vision)
     to
     make
     observations
     of
     its
     environment.
     The
     information
     provided
     by
     the
     robot’s
     odometry,
     plus
     the
     information
     provided
     by
     such
     exteroceptive
     observations
     can
     be
     combined
     to
     enable
     the
     robot
     to
     localise
     as
     well
     as
     possible
     with
     respect
     to
     its
     map.
     The
     processes
     of
     updating
     based
     on
     proprioceptive
     sensor
     values
     and
     exteroceptive
     sensor
     values
     are
     often
     separated
     logically,
     leading
     to
     a
     general
     two-step
     process
     for
     robot
     position
     update.
     Action
     update
     represents
     the
     application
     of
     some
     action
     model
     Act
     to
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     proprioceptive
     encoder
     measurements
     o
     and
     prior
     belief
     state
     s
     to
     yield
     a
     new
     belief
     t
     t-1
     state
     representing
     the
     robot’s
     belief
     about
     its
     current
     position.
     Note
     that
     throughout
     this
     chapter
     we
     will
     assume
     that
     the
     robot’s
     proprioceptive
     encoder
     measurements
     are
     used
     as
     the
     best
     possible
     measure
     of
     its
     actions
     over
     time.
     If,
     for
     instance,
     a
     differential
     drive
     robot
     had
     motors
     without
     encoders
     connected
     to
     its
     wheels
     and
     employed
     open-loop
     control,
     then
     instead
     of
     encoder
     measurements
     the
     robot’s
     highly
     uncertain
     estimates
     of
     wheel
     spin
     would
     need
     to
     be
     incorporated.
     We
     ignore
     such
     cases
     and
     therefore
     have
     a
     simple
     formula:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-82001r1">
         </mstyle>
         <msubsup>
          <mrow>
           <mi>
            s
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
           <mi>
            ′
           </mi>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi class="qopname">
          Act
         </mi>
         <mo>
          ⁡
         </mo>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                o
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <msubsup>
              <mrow>
               <mi>
                s
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.1)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      Perception
     update
     represents
     the
     application
     of
     some
     perception
     model
     See
     to
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     exteroceptive
     sensor
     inputs
     i
     and
     updated
     belief
     state
     s’
     to
     yield
     a
     refined
     belief
     tt
     state
     representing
     the
     robot’s
     current
     position:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-82002r2">
         </mstyle>
         <msubsup>
          <mrow>
           <mi>
            s
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi class="qopname">
          See
         </mi>
         <mo>
          ⁡
         </mo>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                i
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <msubsup>
              <mrow>
               <mi>
                s
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
               <mi>
                ′
               </mi>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.2)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <div class="warning">
      <p class="noindent">
       The
      perception
      model
      See
      and
      sometimes
      the
      action
      model
      Act
      are
      abstract
      functions
      of
      both
      the
      map
      and
      the
      robot’s
      physical
      configuration.
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          17
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           17
          </sup>
         </span>
        </alert>
        <span id="textcolor119">
         such
        as
        sensors
        and
        their
        positions,
        kinematics,
        etc.
        </span>
       </span>
      </p>
     </div>
     <p class="noindent">
      In
     general,
     the
     action
     update
     process
      <alert style="color: #821131;">
       contributes
uncertainty
      </alert>
      to
     the
     robot’s
     belief
     about
     position:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       encoders
      have
      error
      and
      therefore
      motion
      is
      somewhat
      nondeterministic.
      </p>
     </div>
     <p class="noindent">
      In
     contrast,
     perception
     update
     generally
      <alert style="color: #821131;">
       refines
      </alert>
      the
     belief
     state.
     Sensor
     measurements,
     when
     compared
     to
     the
     robot’s
     environmental
     model,
     tend
     to
     provide
     clues
     regarding
     the
     robot’s
     possible
     position.
     </p>
     <p class="noindent">
      In
     the
     case
     of
     Markov
     localisation,
     the
     robot’s
     belief
     state
     is
     usually
     represented
     as
     separate
     probability
     assignments
     for
     every
     possible
     robot
     pose
     in
     its
     map.
     The
     action
     update
     and
     perception
     update
     processes
     must
     update
     the
     probability
     of
     every
     cell
     in
     this
     case.
     Kalman
     filter
     localisation
     represents
     the
     robot’s
     belief
     state
     using
     a
     singe,
     well-defined
     Gaussian
     probability
     density
     function,
     and
     therefore
     retains
     just
     a
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        μ
       </mi>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        σ
       </mi>
      </math>
      parameterisation
     of
     the
     robot’s
     belief
     about
     position
     with
     respect
     to
     the
     map.
     Updating
     the
     parameters
     of
     the
     Gaussian
     distribution
     is
     all
     that
     is
     required.
     This
     fundamental
     difference
     in
     the
     representation
     of
     belief
     state
     leads
     to
     the
     following
     advantages
     and
     disadvantages
     of
     the
     two
     <alert style="color: #821131;">(2)</alert>
     methods,
     as
     presented
     in <a id="x5-82003"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#brock1999high">[69]</a></span>:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        Markov
       localization
       allows
       for
       localization
       starting
       from
       any
       unknown
       position
       and
       can
       thus
       recover
       from
       ambiguous
       situations
       because
       the
       robot
       can
       track
       multiple,
       completely
       disparate
       possible
       positions.
       However,
       to
       update
       the
       probability
       of
       all
       positions
       within
       the
       whole
       state
       space
       at
       any
       time
       requires
       a
       discrete
       representation
       of
       the
       space
       (grid).
       The
       required
       memory
       and
       computational
       power
       can
       thus
       limit
       precision
       and
       map
       size.
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        Kalman
       filter
       localization
       tracks
       the
       robot
       from
       an
       initially
       known
       position
       and
       is
       inherently
       both
       precise
       and
       efficient.
       In
       particular,
       Kalman
       filter
       localization
       can
       be
       used
       in
       continuous
       world
       representations.
       However,
       if
       the
       uncertainty
       of
       the
       robot
       becomes
       too
       large
       (e.g.
       due
       to
       a
       robot
       collision
       with
       an
       object)
       and
       thus
       not
       truly
       un-
       imodal,
       the
       Kalman
       filter
       can
       fail
       to
       capture
       the
       multitude
       of
       possible
       robot
       positions
       and
       can
       become
       irrevocably
       lost.
       </p>
      </li>
     </ul>
     <p class="noindent">
      Improvements
     are
     achieved
     or
     proposed
     by
     either
     only
     updating
     the
     state
     space
     of
     interest
     within
     the
     Markov
     approach <a id="x5-82004"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#borensteinfast">[70]</a></span>
     or
     by
     combining
     both
     methods
     to
     create
     a
     hybrid
     localization
     system <a id="x5-82005"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#brock1999high">[69]</a></span>.
     </p>
     <p class="noindent">
      We
     will
     now
     look
     at
     them
     in
     great
     detail.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.6.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.6.2
      </small>
      <a id="x5-830003.6.2">
      </a>
      Markov
     Localisation
     </h3>
     <p class="noindent">
      Markov
     localization
     tracks
     the
     robot’s
     belief
     state
     using
     an
     arbitrary
     probability
     density
     function
     to
     represent
     the
     robot’s
     position.
     In
     practice,
     all
     known
     Markov
     localization
     systems
     implement
     this
     generic
     belief
     representation
     by
     first
     tessellating
     the
     robot
     configuration
     space
     into
     a
     finite,
     discrete
     number
     of
     possible
     robot
     poses
     in
     the
     map.
     In
     actual
     applications,
     the
     number
     of
     possible
     poses
     can
     range
     from
     several
     hundred
     positions
     to
     millions
     of
     positions.
     </p>
     <p class="noindent">
      Given
     such
     a
     generic
     conception
     of
     robot
     position,
     a
     powerful
     update
     mechanism
     is
     required
     that
     can
     compute
     the
     belief
     state
     that
     results
     when
     new
     information
     (e.g.
     encoder
     values
     and
     sensor
     values)
     is
     incorporated
     into
     a
     prior
     belief
     state
     with
     arbitrary
     probability
     density.
     The
     solution
     is
     born
     out
     of
     probability
     theory,
     and
     so
     the
     next
     section
     describes
     the
     foundations
     of
     probability
     theory
     that
     apply
     to
     this
     problem,
     notably
     Bayes
     formula.
     Then,
     two
     subse-
     quent
     subsections
     provide
     case
     studies,
     one
     robot
     implementing
     a
     simple
     feature-driven
     to-
     pological
     representation
     of
     the
     environment <a id="x5-83001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#brooks1986robust">[71]</a></span>,
     and
     the
     other
     using
     a
     geometric
     grid-based
     map <a id="x5-83002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#borensteinfast">[70]</a></span>.
      <a id="subsubsection*.33">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x5-84000">
      </a>
      Application
     of
     Probability
     for
     Localisation
     </h5>
     <p class="noindent">
      Given
     a
     discrete
     representation
     of
     robot
     positions,
     to
     express
     a
     belief
     state
     we
     wish
     to
     assign
     to
     each
     possible
     robot
     position
     a
     probability
     that
     the
     robot
     is
     indeed
     at
     that
     position.
     </p>
     <p class="noindent">
      From
     probability
     theory
     we
     use
     the
     term
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            A
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      to
     denote
     the
     probability
     that
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        A
       </mi>
      </math>
      is
      <alert style="color: #821131;">
       true
      </alert>
      .
     This
     is
     also
     called
     the
     prior
     probability
     of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        A
       </mi>
      </math>
      because
     it
     measures
     the
     probability
     that
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        A
       </mi>
      </math>
      is
     true
     independent
     of
     any
     additional
     knowledge
     we
     may
     have.
     </p>
     <div class="knowledge">
      <p class="noindent">
       For
      example
      we
      can
      use
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         P
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               r
              </mi>
             </mrow>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-rel" stretchy="false">
             =
            </mo>
            <mi>
             l
            </mi>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       to
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         t
        </mi>
       </math>
       denote
      the
      prior
      probability
      that
      the
      robot
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         r
        </mi>
       </math>
       is
      at
      position
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
       </math>
       at
      time
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         t
        </mi>
       </math>
       .
      </p>
     </div>
     <p class="noindent">
      In
     practice,
     we
     wish
     to
     compute
     the
     probability
     of
     each
     individual
     robot
     position
     given
     the
     encoder
     and
     sensor
     evidence
     the
     robot
     has
     collected.
     For
     this,
     we
     use
     the
     term
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            A
           </mi>
           <mo class="MathClass-rel" stretchy="false">
            |
           </mo>
           <mi>
            B
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      to
     denote
     the
      <alert style="color: #821131;">
       conditional
probability
      </alert>
      of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        A
       </mi>
      </math>
      given
     that
     we
     know
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        B
       </mi>
      </math>
      .
     </p>
     <div class="knowledge">
      <p class="noindent">
       For
      example,
      we
      use
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         P
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               r
              </mi>
             </mrow>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-rel" stretchy="false">
             =
            </mo>
            <mi>
             l
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             |
            </mo>
            <msubsup>
             <mrow>
              <mi>
               i
              </mi>
             </mrow>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       to
      denote
      the
      probability
      that
      the
      robot
      is
      at
      position
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         l
        </mi>
       </math>
       given
      that
      the
      robot’s
      sensor
      inputs
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         i
        </mi>
       </math>
       .
      </p>
     </div>
     <p class="noindent">
      The
     question
     is,
     </p>
     <div class="quoteblock">
      <p class="noindent">
       how
      can
      a
      term
      such
      as
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <mi>
         P
        </mi>
        <msup>
         <mrow>
          <mrow>
           <mo fence="true" form="prefix">
            (
           </mo>
           <mrow>
            <msubsup>
             <mrow>
              <mi>
               r
              </mi>
             </mrow>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
            <mo class="MathClass-rel" stretchy="false">
             =
            </mo>
            <mi>
             l
            </mi>
            <mo class="MathClass-rel" stretchy="false">
             |
            </mo>
            <msubsup>
             <mrow>
              <mi>
               i
              </mi>
             </mrow>
             <mrow>
              <mi>
               t
              </mi>
             </mrow>
             <mrow>
             </mrow>
            </msubsup>
           </mrow>
           <mo fence="true" form="postfix">
            )
           </mo>
          </mrow>
         </mrow>
         <mrow>
         </mrow>
        </msup>
       </math>
       be
      simplified
      to
      its
      constituent
      parts
      so
      that
      it
      can
      be
      computed?
      </p>
     </div>
     <p class="noindent">
      The
     answer
     lies
     in
     the
     product
     rule,
     which
     states:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-84001r3">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              A
             </mi>
             <mo class="MathClass-bin" stretchy="false">
              ∧
             </mo>
             <mi>
              B
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              A
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <mi>
              B
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              B
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.3)
       </td>
      </tr>
     </table>
     <p class="noindent">
      The equation given in Eq. <a href="#x5-84001r3">(3.3)</a>
     is relatively straightforward, as the probability of
      <alert style="color: #821131;">
       both
      </alert>
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        A
       </mi>
      </math>
      and
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         18
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          18
         </sup>
        </span>
       </alert>
       <span id="textcolor122">
        To simplify notation we
       will be using the wedge (
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mo class="MathClass-bin" stretchy="false">
          ∧
         </mo>
        </math>
        ) symbol
       to denote AND, and the vee (
        <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
         <mo class="MathClass-bin" stretchy="false">
          ∨
         </mo>
        </math>
        )
       symbol to denote OR.
       </span>
      </span>
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        B
       </mi>
      </math>
      being
     true is being related to
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        B
       </mi>
      </math>
      being true and the other being conditionally true. But you should be able to convince yourself that the alternate equation is equally correct:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-84002r4">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              A
             </mi>
             <mo class="MathClass-bin" stretchy="false">
              ∧
             </mo>
             <mi>
              B
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              B
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <mi>
              A
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              A
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.4)
       </td>
      </tr>
     </table>
     <p class="noindent">
      Using both Eq. <a href="#x5-84001r3">(3.3)</a>
     and Eq. <a href="#x5-84002r4">(3.4)</a>
     together, we can derive
      <alert style="color: #821131;">
       Bayes formula
      </alert>
      for computing
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            A
           </mi>
           <mo class="MathClass-rel" stretchy="false">
            |
           </mo>
           <mi>
            B
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      :
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-84003r5">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              A
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <mi>
              B
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                B
               </mi>
               <mo class="MathClass-rel" stretchy="false">
                |
               </mo>
               <mi>
                A
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                A
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </mrow>
          <mrow>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                B
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </mrow>
         </mfrac>
        </math>
       </td>
       <td class="eq-no">
        (3.5)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      We use Bayes rule to compute the robot’s new belief state as a function of its sensory inputs and its former
     belief state. But to do this properly, we must recall the basic goal of the Markov localisation approach:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       a
      discrete
      set
      of
      possible
      robot
      positions
      L
      are
      represented.
      </p>
     </div>
     <p class="noindent">
      The belief state of the robot must assign a probability
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <msubsup>
            <mrow>
             <mi>
              r
             </mi>
            </mrow>
            <mrow>
             <mi>
              t
             </mi>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            l
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      for
     each location
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      in
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        L
       </mi>
      </math>
      .
     </p>
     <p class="noindent">
      The
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi class="qopname">
        See
       </mi>
       <mo>
        ⁡
       </mo>
      </math>
      function described in Eq. <a href="#x5-82002r2">(3.2)</a>
     expresses a mapping from a belief state and sensor input to
     a refined belief state. To do this, we must update the probability associated with each position
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      in
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        L
       </mi>
      </math>
      , and we can do this by directly
     applying Bayes formula to every such
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      .
     </p>
     <p class="noindent">
      In denoting this, we will stop representing the temporal index
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        t
       </mi>
      </math>
      for simplicity and
     will further use
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            l
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      to mean
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            r
           </mi>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            l
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      :
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-84004r6">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              l
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <mi>
              i
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mfrac>
          <mrow>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                i
               </mi>
               <mo class="MathClass-rel" stretchy="false">
                |
               </mo>
               <mi>
                l
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </mrow>
          <mrow>
           <mi>
            P
           </mi>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                i
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
          </mrow>
         </mfrac>
        </math>
       </td>
       <td class="eq-no">
        (3.6)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      The value of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            l
           </mi>
           <mo class="MathClass-rel" stretchy="false">
            |
           </mo>
           <mi>
            i
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      is key to Eq. <a href="#x5-84004r6">(3.6)</a>, and this probability of a sensor input at each robot position must be computed using some model. An obvious
     strategy would be to consult the robot’s map, identifying the probability of particular sensor readings with each possible
     map position, given knowledge about the robot’s sensor geometry and the mapped environment. The value of
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            l
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      is easy to recover in this case.
     It is simply the probability
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            r
           </mi>
           <mo class="MathClass-rel" stretchy="false">
            =
           </mo>
           <mi>
            l
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      associated with the belief state before the perceptual update process.
     </p>
     <p class="noindent">
      Finally, note that the denominator
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        P
       </mi>
       <msup>
        <mrow>
         <mrow>
          <mo fence="true" form="prefix">
           (
          </mo>
          <mrow>
           <mi>
            i
           </mi>
          </mrow>
          <mo fence="true" form="postfix">
           )
          </mo>
         </mrow>
        </mrow>
        <mrow>
        </mrow>
       </msup>
      </math>
      does
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      depend upon
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      ; that is,
     as we apply Eq. <a href="#x5-84004r6">(3.6)</a>
     to all positions
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      in
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        L
       </mi>
      </math>
      , the denominator never varies.
     </p>
     <div class="warning">
      <p class="noindent">
       Because it is effectively constant, in practice this denominator is usually dropped and, at the end
      of the perception update step, all probabilities in the belief state are re-normalized to sum at 1.0.
      </p>
     </div>
     <p class="noindent">
      Now consider the
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi class="qopname">
        Act
       </mi>
       <mo>
        ⁡
       </mo>
      </math>
      function of Eq. <a href="#x5-82001r1">(3.1)</a>. Act maps a former belief state and encoder measurement (i.e. robot action) to a new belief state. To compute the
     probability of position
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      in the new belief state, one must integrate over all the possible ways in which the robot may have reached
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        l
       </mi>
      </math>
      according to the potential positions expressed in the former belief state. This is subtle but fundamentally important.
     The same location l can be reached from multiple source locations with the same encoder measurement
     o because the encoder measurement is uncertain. Temporal indices are required in this update equation:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-84005r7">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <msubsup>
              <mrow>
               <mi>
                o
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mo>
          ∫
         </mo>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
               <mi>
                ′
               </mi>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <msubsup>
              <mrow>
               <mi>
                o
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
               <mi>
                ′
               </mi>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mspace class="thinspace" width="0.17em">
         </mspace>
         <mstyle class="text">
          <mtext>
           d
          </mtext>
         </mstyle>
         <msubsup>
          <mrow>
           <mi>
            l
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mi>
            ′
           </mi>
          </mrow>
         </msubsup>
        </math>
       </td>
       <td class="eq-no">
        (3.7)
       </td>
      </tr>
     </table>
     <p class="noindent">
      Thus, the total probability for a specific position l is built up from the individual contribu- tions from every location l’ in the
     former belief state given encoder measurement o. Equations 5.21 and 5.22 form the basis of Markov localization, and they
     incorporate the Markov assumption. Formally, this means that their output is a function only of the robot’s previous state
     and its most recent actions (odometry) and perception. In a general, non- Markovian situation, the state of a
     system depends upon all of its history. After all, the value of a robot’s sensors at time t do not really depend only
     on its position at time t. They depend to some degree on the trajectory of the robot over time; indeed on the
     entire history of the robot. For example, the robot could have experienced a serious collision recently that has
     biased the sensor’s behavior. By the same token, the position of the robot at time t does not really depend only
     on its position at time t-1 and its odometric measurements. Due to its history of motion, one wheel may have
     worn more than the other, causing a left-turning bias over time that affects its current position. So the Markov
     assumption is, of course, not a valid assumption. However the Markov as- sumption greatly simplifies tracking,
     reasoning and planning and so it is an approximation that continues to be extremely popular in mobile robotics.
      <a id="subsubsection*.34">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x5-85000">
      </a>
      Application: Markov Localisation using a Topological Map
     </h5>
     <p class="noindent">
      A straightforward application of Markov localization is possible when the robot’s environ- ment representation already
     provides an appropriate decomposition. This is the case when the environment representation is purely topological.
     </p>
     <p class="noindent">
      Consider a contest in which each robot is to receive a topological description of the environ- ment. The description would describe only
     the connectivity of hallways and rooms, with no mention of geometric distance. In addition, this supplied map would be imperfect,
     contain- ing several false arcs (e.g. a closed door). Such was the case for the 1994 AAAI National Robot Contest, at which each
     robot’s mission was to use the supplied map and its own sen- sors to navigate from a chosen starting position to a target room.
     </p>
     <p class="noindent">
      Dervish
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         19
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/dervish.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          19
         </sup>
        </span>
       </alert>
       <span id="textcolor123">
        .
       </span>
      </span>
      ,
     the
     winner
     of
     this
     contest,
     employed
     probabilistic
     Markov
     localization
     and
     used
     just
     this
     multiple
     hypothesis
     belief
     state
     over
     a
     topological
     environmental
     representation.
     We
     now
     describe
     Dervish
     as
     an
     example
     of
     a
     robot
     with
     a
     topological
     representation
     and
     a
     probabilistic
     localization
     algorithm.
     </p>
     <p class="noindent">
      Dervish,
     shown
     in
     Figure
     5.20,
     includes
     a
     sonar
     arrangement
     custom-designed
     for
     the
     1994
     AAAI
     National
     Robot
     Contest.
     The
     environment
     in
     this
     contest
     consisted
     of
     a
     rectilinear
     indoor
     office
     space
     filled
     with
     real
     office
     furniture
     as
     obstacles.
     Traditional
     sonars
     are
     ar-
     ranged
     radially
     around
     the
     robot
     in
     a
     ring.
     Robots
     with
     such
     sensor
     configurations
     are
     sub-
     ject
     to
     both
     tripping
     over
     short
     objects
     below
     the
     ring
     and
     to
     decapitation
     by
     tall
     objects
     (such
     as
     ledges,
     shelves
     and
     tables)
     that
     are
     above
     the
     ring.
     Dervish’s
     answer
     to
     this
     challenge
     was
     to
     arrange
     one
     pair
     of
     sonars
     diagonally
     upward
     to
     detect
     ledges
     and
     other
     overhangs.
     In
     addition,
     the
     diagonal
     sonar
     pair
     also
     proved
     to
     ably
     detect
     tables,
     enabling
     the
     robot
     to
     avoid
     wandering
     underneath
     tall
     tables.
     The
     remaining
     sonars
     were
     clustered
     in
     sets
     of
     sonars,
     such
     that
     each
     individual
     transducer
     in
     the
     set
     would
     be
     at
     a
     slightly
     varied
     angle
     to
     minimize
     specularity.
     Finally,
     two
     sonars
     near
     the
     robot’s
     base
     were
     able
     to
     detect
     low
     obstacles
     such
     as
     paper
     cups
     on
     the
     floor.
     </p>
     <p class="noindent">
      We
     have
     already
     noted
     that
     the
     representation
     provided
     by
     the
     contest
     organizers
     was
     purely
     topological,
     noting
     the
     connectivity
     of
     hallways
     and
     rooms
     in
     the
     office
     environment.
     Thus,
     it
     would
     be
     appropriate
     to
     design
     Dervish’s
     perceptual
     system
     to
     detect
     matching
     perceptual
     events:
     the
     detection
     and
     passage
     of
     connections
     between
     hallways
     and
     offices.
     </p>
     <p class="noindent">
      This
     abstract
     perceptual
     system
     was
     implemented
     by
     viewing
     the
     trajectory
     of
     sonar
     strikes
     to
     the
     left
     and
     right
     sides
     of
     Dervish
     over
     time.
     Interestingly,
     this
     perceptual
     system
     would
     use
     time
     alone
     and
     no
     concept
     of
     encoder
     value
     in
     order
     to
     trigger
     perceptual
     events.
     Thus,
     for
     instance,
     when
     the
     robot
     detects
     a
     7
     to
     17
     cm
     indentation
     in
     the
     width
     of
     the
     hallway
     for
     more
     than
     one
     second
     continuously,
     a
     closed
     door
     sensory
     event
     is
     triggered.
     If
     the
     sonar
     strikes
     jump
     well
     beyond
     17
     cm
     for
     more
     than
     one
     second,
     an
     open
     door
     sensory
     event
     trig-
     gers.
     </p>
     <p class="noindent">
      Sonars
     have
     a
     notoriously
     problematic
     error
     mode
     known
     as
     specular
     reflection:
     when
     the
     sonar
     unit
     strikes
     a
     flat
     surface
     at
     a
     shallow
     angle,
     the
     sound
     may
     reflect
     coherently
     away
     from
     the
     transducer,
     resulting
     in
     a
     large
     overestimate
     of
     range.
     Dervish
     was
     able
     to
     filter
     such
     potential
     noise
     by
     tracking
     its
     approximate
     angle
     in
     the
     hallway
     and
     completely
     sup-
     pressing
     sensor
     events
     when
     its
     angle
     to
     the
     hallway
     parallel
     exceeded
     9
     degrees.
     Interest-
     ingly,
     this
     would
     result
     in
     a
     conservative
     perceptual
     system
     that
     would
     easily
     miss
     features
     because
     of
     this
     suppression
     mechanism,
     particularly
     when
     the
     hallway
     is
     crowded
     with
     ob-
     stacles
     that
     Dervish
     must
     negotiate.
     Once
     again,
     the
     conservative
     nature
     of
     the
     perceptual
     system,
     and
     in
     particular
     its
     tendency
     to
     issue
     false
     negatives,
     would
     point
     to
     a
     probabilistic
     solution
     to
     the
     localization
     problem
     so
     that
     a
     complete
     trajectory
     of
     perceptual
     inputs
     could
     be
     considered.
     </p>
     <p class="noindent">
      Dervish’s
     environment
     representation
     was
     a
     classical
     topological
     map,
     identical
     in
     abstrac-
     tion
     and
     information
     to
     the
     map
     provided
     by
     the
     contest
     organizers.
     Figure
     5.21
     depicts
     a
     geometric
     representation
     of
     a
     typical
     office
     environment
     and
     the
     topological
     map
     for
     the
     same
     office
     environment.
     One
     can
     place
     nodes
     at
     each
     intersection
     and
     in
     each
     room,
     re-
     sulting
     in
     the
     case
     of
     figure
     5.21
     with
     four
     nodes
     total.
     </p>
     <p class="noindent">
      Once
     again,
     though,
     it
     is
     crucial
     that
     one
     maximize
     the
     information
     content
     of
     the
     represen-
     tation
     based
     on
     the
     available
     percepts.
     This
     means
     reformulating
     the
     standard
     topological
     graph
     shown
     in
     Figure
     5.21
     so
     that
     transitions
     into
     and
     out
     of
     intersections
     may
     both
     be
     used
     for
     position
     updates.
     Figure
     5.22
     shows
     a
     modification
     of
     the
     topological
     map
     in
     which
     just
     this
     step
     has
     been
     taken.
     In
     this
     case,
     note
     that
     there
     are
     7
     nodes
     in
     contrast
     to
     4.
     In
     order
     to
     represent
     a
     specific
     belief
     state,
     Dervish
     associated
     with
     each
     topological
     node
     n
     a
     probability
     that
     the
     robot
     is
     at
     a
     physical
     position
     within
     the
     boundaries
     of
     n:
     p(r
     =
     n)
     .
     t
     As
     will
     become
     clear
     below,
     the
     probabilistic
     update
     used
     by
     Dervish
     was
     approximate,
     therefore
     technically
     one
     should
     refer
     to
     the
     resulting
     values
     as
     likelihoods
     rather
     than
     prob-
     abilities.
     </p>
     <p class="noindent">
      The
     perception
     update
     process
     for
     Dervish
     functions
     precisely
     as
     in
     Equation
     (5.21).
     Per-
     ceptual
     events
     are
     generated
     asynchronously,
     each
     time
     the
     feature
     extractor
     is
     able
     to
     rec-
     ognize
     a
     large-scale
     feature
     (e.g.
     doorway,
     intersection)
     based
     on
     recent
     ultrasonic
     values.
     Each
     perceptual
     event
     consists
     of
     a
     percept-pair
     (a
     feature
     on
     one
     side
     of
     the
     robot
     or
     two
     features
     on
     both
     sides).
     </p>
     <p class="noindent">
     </p>
     <figure class="float">
      <table class="tabularray tblr" id="tbl-1">
       <div display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <tr id="row-1-1-">
         <td id="cell1-1-1" style="text-align:right;vertical-align:top;">
         </td>
         <td id="cell1-1-2" style="text-align:justify;vertical-align:top;">
          Wall
         </td>
         <td id="cell1-1-3" style="text-align:justify;vertical-align:top;">
          Closed
Door
         </td>
         <td id="cell1-1-4" style="text-align:justify;vertical-align:top;">
          Open
Door
         </td>
         <td id="cell1-1-5" style="text-align:justify;vertical-align:top;">
          Open
Hallway
         </td>
         <td id="cell1-1-6" style="text-align:justify;vertical-align:top;">
          Foyer
         </td>
        </tr>
        <tr id="row-1-2-">
         <td id="cell1-2-1" style="text-align:right;vertical-align:top;">
          Nothing
Detected
         </td>
         <td id="cell1-2-2" style="text-align:justify;vertical-align:top;">
          0.70
         </td>
         <td id="cell1-2-3" style="text-align:justify;vertical-align:top;">
          0.40
         </td>
         <td id="cell1-2-4" style="text-align:justify;vertical-align:top;">
          0.05
         </td>
         <td id="cell1-2-5" style="text-align:justify;vertical-align:top;">
          0.001
         </td>
         <td id="cell1-2-6" style="text-align:justify;vertical-align:top;">
          0.30
         </td>
        </tr>
        <tr id="row-1-3-">
         <td id="cell1-3-1" style="text-align:right;vertical-align:top;">
          Closed
Door
Detected
         </td>
         <td id="cell1-3-2" style="text-align:justify;vertical-align:top;">
          0.30
         </td>
         <td id="cell1-3-3" style="text-align:justify;vertical-align:top;">
          0.60
         </td>
         <td id="cell1-3-4" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-3-5" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-3-6" style="text-align:justify;vertical-align:top;">
          0.05
         </td>
        </tr>
        <tr id="row-1-4-">
         <td id="cell1-4-1" style="text-align:right;vertical-align:top;">
          Open
Door
Detected
         </td>
         <td id="cell1-4-2" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-4-3" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-4-4" style="text-align:justify;vertical-align:top;">
          0.90
         </td>
         <td id="cell1-4-5" style="text-align:justify;vertical-align:top;">
          0.10
         </td>
         <td id="cell1-4-6" style="text-align:justify;vertical-align:top;">
          0.15
         </td>
        </tr>
        <tr id="row-1-5-">
         <td id="cell1-5-1" style="text-align:right;vertical-align:top;">
          Closed
Hallway
Detected
         </td>
         <td id="cell1-5-2" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-5-3" style="text-align:justify;vertical-align:top;">
          0
         </td>
         <td id="cell1-5-4" style="text-align:justify;vertical-align:top;">
          0.001
         </td>
         <td id="cell1-5-5" style="text-align:justify;vertical-align:top;">
          0.90
         </td>
         <td id="cell1-5-6" style="text-align:justify;vertical-align:top;">
          0.5
         </td>
        </tr>
       </div>
      </table>
      <a id="x5-85001r1">
      </a>
      <figcaption class="caption">
       <span class="id">
        Table
       3.1:
       </span>
       <span class="content">
        The certainty matrix for the robot <span class="cite"><a href="MobileRoboticsLectureBookad1.html#nourbakhsh1995dervish">[72]</a></span>.
       </span>
      </figcaption>
     </figure>
     <p class="noindent">
      Given a specific percept pair i, Equation (5.21) enables the likelihood of each possible po- sition n to be updated using the formula:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-85003r8">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              n
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <mi>
              i
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              i
             </mi>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.8)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      The value of p(n) is already available from the current belief state of Dervish, and so the challenge lies in computing
     p(i|n). The key simplification for Dervish is based upon the re- alization that, because the feature extraction
     system only extracts 4 total features and be- cause a node contains (on a single side) one of 5 total features,
     every possible combination of node type and extracted feature can be represented in a 4 x 5 table. Dervish’s
     certainty matrix (show in Table 5.1) is just this lookup table. Dervish makes the simplifying assumption that
     the performance of the feature detector (i.e. the probability that it is correct) is only a function of the feature
     extracted and the actual feature in the node. With this assumption in hand, we can populate the certainty matrix
     with confidence estimates for each possible pairing of perception and node type. For each of the five world fea-
     tures that the robot can encounter (wall, closed door, open door, open hallway and foyer) this matrix assigns a
     likelihood for each of the three one-sided percepts that the sensory sys- tem can issue. In addition, this matrix
     assigns a likelihood that the sensory system will fail to issue a perceptual event altogether (nothing detected).
     </p>
     <p class="noindent">
      For example, using the specific values in Table 5.1, if Dervish is next to an open hallway, the likelihood of
     mistakenly recognizing it as an open door is 0.10. This means that for any node n that is of type Open
     Hallway and for the sensor value i=Open door, p(i|n) = 0.10. Together with a specific topological map,
     the certainty matrix enables straightforward com- putation of p(i|n) during the perception update process.
     </p>
     <p class="noindent">
      For Dervish’s particular sensory suite and for any specific environment it intends to navi- gate, humans
     generate a specific certainty matrix that loosely represents its perceptual con- fidence, along with a
     global measure for the probability that any given door will be closed versus opened in the real world.
     </p>
     <p class="noindent">
      Recall that Dervish has no encoders and that perceptual events are triggered asynchronously by the feature extraction processes.
     Therefore, Dervish has no action update step as depicted by Equation (5.22). When the robot does detect a perceptual event,
     multiple perception up- date steps will need to be performed in order to update the likelihood of every possible
     robot position given Dervish’s former belief state. This is because there is often a chance that the robot has
     traveled multiple topological nodes since its previous perceptual event (i.e. false negative errors). Formally, the
     perception update formula for Dervish is in reality a combi- nation of the general form of action update and
     perception update. The likelihood of posi- tion n given perceptual event i is calculated as in Equation (5.22):
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-85004r9">
         </mstyle>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <msubsup>
              <mrow>
               <mi>
                o
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mo>
          ∫
         </mo>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-rel" stretchy="false">
              |
             </mo>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
               <mi>
                ′
               </mi>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mspace class="thinspace" width="0.17em">
             </mspace>
             <msubsup>
              <mrow>
               <mi>
                o
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mi>
          P
         </mi>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                l
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
               <mo class="MathClass-bin" stretchy="false">
                −
               </mo>
               <mn>
                1
               </mn>
              </mrow>
              <mrow>
               <mi>
                ′
               </mi>
              </mrow>
             </msubsup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mspace class="thinspace" width="0.17em">
         </mspace>
         <mstyle class="text">
          <mtext>
           d
          </mtext>
         </mstyle>
         <msubsup>
          <mrow>
           <mi>
            l
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
           <mo class="MathClass-bin" stretchy="false">
            −
           </mo>
           <mn>
            1
           </mn>
          </mrow>
          <mrow>
           <mi>
            ′
           </mi>
          </mrow>
         </msubsup>
        </math>
       </td>
       <td class="eq-no">
        (3.9)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      The value of p(n’ ) denotes the likelihood of Dervish being at position n’ as represented by Dervish’s former belief state. The temporal
     subscript t-i is used in lieu of t-1 because for each possible position n’ the discrete topological distance from n’ to n can vary depending
     on the specific topological map. The calculation of p(n n’ , i ) is performed by multi- tt-it plying the probability of generating
     perceptual event i at position n by the probability of hav- ing failed to generate perceptual events at all nodes between n’ and n:
     </p>
     <p class="noindent">
      For example (figure 5.23), suppose that the robot has only two nonzero nodes in its belief state, 1-2, 2-3, with likelihoods
     associated with each possible position: p(1-2) = 1.0 and p(2-3) = 0.2. For simplicity assume the robot is facing East with
     certainty. Note that the likelihoods for nodes 1-2 and 2-3 do not sum to 1.0. These values are not formal probabil- ities, and so
     computational effort is minimized in Dervish by avoiding normalization alto- gether. Now suppose that a perceptual event is
     generated: the robot detects an open hallway on its left and an open door on its right simultaneously. State 2-3 will
     progress potentially to states 3, 3-4 and 4. But states 3 and 3-4 can be elimi- nated because the likelihood of
     detecting an open door when there is only wall is zero. The likelihood of reaching state 4 is the product of the
     initial likelihood for state 2-3, 0.2, the like- lihood of not detecting anything at node 3, (a), and the likelihood
     of detecting a hallway on the left and a door on the right at node 4, (b). Note that we assume the likelihood
     of detecting nothing at node 3-4 is 1.0 (a simplifying approximation). (a) occurs only if Dervish fails to detect
     the door on its left at node 3 (either closed or open), [(0.6)(0.4) + (1-0.6)(0.05)], and correctly detects nothing
     on its right, 0.7. (b) occurs if Dervish correctly identifies the open hallway on its left at node 4, 0.90, and mis-
     takes the right hallway for an open door, 0.10. The final formula, (0.2)[(0.6)(0.4)+(0.4)(0.05)](0.7)[(0.9)(0.1)],
     yields a likelihood of 0.003 for state 4. This is a partial result for p(4) following from the prior belief state node
     2-3. Turning to the other node in Dervish’s prior belief state, 1-2 will potentially progress to states 2, 2-3, 3,
     3-4 and 4. Again, states 2-3, 3 and 3-4 can all be eliminated since the like- lihood of detecting an open door
     when a wall is present is zero. The likelihood of state 2 is the product of the prior likelihood for state 1-2, (1.0),
     the likelihood of detecting the door on the right as an open door, [(0.6)(0) + (0.4)(0.9)], and the likelihood of
     correctly detecting an open hallway to the left, 0.9. The likelihood for being at state 2 is then (1.0)(0.4)(0.9)(0.9) =
     0.3. In addition, 1-2 progresses to state 4 with a certainty factor of -6 4.3  10 , which is added to the certainty
     factor above to bring the total for state 4 to 0.00328. Dervish would therefore track the new belief state to be 2,
     4, assigning a very high likelihood to position 2 and a low likelihood to position 4. Empirically, Dervish’s map
     representation and localization system have proven to be suffi- cient for navigation of four indoor office environments: the
     artificial office environment cre- ated explicitly for the 1994 National Conference on Artificial Intelligence; the
     psychology department, the history department and the computer science department at Stanford Uni- versity. All of
     these experiments were run while providing Dervish with no notion of the distance between adjacent nodes in its
     topological map. It is a demonstration of the power of probabilistic localization that, in spite of the tremendous lack of
     action and encoder infor- mation, the robot is able to navigate several real-world office buildings successfully.
     </p>
     <p class="noindent">
      One open question remains with respect to Dervish’s localization system. Dervish was not just a localizer but also a navigator. As
     with all multiple hypothesis systems, one must ask the question, how does the robot decide how to move, given that it has
     multiple possible ro- bot positions in its representation? The technique employed by Dervish is a most common technique in the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ics field: plan the robot’s actions by assuming that the robot’s actual position is its most likely node in the
     belief state. Generally, the most likely position is a good measure of the robot’s actual world position. However,
     this technique has short- comings when the highest and second highest most likely positions have similar values.
     In the case of Dervish, it nonetheless goes with the highest likelihood position at all times, save at one critical
     juncture. The robot’s goal is to enter a target room and remain there. There- fore, from the point of view of
     its goal, it is critical that it finish navigating only when the robot has strong confidence in being at the correct
     final location. In this particular case, Der- vish’s execution module refuses to enter a room if the gap between
     the most likely position and the second likeliest position is below a preset threshold. In such a case, Dervish
     will actively plan a path that causes it to move further down the hallway in an attempt to collect more sensor
     data and thereby increase the relative likelihood of one position in the belief state. Although computationally
     unattractive, one can go further, imagining a planning system for robots such as Dervish for which one specifies
     a goal belief state rather than a goal position. The robot can then reason and plan in order to achieve a goal
     confidence level, thus explic- itly taking into account not only robot position but also the measured likelihood
     of each po- sition. An example of just such a procedure is the Sensory Uncertainty Field of Latombe [90], in
     which the robot must find a trajectory that reaches its goal while maximizing its lo- calization confidence enroute.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.6.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.6.3
      </small>
      <a id="x5-860003.6.3">
      </a>
      Kalman  Filter  Localisation
     </h3>
     <p class="noindent">
      The Markov localization model can represent any probability density function over robot
     </p>
     <p class="noindent">
      position. This approach is very general but, due to its generality, inefficient. A successful alternative is to use a more compact
     representation of a specific class of probability densi- ties. The Kalman filter does just this, and is an optimal recursive data
     processing algorithm. It incorporates all information, regardless of precision, to estimate the current value of the variable
     of interest. A comprehensive introduction can be found in [46] and a more detailed treatment is presented in
     [28]. Figure 5.26 depicts the a general scheme of Kalman filter estimation, where the system has a control signal
     and system error sources as inputs. A measuring device enables measuring some system states with errors. The
     Kalman filter is a mathematical mechanism for produc- ing an optimal estimate of the system state based on
     the knowledge of the system and the measuring device, the description of the system noise and measurement
     errors and the un- certainty in the dynamics models. Thus the Kalman filter fuses sensor signals and system
     knowledge in an optimal way. Optimality depends on the criteria chosen to evaluate the per- formance and on the
     assumptions. Within the Kalman filter theory the system is assumed to be linear and white with Gaussian noise. As we
     have discussed earlier, the assumption of Gaussian error is invalid for our
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      applications but, nevertheless,
     the results are extremely useful. In other engineering disciplines, the Gaussian error assumption has in some
     cases been shown to be quite accurate [46]. We begin with a subsection that introduces Kalman filter theory,
     then we present an appli- cation of that theory to the problem of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      localization. Finally, the third subsec-
     tion will present a case study of a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      that navigates indoor spaces by virtue of Kalman filter localization.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.6.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.6.4
      </small>
      <a id="x5-870003.6.4">
      </a>
      An  Implementation  of  Kalman  Filter
     </h3>
     <p class="noindent">
      Let’s
     make
     a
     toy
     example:
     You’ve
     built
     a
     little
     robot
     that
     can
     wander
     around
     in
     the
     woods,
     and
     the
     robot
     needs
     to
     know
     exactly
     where
     it
     is
     so
     that
     it
     can
     navigate.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         20
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/toybot-goes.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          20
         </sup>
        </span>
       </alert>
       <span id="textcolor124">
        The
       toybot
       is
       currently
       observing
       its
       surrounding.
       </span>
      </span>
     </p>
     <p class="noindent">
      We’ll say our robot has a state
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mover accent="true">
          <mrow>
           <mi>
            x
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
        </mrow>
        <mrow>
         <mi>
          k
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      ,
     which  is  just  a  position  and  a  velocity:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-87001r10">
         </mstyle>
         <msubsup>
          <mrow>
           <mover accent="true">
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mo accent="true">
             →
            </mo>
           </mover>
          </mrow>
          <mrow>
           <mi>
            k
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mover accent="true">
              <mrow>
               <mi>
                p
               </mi>
              </mrow>
              <mo accent="true">
               →
              </mo>
             </mover>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <mover accent="true">
              <mrow>
               <mi>
                v
               </mi>
              </mrow>
              <mo accent="true">
               →
              </mo>
             </mover>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.10)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      Note that the state is just a list of numbers about the underlying configuration of your system; it could be anything. In our
     example it’s position and velocity, but it could be data about the amount of fluid in a tank, the temperature of a
     car engine, the position of a user’s finger on a touchpad, or any number of things you need to keep track of.
     </p>
     <p class="noindent">
      Our
     robot
     also
     has
     a
     GPS
     sensor,
     which
     is
     accurate
     to
     about
     10
     meters,
     which
     is
     good,
     but
     it
     needs
     to
     know
     its
     location
     more
     precisely
     than
     10
     meters.
     There
     are
     lots
     of
     gullies
     and
     cliffs
     in
     these
     woods,
     and
     if
     the
     robot
     is
     wrong
     by
     more
     than
     a
     few
     feet,
     it
     could
     fall
     off
     a
     cliff.
     So
     GPS
     by
     itself
     is
     not
     good
     enough.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         21
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/toybot-falls.png" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          21
         </sup>
        </span>
       </alert>
       <span id="textcolor125">
        The
       toybot
       has
       miscalculated
       its
       position.
       </span>
      </span>
      <a id="subsubsection*.35">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x5-88000">
      </a>
      Kalman
     Filter
     Localisation
     </h5>
     <p class="noindent">
      The
     Kalman
     filter
     is
     an
     optimal
     and
     efficient
      <alert style="color: #821131;">
       sensor
fusion
      </alert>
      technique.
     </p>
     <div class="warning">
      <p class="noindent">
       Application
      of
      the
      Kalman
      filter
      to
      localisation
      requires
      posing
      the
      robot
      localisation
      problem
      as
      a
      sensor
      fusion
      problem.
      </p>
     </div>
     <p class="noindent">
      Recall
     that
     the
     basic
     probabilistic
     update
     of
     robot
     belief
     state
     can
     be
     segmented
     into
     two
     <alert style="color: #821131;">(2)</alert>
     phases:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        perception
       update,
       and
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        action
       update
       </p>
      </li>
     </ul>
     <p class="noindent">
      The
     fundamental
     difference
     between
     the
     Kalman
     filter
     approach
     and
     Markov
     localisation
     approach
     lies
     in
     the
     perception
     update
     process.
     </p>
     <p class="noindent">
     </p>
     <div class="quoteblock">
      <p class="noindent">
       In
      Markov
      localisation,
      the
      entire
      perception
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          22
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           22
          </sup>
         </span>
        </alert>
        <span id="textcolor128">
         i.e.,
        the
        robot’s
        set
        of
        instantaneous
        sensor
        measurements.
        </span>
       </span>
       is
      used
      to
      update
      each
      possible
      robot
      position
      in
      the
      belief
      state
      individually
      using
      Bayes
      formula.
      </p>
     </div>
     <div class="knowledge">
      <p class="noindent">
       In
      some
      cases,
      the
      perception
      is
      abstract,
      having
      been
      produced
      by
      a
      feature
      extraction
      mechanism.
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          23
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           23
          </sup>
         </span>
        </alert>
        <span id="textcolor129">
         as
        in
        Dervish.
        </span>
       </span>
       In
      other
      cases,
      as
      with
      Rhino,
      the
      perception
      consists
      of
      raw
      sensor
      readings.
      </p>
     </div>
     <p class="noindent">
      By
     contrast,
     perception
     update
     using
     a
     Kalman
     filter
     is
     a
      <alert style="color: #821131;">
       multi-step
      </alert>
      process.
     The
     robot’s
     total
     sensory
     input
     is
     treated,
     not
     as
     a
     monolithic
     whole,
     but
     as
     a
     set
     of
     extracted
     features
     which
     each
     relate
     to
     objects
     in
     the
     environment.
     Given
     a
     set
     of
     possible
     features,
     the
     Kalman
     filter
     is
     used
     to
     fuse
     the
     distance
     estimate
     from
     each
     feature
     to
     a
     matching
     object
     in
     the
     map.
     Instead
     of
     carrying
     out
     this
     matching
     process
     for
     many
     possible
     robot
     locations
     individually
     as
     in
     the
     Markov
     approach,
     the
     Kalman
     filter
     accomplishes
     the
     same
     probabilistic
     update
     by
     treating
     the
     whole,
     unimodal
     and
     Gaussian
     belief
     state
     at
     once.
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-88001r11">
       3.11
      </a>
      depicts
     the
     particular
     schematic
     for
     Kalman
     filter
     localisation.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/kalman-filter-localisation.png" width="150%"/>
       <a id="x5-88001r11">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.11:
       </span>
       <span class="content">
        The schematic for the Kalman filter localisation
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The
     first
     step
     is
     action
     update
     or
     position
     prediction,
     the
     straightforward
     application
     of
     a
     Gaussian
     error
     motion
     model
     to
     the
     robot’s
     measured
     encoder
     travel.
     The
     robot
     then
     collects
     actual
     sensor
     data
     and
     extracts
     appropriate
     features
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         24
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          24
         </sup>
        </span>
       </alert>
       <span id="textcolor130">
        e.g.
       lines,
       doors,
       or
       even
       the
       value
       of
       a
       specific
       sensor
       </span>
      </span>
      in
     the
     observation
     step.
     At
     the
     same
     time,
     based
     on
     its
     predicted
     position
     in
     the
     map,
     the
     robot
     generates
     a
     measurement
     prediction
     which
     identifies
     the
     features
     which
     the
     robot
     expects
     to
     find
     and
     the
     positions
     of
     those
     features.
     In
     matching
     the
     robot
     identifies
     the
     best
     pairings
     between
     the
     features
     actually
     extracted
     during
     observation
     and
     the
     expected
     features
     due
     to
     measurement
     prediction.
     Finally,
     the
     Kalman
     filter
     can
     fuse
     the
     information
     provided
     by
     all
     of
     these
     matches
     in
     order
     to
     update
     the
     robot
     belief
     state
     in
     estimation.
     </p>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-7" sec="section-7">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.7" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.7
      </small>
      <a id="x5-890003.7">
      </a>
      Other  Examples  of  Localisation  Methods
     </h2>
     <p class="noindent">
      Markov localisation and Kalman filter localisation have been two extremely popular strat- egies for research
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      systems
     navigating indoor environments. They have strong formal bases and therefore well-defined behavior. But there are a large number
     of other lo- calisation techniques that have been used with varying degrees of success on commercial and research
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      platforms. We will not explore the space of all localisation sys- tems in detail. Refer to surveys such as [4] for such information.
     There are, however, several categories of localisation techniques that deserve mention. Not surprisingly, many
     implementations of these techniques in commercial robotics employ modifications of the robot’s environment, something
     that the Markov localisation and Kal- man filter localisation communities eschew. In the following sections, we
     briefly identify the general strategy incorporated by each category and reference example systems, includ- ing as
     appropriate those that modify the environment and those that function without envi- ronmental modification.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.7.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.7.1
      </small>
      <a id="x5-900003.7.1">
      </a>
      Landmark-based  Navigation
     </h3>
     <p class="noindent">
      Landmarks are generally defined as
      <alert style="color: #821131;">
       passive objects
      </alert>
      in the environment which provide a high degree of localisation
     accuracy when they are within the robot’s field of view. Mobile robots that make use of landmarks for localisation
     generally use artificial markers that have been placed by the robot’s designers to make localisation easy.
     </p>
     <p class="noindent">
      The control system for a landmark-based navigator consists of two <alert style="color: #821131;">(2)</alert>
     discrete phases.
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        When
       a
       landmark
       is
       in
       view,
       the
       robot
       localizes
       frequently
       and
       accurately,
       using
       action
       update
       and
       perception
       update
       to
        <alert style="color: #821131;">
         track
its
position
without
cumulative
error
        </alert>
        .
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        when
       the
       robot
       is
       in
       no
       landmark
       “zone”,
       then
       only
       action
       update
       occurs,
       and
       the
       robot
       accumulates
       position
       uncertainty
       until
       the
       next
       landmark
       enters
       the
       robot’s
       field
       of
       view.
       </p>
      </li>
     </ul>
     <p class="noindent">
      The robot is thus effectively dead-reckoning from landmark zone to landmark zone. This in turn means
     the robot must consult its map carefully, ensuring that each motion between landmarks is sufficiently
     short, given its motion model, that it will be able to localize successful upon reaching the next landmark.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/landmark-based-navigation.png" width="150%"/>
       <a id="x5-90001r12">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.12:
       </span>
       <span class="content">
        An  illustration  showing  the  object-level  landmarks  in  blue-boxes.  (a,b)  shows  two  different  indoor
       scenarios. The blue boxes represent the 3D object detection of object-level landmarks. The red dots
       indicate the nodes of the topological map. The yellow lines indicate the edges of the topological map.
       The green curve is the feasible navigation trajectory generated based on the proposed method <span class="cite"><a href="MobileRoboticsLectureBookad1.html#wang2022object">[73]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-90001r12">
       3.12
      </a>
      shows one instantiating of landmark-based localisation. The particular shape of the landmarks enables
     reliable and accurate pose estimation by the robot, which must travel using dead reckoning between the landmarks.
     </p>
     <p class="noindent">
      One key advantage of the landmark-based navigation approach is that a strong formal theory has been developed for this general
     system architecture [113]. In this work, the authors have shown precise assumptions and conditions which, when satisfied,
     guarantee that the robot will always be able to localize successfully. This work also led to a real-world demonstration of
     landmark-based localisation. Standard sheets of paper were placed on the ceiling of the Robotics Laboratory at Stanford
     University, each with a unique checkerboard pattern. A Nomadics 200
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      was fitted with a monochrome
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera aimed
     vertically up at the ceiling. By recognizing the paper landmarks, which were placed approximately 2 meters apart, the robot
     was able to localize to within several centimeters, then move using dead-reckoning to another landmark zone.
     </p>
     <p class="noindent">
      The primary disadvantage of landmark-based navigation is that in general
      <alert style="color: #821131;">
       it requires significant environmental modification
      </alert>
      .
     Landmarks are local, and therefore a large number is usually required to cover a large factory area or research laboratory. For
     example, the Robotics Laboratory at Stanford made use of approximately 30 discrete landmarks, all affixed individually to the ceiling.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.7.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.7.2
      </small>
      <a id="x5-910003.7.2">
      </a>
      Globally  Unique  Localisation
     </h3>
     <p class="noindent">
      The landmark-based navigation approach makes a strong general assumption:
     </p>
     <div class="warning">
      <p class="noindent">
       when the landmark is in the robot’s field of view, localisation is essentially perfect.
      </p>
     </div>
     <p class="noindent">
      One way to reach the near perfect
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      localisation is to effectively enable such an assumption to be valid wherever the robot is
     located. It would be revolutionary the robot’s sensors immediately identified its particular location, uniquely, and repeatedly.
     </p>
     <p class="noindent">
      Such a strategy for localisation is surely aggressive, but the question of whether it can be done is primarily a question of sensor
     technology software. Clearly, such a localisation system would need to use a sensor which collects a very large amount of information.
     </p>
     <div class="knowledge">
      <p class="noindent">
       Since vision does indeed collect far more information than other sensors, it has been used as the sensor of choice in research towards globally unique localisation.
      </p>
     </div>
     <div class="warning">
      <p class="noindent">
       If humans were able to look at an individual picture and identify the robot’s location in a well-known environment, then one could
      argue that the information for globally unique localisation does exist within the picture. It must simply be interpreted correctly.
      </p>
     </div>
     <p class="noindent">
      One such approach has been attempted by several researchers and involves constructing one or more image histograms to
     represent the information content of an image stably (see for example Figure 4.51 and Section 4.3.2.2). A robot using such
     an image histogramming system has been shown to uniquely identify individual rooms in an office building as
     well as individual sidewalks in an outdoor environment. However, such a system is highly sensitive to external
     illumination and provides only a level of localisation resolution equal to the visual footprint of the camera optics.
     </p>
     <p class="noindent">
      The Angular histogram depicted in Figure 5.37 is another example in which the robot’s sensor values are transformed into an
     identifier of location. However, due to the limited information content of sonar ranging strikes, it is likely that two places
     in the robot’s environment may have angular histograms that are too similar to be differentiated successfully.
     </p>
     <p class="noindent">
      One way of attempting to gather sufficient sonar information for global localisation is to allow the robot time to gather a large
     amount of sonar data into a local evidence grid (i.e. occupancy grid) first, then match the local evidence grid with a global metric
     map of the environment. In [115] the researchers demonstrate such a system as able to localize on-thefly even as significant
     changes are made to the environment, degrading the fidelity of the map. Most interesting is that the local evidence grid
     represents information well enough that it can be used to correct and update the map over time, thereby leading to a localisation
     system that provides corrective feedback to the environment representation directly. This is similar in spirit to the idea of taking
     rejected observed features in the Kalman filter localisation algorithm and using them to create new features in the map.
     </p>
     <p class="noindent">
      A most promising, new method for globally unique localisation is called Mosaic-based localisation [114]. This fascinating
     approach takes advantage of an environmental feature that is rarely used by
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      s: fine-grained floor texture. This method
     succeeds primarily because of the recent ubiquity of very fast processors, very fast cameras and very large storage media.
     </p>
     <p class="noindent">
      The robot is fitted with a high-quality high-speed
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:ccd">
       CCD
      </a>
      camera pointed toward the floor, ideally situated between the robot’s
     wheels and illuminated by a specialized light pattern off the camera axis to enhance floor texture. The robot
     begins by collecting images of the entire floor in the robot’s workspace using this camera. Of course the memory
     requirements are significant, requiring a 10GB drive in order to store the complete image library of a 300 x 300
     meter area. Once the complete image mosaic is stored, the robot can travel any trajectory on the floor while
     tracking its own position without difficulty. Localisation is performed by simply re cording one image, performing
     action update, then performing perception update by matching the image to the mosaic database using simple
     techniques based on image database matching. The resulting performance has been impressive: such a robot has been
     shown to localize repeatedly with 1mm precision while moving at 25 km/hr. The key advantage of globally unique
     localisation is that, when these systems function correctly, they greatly simplify robot navigation. The robot can
     move to any point and will always be assured of localizing by collecting a sensor scan. But the main disadvantage
     of globally unique localisation is that it is likely that this method will never offer a complete solution to the
     localisation problem. There will always be cases where local sensory information is truly ambiguous and, therefore,
     globally unique localisa- tion using only current sensor information is unlikely to succeed. Humans often have excel-
     lent local positioning systems, particularly in non-repeating and well-known environments such as their homes.
     However, there are a number of environments in which such immediate localisation is challenging even for humans:
     consider hedge mazes and large new office buildings with repeating halls that are identical. Indeed, the mosaic-based
     localisation pro- totype described above encountered such a problem in its first implementation. The floor of the
     factory floor had been freshly painted and was thus devoid of sufficient micro-fractures to generate texture for
     correlation. Their solution was to modify the environment after all, painting random texture onto the factory floor.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.7.3" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.7.3
      </small>
      <a id="x5-920003.7.3">
      </a>
      Positioning  Beacon  systems
     </h3>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/beacon-based-localisation.png" width="150%"/>
       <a id="x5-92001r13">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.13:
       </span>
       <span class="content">
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      With most beacon systems, the design depicted depends foremost upon geometric principles to effect localisation. In this case the
     robots must know the positions of the two pinger units in the global coordinate frame in order to localize themselves to the global
     coordinate frame. A popular type of beacon system in industrial robotic applications is depicted in Figure 5.39. In this case
     beacons are retroreflective markers that can be easily detected by a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      based on their reflection of energy back to the robot.
     Given known positions for the optical retroreflectors, a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      can identify its position whenever it has three such beacons in sight
     simultaneously. Of course, a robot with encoders can localize over time as well, and does not need to measure its angle to
     all three beacons at the same instant. The advantage of such beacon-based systems is usually extremely high
     engineered reliabil- ity. By the same token, significant engineering usually surrounds the installation of such a
     system in a specific commercial setting. Therefore, moving the robot to a different factory floor will be both
     time-consuming and expensive. Usually, even changing the routes used by the robot will require serious re-engineering.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.7.4" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.7.4
      </small>
      <a id="x5-930003.7.4">
      </a>
      Route-Based  Localisation
     </h3>
     <p class="noindent">
      Even
     more
     reliable
     than
     beacon-based
     systems
     are
     route-based
     localisation
     strategies.
     In
     this
     case,
     the
     route
     of
     the
     robot
     is
     explicitly
     marked
     so
     that
     it
     can
     determine
     its
     position,
     not
     relative
     to
     some
     global
     coordinate
     frame,
     but
     relative
     to
     the
     specific
     path
     it
     is
     allowed
     to
     travel.
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         25
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/line-following-robot.jpg" width="100%"/>
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          25
         </sup>
        </span>
       </alert>
       <span id="textcolor133">
        A
       perfect
       example
       for
       these
       kind
       of
       localisation
       is
       the
       traditional
       line
       following
       robot.
       The
       robot
       does
       not
       need
       to
       know
       where
       it
       is
       as
       its
       only
       job
       is
       to
       make
       sure
       the
       line
       it
       is
       following
       is
       within
       its
       vision <a id="x5-93001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#AaaravG2018">[74]</a></span>.
       </span>
      </span>
      There
     are
     many
     techniques
     for
     marking
     such
     a
     route
     and
     the
     subsequent
     intersections.
     </p>
     <div class="warning">
      <p class="noindent">
       In all cases, one is effectively creating a railway system, except the railway system is somewhat more flexible and certainly more human-friendly actual rail.
      </p>
     </div>
     <p class="noindent">
      For example, high UV-reflective, optically transparent paint can mark the route such that only the robot, using a specialized sensor,
     easily detects it. Alternatively, a guide wire buried underneath the hall can be detected using inductive coils located on the robot chassis.
     </p>
     <p class="noindent">
      In all such cases, the robot localisation problem is effectively trivialized by forcing the robot to always follow a prescribed path.
     While this may remove the
      <alert style="color: #821131;">
       autonomous
      </alert>
      part of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>, there are industrial unmanned guided vehicles that do deviate
     briefly from their route in order to avoid obstacles. Nevertheless, the cost of this extreme reliability is obvious:
     </p>
     <div class="warning">
      <p class="noindent">
       the robot is much more inflexible give such localisation means, and therefore any change to the robot’s behavior requires significant engineering and time.
      </p>
     </div>
     <p class="noindent">
     </p>
    </div>
    <div class="section-head" id="section-8" sec="section-8">
     <h2 class="sectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#section.3.8" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.8
      </small>
      <a id="x5-940003.8">
      </a>
      Building  Maps
     </h2>
     <p class="noindent">
      Humans are excellent navigators due to their remarkable ability to build cognitive maps <a id="x5-94001"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#mcnamara1989subjective">[75]</a></span> which form the
     basis of spatial memory <a id="x5-94002"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#chun1998contextual">[76]</a>, <a id="x5-94003"></a><a href="MobileRoboticsLectureBookad1.html#huang2023visual">[77]</a></span>. However, when it comes to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>, we unfortunately need to be more hands on.
     </p>
     <p class="noindent">
      All of the localisation strategies we have discussed previously require active human effort to install the robot into a space. Artificial environmental
     modifications may be necessary to reduce ambiguity <a id="x5-94004"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#meyer2011using">[78]</a></span>. Even if this is not so, a map of the environment must be created for the robot.
     </p>
     <div class="warning">
      <p class="noindent">
       But a robot which localizes successfully has the right sensors for detecting the environment, and so the robot ought to build its own map.
      </p>
     </div>
     <p class="noindent">
      This ambition goes to the heart of
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>. In prose, we can express our eventual goal as follows:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       Starting
      from
      an
      arbitrary
      initial
      point,
      a
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       should
      be
      able
      to
      autonomously
      explore
      the
      environment
      with
      its
      on-board
      sensors,
      gain
      knowledge
      about
      it,
      interpret
      the
      scene,
      build
      an
      appropriate
      map
      and
      localize
      itself
      relative
      to
      this
      map.
      </p>
     </div>
     <p class="noindent">
      While
     we
     have
     system
     which
     allows
     certain
     level
     of
     intelligence
     to
     robots,
     most
     applications
     require
     a
     connected
     network
     or
     a
     central
     node
     to
     achieve
     any
     autonomous
     action <a id="x5-94005"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#hazik2022fleet">[79]</a>, <a id="x5-94006"></a><a href="MobileRoboticsLectureBookad1.html#xidias2022intelligent">[80]</a></span>.
     Accomplishing
     this
     goal
     purely
     using
     internal
     components
     in
     a
     robust
     is
     probably
     years
     away,
     but
     an
     important
     sub-goal
     is
     the
     invention
     of
     techniques
     for
     autonomous
     creation
     and
     modification
     of
     an
     environment
     map.
     Of
     course
     a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      ’s
     sensors
     have
     only
     limited
     range,
     and
     so
     it
     must
     physically
     explore
     its
     environment
     to
     build
     such
     a
     map.
     So,
     the
     robot
     must
     not
     only
     create
     a
     map
     but
     it
     must
     do
     so
     while
     moving
     and
     localizing
     to
     explore
     the
     environment.
     This
     is
     often
     called
     the
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:slam">
       Simultaneous
      Localisation
      and
      Mapping
      (SLAM)
      </a>
      problem,
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         26
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          26
         </sup>
        </span>
       </alert>
       <span id="textcolor134">
        Computational
       problem
       of
       constructing
       or
       updating
       a
       map
       of
       an
       unknown
       environment
       while
       simultaneously
       keeping
       track
       of
       an
       agent’s
       location
       within
       it.
       While
       this
       initially
       appears
       to
       be
       a
       chicken
       or
       the
       egg
       problem,
       there
       are
       several
       algorithms
       known
       to
       solve
       it
       in,
       at
       least
       approximately
       and
       in
       reasonable
       time
       for
       certain
       environments.
       Popular
       solutions
       include
       the
       particle
       filter,
       extended
       Kalman
       filter,
       covariance
       intersection,
       and
       GraphSLAM.
       SLAM
       algorithms
       are
       based
       on
       concepts
       in
       computational
       geometry
       and
       computer
       vision,
       and
       are
       used
       in
       robot
       navigation,
       robotic
       mapping
       and
       odometry
       for
       virtual
       reality
       or
       augmented
       reality.
       </span>
      </span>
      arguably
     the
     most
     difficult
     problem
     specific
     to
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      systems.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/slam-car.jpg" width="150%"/>
       <a id="x5-94007r14">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.14:
       </span>
       <span class="content">
        2005 DARPA Grand Challenge winner Stanley performed SLAM as part of its autonomous driving system <span class="cite"><a href="MobileRoboticsLectureBookad1.html#Kivaan2007">[81]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      The reason why
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:slam">
       SLAM
      </a>
      is difficult is born precisely from the interaction between the robot’s position updates as it localises and
     its mapping actions. If a
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      updates its position based on an observation of an imprecisely known feature, the resulting
     position estimate becomes correlated with the feature location estimate. Similarly, the map becomes correlated with the position
     estimate if an observation taken from an imprecisely known position is used to update or add a feature to the map.
     </p>
     <div class="warning">
      <p class="noindent">
       For localisation the robot needs to know where the features are whereas for map building the robot needs to know where it is on the map.
      </p>
     </div>
     <p class="noindent">
      The only path to a complete and optimal solution to this joint problem is to consider all the correlations
     between between position estimation and feature location estimation. Such cross-correlated maps are called
     stochastic maps <a id="x5-94009"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#hashimoto2002humanoid">[82]</a></span>. Unfortunately, implementing such an optimal solution is computationally prohibitive.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.8.1" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.8.1
      </small>
      <a id="x5-950003.8.1">
      </a>
      Stochastic  Map  Technique
     </h3>
     <p class="noindent">
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-95001r15">
       3.15
      </a>
      shows a general schematic incorporating map building and maintenance into the standard localisation loop depicted by
     Figure (5.29) during discussion of Kalman filter localisation [9]. The added arcs represent the additional flow of
     information that occurs when there is an imperfect match between observations and measurement predictions.
     </p>
     <p class="noindent">
      Unexpected observations will affect the creation of new features in the map whereas unobserved measurement predictions will affect
     the removal of features from the map. As discussed earlier, each specific prediction or observation has an unknown exact value
     and so it is represented by a distribution. The uncertainties of all of these quantities must be considered throughout this process.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/concurrent-localisation.png" width="150%"/>
       <a id="x5-95001r15">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.15:
       </span>
       <span class="content">
        General schematic for concurrent localization and map building.
       </span>
      </figcaption>
     </div>
     <div class="warning">
      <p class="noindent">
       The new type of map we are creating not only has features in it as did previous maps, but it
      also has varying degrees of probability that each feature is indeed part of the environment.
      </p>
     </div>
     <p class="noindent">
      We represent this new map
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        M
       </mi>
      </math>
      with a
     set
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        n
       </mi>
      </math>
      of probabilistic feature locations
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          ẑ
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      , each with the covariance matrix
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi mathvariant="normal">
          Σ
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      and an associated
      <alert style="color: #821131;">
       credibility
factor
      </alert>
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          c
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      between 0 and 1.
     </p>
     <div class="knowledge">
      <p class="noindent">
       The purpose of
       <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
        <msubsup>
         <mrow>
          <mi>
           c
          </mi>
         </mrow>
         <mrow>
          <mi>
           t
          </mi>
         </mrow>
         <mrow>
         </mrow>
        </msubsup>
       </math>
       is to quantify the belief in the existence of the feature in the environment (see Fig. (5.41)):
      </p>
     </div>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-95002r11">
         </mstyle>
         <mi>
          M
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             {
            </mo>
            <mrow>
             <msubsup>
              <mrow>
               <mi>
                z
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <msubsup>
              <mrow>
               <mi mathvariant="normal">
                Σ
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mo class="MathClass-punc" stretchy="false">
              ,
             </mo>
             <msubsup>
              <mrow>
               <mi>
                c
               </mi>
              </mrow>
              <mrow>
               <mi>
                t
               </mi>
              </mrow>
              <mrow>
              </mrow>
             </msubsup>
             <mstyle mathsize="1.61em">
              <mrow>
               <mo fence="true" form="prefix">
                |
               </mo>
               <mrow>
               </mrow>
               <mo fence="true" form="postfix">
               </mo>
              </mrow>
             </mstyle>
             <msup>
              <mrow>
               <mrow>
                <mo fence="true" form="prefix">
                 (
                </mo>
                <mrow>
                 <mn>
                  1
                 </mn>
                 <mo class="MathClass-rel" stretchy="false">
                  ≦
                 </mo>
                 <mi>
                  t
                 </mi>
                 <mo class="MathClass-rel" stretchy="false">
                  ≦
                 </mo>
                 <mi>
                  n
                 </mi>
                </mrow>
                <mo fence="true" form="postfix">
                 )
                </mo>
               </mrow>
              </mrow>
              <mrow>
              </mrow>
             </msup>
            </mrow>
            <mo fence="true" form="postfix">
             }
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.11)
       </td>
      </tr>
     </table>
     <p class="noindent">
      In contrast to the map used for Kalman filter localisation previously, the map
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        M
       </mi>
      </math>
      is
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      assumed to be
      <alert style="color: #821131;">
       precisely known
      </alert>
      as it will be created by an uncertain robot over time. This is why the features
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          ẑ
         </mi>
        </mrow>
        <mrow>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      are described with associated
     covariance matrices
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi mathvariant="normal">
          Σ
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      .
     </p>
     <p class="noindent">
      Similar to Kalman filter localisation, the matching steps has three <alert style="color: #821131;">(3)</alert>
     outcomes in regard to measurement predictions and observations:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        matched
       prediction
       and
       observations,
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        unexpected
       observations,
       and
       </p>
      </li>
      <li class="itemize">
       <p class="noindent">
        unobserved
       predictions
       </p>
      </li>
     </ul>
     <p class="noindent">
      Localisation, or the position update of the robot, proceeds as before. However, the map is also
     updated now, using all three outcomes and complete propagation of all the correllated uncertainties.
     </p>
     <p class="noindent">
      The interesting concept in this modelling is the
      <alert style="color: #821131;">
       credibility factor
      </alert>
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          c
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      , which governs the likelihood that the mapped feature is indeed in the environment.
     </p>
     <div class="quoteblock">
      <p class="noindent">
       How
      should
      the
      robot’s
      failure
      to
      match
      observed
      features
      to
      a
      particular
      map
      feature
      reduce
      that
      map
      feature’s
      credibility?
      </p>
      <p class="noindent">
       How
      should
      the
      robot’s
      success
      at
      matching
      a
      mapped
      feature
      increase
      the
      chance
      that
      the
      mapped
      feature
      is
      “correct?”
      </p>
     </div>
     <p class="noindent">
      As an example, in <a id="x5-95003"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#carroll2017transformation">[83]</a></span> the following function is proposed for calculating credibility:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-95004r12">
         </mstyle>
         <msubsup>
          <mrow>
           <mi>
            c
           </mi>
          </mrow>
          <mrow>
           <mi>
            t
           </mi>
          </mrow>
          <mrow>
          </mrow>
         </msubsup>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mi>
              k
             </mi>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mn>
          1
         </mn>
         <mo class="MathClass-bin" stretchy="false">
          −
         </mo>
         <mi class="loglike">
          exp
         </mi>
         <mo>
          ⁡
         </mo>
         <msup>
          <mrow>
           <mrow>
            <mo fence="true" form="prefix">
             (
            </mo>
            <mrow>
             <mo class="MathClass-bin" stretchy="false">
              −
             </mo>
             <msup>
              <mrow>
               <mrow>
                <mo fence="true" form="prefix">
                 (
                </mo>
                <mrow>
                 <mfrac>
                  <mrow>
                   <msubsup>
                    <mrow>
                     <mi>
                      n
                     </mi>
                    </mrow>
                    <mrow>
                     <mi>
                      s
                     </mi>
                    </mrow>
                    <mrow>
                    </mrow>
                   </msubsup>
                  </mrow>
                  <mrow>
                   <mi>
                    a
                   </mi>
                  </mrow>
                 </mfrac>
                 <mo class="MathClass-bin" stretchy="false">
                  −
                 </mo>
                 <mfrac>
                  <mrow>
                   <msubsup>
                    <mrow>
                     <mi>
                      n
                     </mi>
                    </mrow>
                    <mrow>
                     <mi>
                      u
                     </mi>
                    </mrow>
                    <mrow>
                    </mrow>
                   </msubsup>
                  </mrow>
                  <mrow>
                   <mi>
                    a
                   </mi>
                  </mrow>
                 </mfrac>
                </mrow>
                <mo fence="true" form="postfix">
                 )
                </mo>
               </mrow>
              </mrow>
              <mrow>
              </mrow>
             </msup>
            </mrow>
            <mo fence="true" form="postfix">
             )
            </mo>
           </mrow>
          </mrow>
          <mrow>
          </mrow>
         </msup>
        </math>
       </td>
       <td class="eq-no">
        (3.12)
       </td>
      </tr>
     </table>
     <p class="noindent">
      where
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        a
       </mi>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        a
       </mi>
      </math>
      define the
      <alert style="color: #821131;">
       learning
      </alert>
      and
      <alert style="color: #821131;">
       forgetting
      </alert>
      rate and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          n
         </mi>
        </mrow>
        <mrow>
         <mi>
          s
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      and
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          n
         </mi>
        </mrow>
        <mrow>
         <mi>
          u
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      are the number of matched and unobserved
     predictions up to time
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <mi>
        k
       </mi>
      </math>
      , respectively.
     The update of the covariance matrix
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi mathvariant="normal">
          Σ
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      building the feature positions and the robot’s position are strongly correlated.
     </p>
     <div class="warning">
      <p class="noindent">
       This forces us to use a stochastic map, in which all cross-correlations must be updated in each cycle.
      </p>
     </div>
     <p class="noindent">
      The stochastic map consists of a stacked system state vector:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-95005r13">
         </mstyle>
         <mover accent="true">
          <mrow>
           <mi>
            X
           </mi>
          </mrow>
          <mo accent="true">
           →
          </mo>
         </mover>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mrow>
          <mo fence="true" form="prefix">
           [
          </mo>
          <mrow>
           <msubsup>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mrow>
             <mi>
              r
             </mi>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                k
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mspace class="quad" width="1em">
           </mspace>
           <msubsup>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mrow>
             <mn>
              1
             </mn>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                k
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mspace class="quad" width="1em">
           </mspace>
           <msubsup>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mrow>
             <mn>
              2
             </mn>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                k
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mspace class="quad" width="1em">
           </mspace>
           <mo>
            …
           </mo>
           <mspace class="quad" width="1em">
           </mspace>
           <msubsup>
            <mrow>
             <mi>
              x
             </mi>
            </mrow>
            <mrow>
             <mi>
              n
             </mi>
            </mrow>
            <mrow>
            </mrow>
           </msubsup>
           <msup>
            <mrow>
             <mrow>
              <mo fence="true" form="prefix">
               (
              </mo>
              <mrow>
               <mi>
                k
               </mi>
              </mrow>
              <mo fence="true" form="postfix">
               )
              </mo>
             </mrow>
            </mrow>
            <mrow>
            </mrow>
           </msup>
           <mspace class="quad" width="1em">
           </mspace>
          </mrow>
          <mo fence="true" form="postfix">
           ]
          </mo>
         </mrow>
         <mi>
          T
         </mi>
        </math>
       </td>
       <td class="eq-no">
        (3.13)
       </td>
      </tr>
     </table>
     <p class="noindent">
      and  a  system  state  covariance  matrix:
     </p>
     <table class="equation">
      <tr>
       <td>
        <p class="noindent">
        </p>
        <math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML">
         <mstyle class="label" id="x5-95006r14">
         </mstyle>
         <mi mathvariant="normal">
          Σ
         </mi>
         <mo class="MathClass-rel" stretchy="false">
          =
         </mo>
         <mrow>
          <mo fence="true" form="prefix">
           [
          </mo>
          <mrow>
           <mtable align="axis" class="array" columnlines="none none none none none none none none none" displaystyle="true" equalcolumns="false" equalrows="false" style="">
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 r
                </mi>
                <mi>
                 r
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 r
                </mi>
                <mn>
                 1
                </mn>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mo class="MathClass-rel" stretchy="false">
               ⋯
              </mo>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 r
                </mi>
                <mi>
                 n
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mn>
                 1
                </mn>
                <mi>
                 r
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mn>
                 1
                </mn>
                <mn>
                 1
                </mn>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mo class="MathClass-rel" stretchy="false">
               ⋯
              </mo>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mn>
                 1
                </mn>
                <mi>
                 n
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
              <mi>
               ⋮
              </mi>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mi>
               ⋮
              </mi>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mo class="MathClass-rel" stretchy="false">
               ⋱
              </mo>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mi>
               ⋮
              </mi>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 n
                </mi>
                <mi>
                 r
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 n
                </mi>
                <mn>
                 1
                </mn>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <mo class="MathClass-rel" stretchy="false">
               ⋯
              </mo>
             </mtd>
             <mtd class="array-td" columnalign="center">
              <msubsup>
               <mrow>
                <mi>
                 C
                </mi>
               </mrow>
               <mrow>
                <mi>
                 n
                </mi>
                <mi>
                 n
                </mi>
               </mrow>
               <mrow>
               </mrow>
              </msubsup>
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
            <mtr class="array-row">
             <mtd class="array-td" columnalign="center">
             </mtd>
            </mtr>
           </mtable>
          </mrow>
          <mo fence="true" form="postfix">
           ]
          </mo>
         </mrow>
        </math>
       </td>
       <td class="eq-no">
        (3.14)
       </td>
      </tr>
     </table>
     <p class="noindent">
     </p>
     <p class="noindent">
      where the index r stands for the robot and the index i = 1 to n for the features in the map.
     </p>
     <p class="noindent">
      In contrast to localization based on an a priori accurate map, in the case of a stochastic map the cross-correlations must
     be maintained and updated as the robot is performing automatic map-building. During each localization cycle,
     the cross-correlations robot-to-feature and feature-to-robot are also updated. In short, this optimal approach
     requires every value in the map to depend on every other value, and therein lies the reason that such a complete
     solution to the automatic mapping problem is beyond the reach of even today’s computational resources.
     </p>
     <h3 class="subsectionHead">
      <a aria-label="Link to this section in PDF version" class="pdf-link" href="MobileRoboticsLectureBook.pdf#subsection.3.8.2" style="color: red;  position: absolute;  left: -30px;" target="_blank">
       <svg class="bi bi-file-pdf" fill="currentColor" height="20" viewbox="0 0 16 16" width="20" xmlns="http://www.w3.org/2000/svg">
        <path d="M4 0a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h8a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H4zm0 1h8a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V2a1 1 0 0 1 1-1z">
        </path>
        <path d="M4.603 12.087a.81.81 0 0 1-.438-.42c-.195-.388-.13-.776.08-1.102.198-.307.526-.568.897-.787a7.68 7.68 0 0 1 1.482-.645 19.701 19.701 0 0 0 1.062-2.227 7.269 7.269 0 0 1-.43-1.295c-.086-.4-.119-.796-.046-1.136.075-.354.274-.672.65-.823.192-.077.4-.12.602-.077a.7.7 0 0 1 .477.365c.088.164.12.356.127.538.007.187-.012.395-.047.614-.084.51-.27 1.134-.52 1.794a10.954 10.954 0 0 0 .98 1.686 5.753 5.753 0 0 1 1.334.05c.364.065.734.195.96.465.12.144.193.32.2.518.007.192-.047.382-.138.563a1.04 1.04 0 0 1-.354.416.856.856 0 0 1-.51.138c-.331-.014-.654-.196-.933-.417a5.716 5.716 0 0 1-.911-.95 11.642 11.642 0 0 0-1.997.406 11.311 11.311 0 0 1-1.021 1.51c-.29.35-.608.655-.926.787a.793.793 0 0 1-.58.029zm1.379-1.901c-.166.076-.32.156-.459.238-.328.194-.541.383-.647.547-.094.145-.096.25-.04.361.01.022.02.036.026.044a.27.27 0 0 0 .035-.012c.137-.056.355-.235.635-.572a8.18 8.18 0 0 0 .45-.606zm1.64-1.33a12.647 12.647 0 0 1 1.01-.193 11.666 11.666 0 0 1-.51-.858 20.741 20.741 0 0 1-.5 1.05zm2.446.45c.15.162.296.3.435.41.24.19.407.253.498.256a.107.107 0 0 0 .07-.015.307.307 0 0 0 .094-.125.436.436 0 0 0 .059-.2.095.095 0 0 0-.026-.063c-.052-.062-.2-.152-.518-.209a3.881 3.881 0 0 0-.612-.053zM8.078 5.8a6.7 6.7 0 0 0 .2-.828c.031-.188.043-.343.038-.465a.613.613 0 0 0-.032-.198.517.517 0 0 0-.145.04c-.087.035-.158.106-.196.283-.04.192-.03.469.046.822.024.111.054.227.09.346z">
        </path>
       </svg>
      </a>
      <small class="titlemark">
       3.8.2
      </small>
      <a id="x5-960003.8.2">
      </a>
      Other  Mapping  Techniques
     </h3>
     <p class="noindent">
      The
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      research community has spent significant research effort on the problem of automatic mapping, and has demonstrating
     working systems in many environments without having solved the complete stochastic map problem described earlier.
     </p>
     <div class="warning">
      <p class="noindent">
       This field of
       <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
        AMR
       </a>
       research is extremely large, and this Lecture Book will
       <span id="bold" style="font-weight:bold;">
        NOT
       </span>
       present a comprehensive survey of the field
      </p>
     </div>
     <p class="noindent">
      Instead, let’s look at the two <alert style="color: #821131;">(2)</alert>
     key considerations associated with automatic mapping, together with brief
     discussions of the approaches taken by several automatic mapping solutions to overcome these challenges.
      <a id="subsubsection*.36">
      </a>
     </p>
     <h5 class="subsubsectionHead">
      <a id="x5-97000">
      </a>
      Cyclic  Environments
     </h5>
     <p class="noindent">
      Possibly the single hardest challenge for automatic mapping to be conquered is to correctly map cyclic environments. The problem is simple:
     </p>
     <div class="quoteblock">
      <p class="noindent">
       Given
      an
      environment
      which
      has
      one
      or
      more
      loops
      or
      cycles,
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          27
         </sup>
        </span>
       </alert>
       <span class="marginnote">
        <alert style="color: #821131;">
         <span id="bold" style="font-weight:bold;">
          <sup class="textsuperscript">
           27
          </sup>
         </span>
        </alert>
        <span id="textcolor138">
         Such
        as
        four
        <alert style="color: #821131;">(4)</alert>
        hallways
        that
        intersect
        to
        form
        a
        rectangle
        </span>
       </span>
       create
      a
      globally
      consistent
      map
      for
      the
      whole
      environment.
      </p>
     </div>
     <p class="noindent">
      This problem is hard because of the fundamental behavior of automatic mapping systems:
     </p>
     <div class="warning">
      <p class="noindent">
       The  maps  they  create  are  not  perfect.
      </p>
     </div>
     <p class="noindent">
      And, given any local imperfection, accumulating such imperfections over time can lead to arbitrarily
     large global errors between a map, at the macro level, and the real world, as shown in
      <span id="bold" style="font-weight:bold;">
       Fig.
      </span>
      <a href="#x5-97001r16">
       3.16
      </a>.
     </p>
     <div class="figure">
      <p class="noindent">
       <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/cyclic-environments.png" width="150%"/>
       <a id="x5-97001r16">
       </a>
      </p>
      <figcaption class="caption">
       <span class="id">
        Figure
       3.16:
       </span>
       <span class="content">
        A naive, local mapping strategy with small local error leads to global maps that have a significant error,
       as demonstrated by this real-world run on the left. By applying topological correction, the grid map on
       the right is extracted <span class="cite"><a href="MobileRoboticsLectureBookad1.html#bruce2000fast">[84]</a></span>.
       </span>
      </figcaption>
     </div>
     <p class="noindent">
      Such global error is usually irrelevant for
      <a class="glossary" href="MobileRoboticsLectureBookli2.html#glo:amr">
       AMR
      </a>
      localisation and navigation. After all, a warped map will still serve the robot
     perfectly well
      <alert style="color: #821131;">
       so long as the local error is bounded
      </alert>
      . However, an extremely large loop still eventually returns to the same spot,
     and the robot must be able to note this fact in its map. Therefore, global error does indeed matter in the case
     of cycles. In some of the earliest work attempting to solve the cyclic environment problem, [116] used a purely
     topological representation of the environment, reasoning that the topological representation only captures the
     most abstract, most important features and avoids a great deal of irrelevant detail. When the robot arrives at a
     topological node that could be the same as a previously visited and mapped node (e.g. similar distinguishing
     features), then the robot postulates that it has indeed returned to the same node. To check this hypothesis, the robot
     explicitly plans and moves to adjacent nodes to see if its perceptual readings are consistent with the cycle hypothesis.
     </p>
     <p class="noindent">
      With the recent popularity of metric maps such as fixed decomposition grid representations, the cycle detection strategy is not as
     straightforward. Two important features are found in most autonomous mapping systems that claim to solve the cycle detection problem:
     </p>
     <ul class="itemize1">
      <li class="itemize">
       <p class="noindent">
        First, as with many recent systems, these mobile robots tend to accumulate recent perceptual history to
        <alert style="color: #821131;">
         create
small-scale local sub-maps
        </alert> <a id="x5-97003"></a><span class="cite"><a href="MobileRoboticsLectureBookad1.html#nourbakhsh1997mobile">[85]</a></span>. In this approach, each sub-map is treated as a singular sensor during the robot’s position
       update. The advantage of this approach is two-fold.
       </p>
       <ul class="itemize2">
        <li class="itemize">
         <p class="noindent">
          As
         odometry
         is
         relatively
         accurate
         over
         small
         distances,
         the
         relative
         registration
         of
         features
         and
         raw
         sensor
         strikes
         in
         a
         local
         sub-map
         will
         be
         quite
         accurate.
         </p>
        </li>
        <li class="itemize">
         <p class="noindent">
          The
         robot
         will
         have
         created
         a
         virtual
         sensor
         system
         with
         a
         significantly
         larger
         horizon
         than
         its
         actual
         sensor
         system’s
         range.
         In
         a
         sense,
         this
         strategy
         at
         the
         very
         least
         defers
         the
         problem
         of
         very
         large
         cyclic
         environments
         by
         increasing
         the
         map
         scale
         that
         can
         be
         handled
         well
         by
         the
         robot.
         </p>
        </li>
       </ul>
      </li>
      <li class="itemize">
       <p class="noindent">
        The second recent technique for dealing with cycle environments is in fact a
        <alert style="color: #821131;">
         return to the topological representation
        </alert>
        .
       Some recent automatic mapping systems will attempt to identify cycles by associating a topology with the set of metric
       sub-maps, explicitly identifying the loops first at the topological level.
       </p>
      </li>
     </ul>
     <p class="noindent">
      One could certainly imagine other augmentations based on known topological methods.
     </p>
     <div class="knowledge">
      <p class="noindent">
       For example, the globally unique localisation methods described previously could be used to identify topological correctness.
      </p>
     </div>
     <a id="subsubsection*.37">
     </a>
     <h5 class="subsubsectionHead">
      <a id="x5-98000">
      </a>
      Dynamic  Environments
     </h5>
     <p class="noindent">
      A second challenge extends
      <span id="bold" style="font-weight:bold;">
       NOT
      </span>
      just to existing autonomous mapping solutions but even to the basic execution of the stochastic map approach.
     </p>
     <div class="warning">
      <p class="noindent">
       All previously mentioned strategies tend to assume the environment is either
       <alert style="color: #821131;">
        unchanging
       </alert>
       or changes
      in ways that are
       <alert style="color: #821131;">
        virtually insignificant
       </alert>
       . Such assumptions are certainly valid with respect to some
      environments, such as for example the computer science department of a university at 3:00 AM.
      </p>
     </div>
     <p class="noindent">
      However, for many practical applications, this assumption is lacking at best. In the case of wide-open spaces that
     are popular gathering places for humans, there is rapid change in the freespace and a vast majority of sensor
     strikes represent detection of the transient humans rather than fixed surfaces such as the perimeter wall.
     </p>
     <p class="noindent">
      Another class of dynamic environments are spaces such as factory floors and warehouses, where the objects
     being stored redefine the topology of the pathways on a day-to-day basis as shipments are moved in and out.
     </p>
     <p class="noindent">
      In
     all
     such
     dynamic
     environments,
     an
     automatic
     mapping
     system
     should
     capture
     the
     salient
      <alert style="color: #821131;">
       <span id="bold" style="font-weight:bold;">
        <sup class="textsuperscript">
         28
        </sup>
       </span>
      </alert>
      <span class="marginnote">
       <alert style="color: #821131;">
        <span id="bold" style="font-weight:bold;">
         <sup class="textsuperscript">
          28
         </sup>
        </span>
       </alert>
       <span id="textcolor141">
        In
       this
       context,
       salient
       means
       anything
       which
       is
       sticking
       out.
       </span>
      </span>
      objects
     detected
     by
     its
     sensors
     and,
     furthermore,
     the
     robot
     should
     have
     the
     flexibility
     to
     modify
     its
     map
     as
     the
     position
     of
     these
     salient
     objects
     changes.
     </p>
     <p class="noindent">
      The
     subject
     of
      <alert style="color: #821131;">
       continuous
mapping
      </alert>
      ,
     or
     mapping
     of
     dynamic
     environments
     is
     to
     some
     degree
     a
     direct
     outgrowth
     of
     successful
     strategies
     for
     automatic
     mapping
     of
     unfamiliar
     environments.
     </p>
     <p class="noindent">
      For
     example,
     in
     the
     case
     of
     stochastic
     mapping
     using
     the
     credibility
     factor
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          c
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      mechanism,
     the
     credibility
     equation
     can
     continue
     to
     provide
     feedback
     regarding
     the
     probability
     of
     existence
     of
     various
     mapped
     features
     after
     the
     initial
     map
     creation
     process
     is
     ostensibly
     complete.
     Therefore,
     a
     mapping
     system
     can
     become
     a
     map-modifying
     system
     by
     simply
     continuing
     to
     operate.
     </p>
     <div class="knowledge">
      <p class="noindent">
       This
      is
      most
      effective,
      of
      course,
      if
      the
      mapping
      system
      is
      real-time
      and
      incremental.
      </p>
     </div>
     <p class="noindent">
      If
     map
     construction
     requires
     off-line
     global
     optimisation,
     then
     the
     desire
     to
     make
     small-grained,
     incremental
     adjustments
     to
     the
     map
     is
     more
     difficult
     to
     satisfy.
     </p>
     <p class="noindent">
      Earlier
     we
     stated
     that
     a
     mapping
     system
     should
     capture
     only
     the
     salient
     objects
     detected
     by
     its
     sensors.
     One
     common
     argument
     for
     handling
     the
     detection
     of,
     for
     instance,
     humans
     in
     the
     environment
     is
     that
     mechanisms
     such
     as
      <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
       <msubsup>
        <mrow>
         <mi>
          c
         </mi>
        </mrow>
        <mrow>
         <mi>
          t
         </mi>
        </mrow>
        <mrow>
        </mrow>
       </msubsup>
      </math>
      serve
     to
     be
     mapped
     in
     the
     first
     place.
     </p>
     <p class="noindent">
      The
     general
     solution
     to
     the
     problem
     of
     detecting
     salient
     features,
     however,
     requires
     a
     solution
     to
     the
     perception
     problem
     in
     general.
     When
     a
     robot’s
     sensor
     system
     can
     reliably
     detect
     the
     difference
     between
     a
     wall
     and
     a
     human,
     using
     for
     example
     a
     vision
     system,
     then
     the
     problem
     of
     mapping
     in
     dynamic
     environments
     will
     become
     significantly
     more
     straightforward.
     </p>
     <p class="noindent">
      We
     have
     discussed
     just
     two
     important
     considerations
     for
     automatic
     mapping.
     There
     is
     still
     a
     great
     deal
     of
     research
     activity
     focusing
     on
     the
     general
     map
     building
     and
     localisation
     problem.
     This
     field
     is
     certain
     to
     produce
     significant
     new
     results
     in
     the
     next
     several
     years,
     and
     as
     the
     perceptual
     power
     of
     robots
     improves
     we
     expect
     the
     payoff
     to
     be
     greatest
     here.
     </p>
     <p class="noindent">
      <span class="marginnote">
       <iframe height="250" src="https://www.youtube.com/embed/7a6GrKqOxeU" width="380">
       </iframe>
       ttt
      </span>
     </p>
     <div class="informationblock" id="tcolobox-5">
      <div class="title">
       <p class="noindent">
        <span id="bold" style="font-weight:bold;">
         Information
        </span>
        :
       DARPA
       Grand
       Challenge
       </p>
      </div>
      <div class="box-content">
       <p class="noindent">
        A prize competition for American autonomous vehicles, funded by the Defense Advanced Research Projects
       Agency (DARPA). The goal of the challenge is too further DARPA’s mission to sponsor revolutionary, high-payoff
       research that bridges the gap between fundamental discoveries and military use. The initial DARPA Grand
       Challenge in 2004 was created to spur the development of technologies needed to create the first fully autonomous
       ground vehicles capable of completing a substantial off-road course within a limited time. The third event, the
       DARPA Urban Challenge in 2007, extended the initial Challenge to autonomous operation in a mock urban
       environment. The 2012 DARPA Robotics Challenge, focused on autonomous emergency-maintenance robots, and
       new Challenges are still being conceived. The DARPA Subterranean Challenge was tasked with building robotic
       teams to autonomously map, navigate, and search subterranean environments. Such teams could be useful in
       exploring hazardous areas and in search and rescue.
       </p>
       <div class="figure">
        <p class="noindent">
         <img alt="PIC" height="" src="figures/Mobile-Robot-Localisation/raster/darpa-challenge.jpg" width="150%"/>
         <a id="x5-98001r17">
         </a>
        </p>
        <figcaption class="caption">
         <span class="id">
          Figure
         3.17:
         </span>
         <span class="content">
          Stanford Racing and Victor Tango together at an intersection in the DARPA Urban Challenge
         Finals.
         </span>
        </figcaption>
       </div>
      </div>
     </div>
    </div>
   </article>
   <footer>
    <div class="next">
     <p class="noindent">
      <a href="MobileRoboticsLectureBookch4.html" style="float: right;">
       Next Chapter →
      </a>
      <a href="MobileRoboticsLectureBookch2.html" style="float: left;">
       ← Previous Chapter
      </a>
      <a href="index.html">
       TOP
      </a>
     </p>
    </div>
   </footer>
  </div>
  <div class="page">
   <article class="chapter">
    <p class="noindent">
     <a id="tailMobileRoboticsLectureBookch3.html">
     </a>
    </p>
   </article>
  </div>
 </body>
</html>
